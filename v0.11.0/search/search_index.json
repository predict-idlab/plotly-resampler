{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to plotly-resampler\u2019s documentation!","text":"<p>This is the documentation of ; a wrapper for plotly Figures to visualize large time-series data.</p> <p></p> <p>As shown in the demo above, <code>plotly-resampler</code> maintains its interactiveness on large data by applying front-end resampling respective to the view.</p> <p> PyPI  Github  DOI</p>"},{"location":"FAQ/","title":"FAQ \u2753","text":"What does the orange <code>~time|number</code> suffix in legend name indicate? <p>This tilde suffix is only shown when the data is aggregated and represents the mean aggregation bin size which is the mean index-range difference between two consecutive aggregated samples.</p> <ul> <li>for time-indexed data: the mean time-range between 2 consecutive (sampled) samples.</li> <li>for numeric-indexed data: the mean numeric range between 2 consecutive (sampled) samples.</li> </ul> <p>When the index is a range-index; the mean aggregation bin size represents the mean downsample ratio; i.e., the mean number of samples that are aggregated into one sample.</p> What is the difference between plotly-resampler figures and plain plotly figures? <p>plotly-resampler can be thought of as wrapper around plain plotly figures which adds line-chart visualization scalability by dynamically aggregating the data of the figures w.r.t. the front-end view. plotly-resampler thus adds dynamic aggregation functionality to plain plotly figures.</p> <p>important to know:</p> <ul> <li><code>show</code> always returns a static html view of the figure, i.e., no dynamic aggregation can be performed on that view.</li> <li>To have dynamic aggregation:<ul> <li>with <code>FigureResampler</code>, you need to call <code>show_dash</code> (or output the object in a cell via <code>IPython.display</code>) -&gt;   which spawns a dash-web app, and the dynamic aggregation is realized with dash callback</li> <li>with <code>FigureWidgetResampler</code>, you need to use <code>IPython.display</code> on the object,   which uses widget-events to realize dynamic aggregation (via the running IPython kernel).</li> </ul> </li> </ul> <p>other changes of plotly-resampler figures w.r.t. vanilla plotly:</p> <ul> <li>double-clicking within a line-chart area does not Reset Axes, as it results in an \u201cAutoscale\u201d event. We decided to implement an Autoscale event as updating your y-range such that it shows all the data that is in your x-range<ul> <li>Note: vanilla Plotly figures their Autoscale result in Reset Axes behavior,  in our opinion this did not make a lot of sense.  It is therefore that we have overriden this behavior in plotly-resampler.</li> </ul> </li> </ul> My <code>FigureResampler.show_dash</code> keeps hanging (indefinitely) with the error message: <code>OSError: Port already in use</code> <p>Disclaimer</p> <p>Since v0.9.0 we use Dash instead of JupyterDash for Jupyter integration which should have resolved this issue!</p> <p>Plotly-resampler its <code>FigureResampler.show_dash</code> method leverages the jupyterdash toolkit to easily allow integration of dash apps in notebooks. However, there is a known issue with jupyterDash that causes the <code>FigureResampler.show_dash</code> method to hang when the port is already in use. In a future Pull-Request they will hopefully fix this issue. We internally track this issue as well - please comment there if you want to provide feedback.</p> <p>In the meantime, you can use the following workaround (if you do not care about the Werkzeug security issue): <code>pip install werkzeug==2.1.2</code>.</p> What is the difference in approach between plotly-resampler and datashader? <p>Datashader is a highly scalable open-source library for analyzing and visualizing large datasets. More specifically, datashader \u201crasterizes\u201d or \u201caggregates\u201d datasets into regular grids that can be analyzed further or viewed as images.</p> <p>The main differences are:</p> <p>Datashader can deal with various kinds of data (e.g., location related data, point clouds), whereas plotly-resampler is more tailored towards time-series data visualizations. Furthermore, datashader outputs a rasterized image/array encompassing all traces their data, whereas plotly-resampler outputs an aggregated series per trace. Thus, datashader is more suited for analyzing data where you do not want to pin-out a certain series/trace.</p> <p>In our opinion, datashader truly shines (for the time series use case) when:</p> <ul> <li>you want a global, overlaying view of all your traces</li> <li>you want to visualize a large number of time series in a single plot (many traces)</li> <li>there is a lot of noise on your high-frequency data and you want to uncover the underlying pattern</li> <li>you want to render all data points in your visualization</li> </ul> <p>In our opinion, plotly-resampler shines when:</p> <ul> <li>you need the capabilities to interact with the traces (e.g., hovering, toggling traces, hovertext per trace)</li> <li>you want to use a less complex (but more restricted) visualization interface (as opposed to holoviews), i.e., plotly</li> <li>you want to make existing plotly time-series figures more scalable and efficient</li> <li>to build scalable Dash apps for time-series data visualization</li> </ul> <p>Furthermore combined with holoviews, datashader can also be employed in an interactive manner, see the example below.</p> <pre><code>   from holoviews.operation.datashader import datashade\n   import datashader as ds\n   import holoviews as hv\n   import numpy as np\n   import pandas as pd\n   import panel as pn\n\n   hv.extension(\"bokeh\")\n   pn.extension(comms='ipywidgets')\n\n   # Create the dummy dataframe\n   n = 1_000_000\n   x = np.arange(n)\n   noisy_sine = (np.sin(x / 3_000) + (np.random.randn(n) / 10)) * x / 5_000\n   df = pd.DataFrame(\n      {\"ns\": noisy_sine, \"ns_abs\": np.abs(noisy_sine),}\n   )\n\n   # Visualize interactively with datashader\n   opts = hv.opts.RGB(width=800, height=400)\n   ndoverlay = hv.NdOverlay({c:hv.Curve((df.index, df[c])) for c in df.columns})\n   datashade(ndoverlay, cnorm='linear', aggregator=ds.count(), line_width=3).opts(opts)\n</code></pre> <p></p> Pandas or numpy datetime works much slower than unix epoch timestamps? <p>This stems from the plotly scatter(gl) constructor being much slower for non-numeric data. Plotly performs a different serialization for datetime arrays (which are interpreted as object arrays). However, plotly-resampler should not be limited by this - to avoid this issue, add your datetime data as hf_x to your plotly-resampler <code>FigureResampler.add_trace</code> (or <code>FigureWidgetResampler.add_trace</code>) method. This avoids adding (&amp; serializing) all the data to the scatter object, since plotly-resampler will pass the aggregated data to the scatter object.</p> <p>Some illustration:</p> <pre><code>import plotly.graph_objects as go\nimport pandas as pd\nimport numpy as np\nfrom plotly_resampler import FigureResampler\n\n# Create the dummy dataframe\ny = np.arange(1_000_000)\nx = pd.date_range(start=\"2020-01-01\", periods=len(y), freq=\"1s\")\n\n# Create the plotly-resampler figure\nfig = FigureResampler()\n# fig.add_trace(go.Scatter(x=x, y=y))  # This is slow\nfig.add_trace(go.Scatter(), hf_x=x, hf_y=y)  # This is fast\n\n# ... (add more traces, etc.)\n</code></pre>"},{"location":"dash_app_integration/","title":"Dash apps \ud83e\udd1d","text":"<p>This documentation page describes how you can integrate <code>plotly-resampler</code> in a dash application.</p> <p>Examples of dash apps with <code>plotly-resampler</code> can be found in the examples folder of the GitHub repository.</p>"},{"location":"dash_app_integration/#registering-callbacks-in-a-new-dash-app","title":"Registering callbacks in a new dash app","text":"<p>When you add a <code>FigureResampler</code> figure in a basic dash app, you should:</p> <ul> <li>Register the <code>FigureResampler</code> figure its callbacks to the dash app.<ul> <li>The id of the dcc.Graph component that contains the   <code>FigureResampler</code> figure should be passed to the   <code>register_update_graph_callback</code> method.</li> </ul> </li> </ul> <p>Code illustration:</p> <pre><code># Construct the to-be resampled figure\nfig = FigureResampler(px.line(...))\n\n# Construct app &amp; its layout\napp = dash.Dash(__name__)\napp.layout = html.Div(children=[dcc.Graph(id=\"graph-id\", figure=fig)])\n\n# Register the callback\nfig.register_update_graph_callback(app, \"graph-id\")\n\n# start the app\napp.run_server(debug=True)\n</code></pre> <p>Warning</p> <p>The above example serves as an illustration, but uses a global variable to store the <code>FigureResampler</code> instance; this is not a good practice. Ideally you should cache the <code>FigureResampler</code> per session on the server side. In the examples folder, we provide several dash app examples where we perform server side caching of such figures.</p>"},{"location":"getting_started/","title":"Get started \ud83d\ude80","text":"<p>The <code>plotly-resampler</code> package offers two primary modules:</p> <ul> <li><code>figure_resampler</code>: a wrapper for plotly.graph_objects Figures,   coupling dynamic resampling functionality with the Figure.</li> <li><code>aggregation</code>: This module contains interfaces for the various aggregation methods implemented in tsdownsample.</li> </ul>"},{"location":"getting_started/#installation","title":"Installation \u2699\ufe0f","text":"<p>Install via pip:</p> <pre><code>pip install plotly-resampler\n</code></pre>"},{"location":"getting_started/#usage","title":"Usage \ud83d\udcc8","text":"<p>Plotly-Resampler facilitates dynamic resampling in two ways:</p> <ul> <li> <p>Automatic Approach (low code overhead)</p> <ul> <li>utilize the <code>register_plotly_resampler</code> function</li> <li>steps:<ol> <li>Import and invoke <code>register_plotly_resampler</code></li> <li>That\u2019s it! \ud83c\udf89Just proceed with your standard gaph construction workflow.</li> </ol> </li> <li>Upon invoking <code>register_plotly_resampler</code>, all new defined plotly graph objects are transformed into either    <code>FigureResampler</code> or   <code>FigureWidgetResampler</code> object.  The <code>mode</code> parameter in this method determines which resampling Figure type is used.</li> </ul> </li> <li> <p>Manual Approach (data aggregation configurability, graph construction speedups)</p> <ol> <li> <p>By utilizing Dash callbacks to augment a <code>go.Figure</code> with dynamic aggregation functionality.</p> <ul> <li>steps:<ol> <li>wrap the plotly Figure with <code>FigureResampler</code></li> <li>call <code>.show_dash()</code> on the Figure</li> </ol> </li> </ul> <p>Note</p> <p>This is particularly advantageous when working with Dash or outside Jupyter environments.</p> </li> <li> <p>By utilizing FigureWidget.layout.on_change   , when a <code>go.FigureWidget</code> is used within a <code>.ipynb</code> environment.</p> <ul> <li> <p>steps:</p> <ol> <li>wrap your plotly Figure   (can be a <code>go.Figure</code> with <code>FigureWidgetResampler</code>)</li> <li>output the <code>FigureWidgetResampler</code> instance in a cell</li> </ol> <p>Note</p> <p>This is especially useful when developing in <code>jupyter</code> environments and when you cannot open/forward a network-port.</p> </li> </ul> </li> </ol> </li> </ul> <p>Tip</p> <p>For significant faster initial loading of the Figure, we advise to </p> <ol> <li>wrap the constructor of the plotly Figure with either <code>FigureResampler</code> or <code>FigureWidgetResampler</code></li> <li>add the trace data as <code>hf_x</code> and <code>hf_y</code></li> </ol> <p>Note</p> <p>Any plotly Figure can be wrapped with dynamic aggregation functionality! \ud83c\udf89 But only <code>go.Scatter/go.Scattergl</code> traces will be resampled!</p>"},{"location":"getting_started/#examples","title":"Examples \u2705","text":""},{"location":"getting_started/#register_plotly_resampler","title":"register_plotly_resampler","text":"<pre><code>import plotly.graph_objects as go; import numpy as np\nfrom plotly_resampler import register_plotly_resampler, unregister_plotly_resampler\n\n# Call the register function once and all Figures/FigureWidgets will be wrapped\n# according to the register_plotly_resampler its `mode` argument\nregister_plotly_resampler(mode='auto')\n\nx = np.arange(1_000_000)\nnoisy_sin = (3 + np.sin(x / 200) + np.random.randn(len(x)) / 10) * x / 1_000\n\n\n# when working in an IPython environment, this will automatically be a\n# FigureWidgetResampler else, this will be an FigureResampler\nf = go.Figure()\nf.add_trace({\"y\": noisy_sin + 2, \"name\": \"yp2\"})\nf\n\n# to undo the wrapping, call the unregister_plotly_resampler function\n</code></pre>"},{"location":"getting_started/#figureresampler","title":"FigureResampler","text":"<pre><code># NOTE: this example works in a notebook environment\nimport plotly.graph_objects as go; import numpy as np\nfrom plotly_resampler import FigureResampler\n\nx = np.arange(1_000_000)\nsin = (3 + np.sin(x / 200) + np.random.randn(len(x)) / 10) * x / 1_000\n\nfig = FigureResampler(go.Figure())\nfig.add_trace(go.Scattergl(name='noisy sine', showlegend=True), hf_x=x, hf_y=sin)\n\nfig.show_dash(mode='inline')\n</code></pre>"},{"location":"getting_started/#overview","title":"Overview","text":"<p>In the example below, we demonstrate the (x-axis)<code>overview</code> feature of plotly-ressampler. For more information you can check out the examples to find dash apps and in-notebook use-cases.</p> <p>!!! Note:   - This overview is only available for the <code>FigureResampler</code> and not for the <code>FigureWidgetResampler</code>.   - As a recent and experimental feature, user feedback is crucial. Please report any issues encountered!</p> <p></p>"},{"location":"getting_started/#figurewidget","title":"FigureWidget","text":"<p>The gif below demonstrates the example usage of <code>FigureWidgetResampler</code>, where <code>JupyterLab</code> is used as the environment and the <code>FigureWidgetResampler</code>. Note how (i) the figure output is redirected into a new view, and (ii) how you are able to dynamically add traces!</p> <p></p> <p>Furthermore, plotly\u2019s <code>FigureWidget</code> allows to conveniently add callbacks to for example click events. This allows creating a high-frequency time series annotation app in a couple of lines; as shown in the gif below and in this notebook.</p> <p></p>"},{"location":"getting_started/#important-considerations-tips","title":"Important considerations &amp; tips \ud83d\udea8","text":"<ul> <li>When running the code on a server, you should forward the port of the   <code>FigureResampler.show_dash</code> method to your local machine. Note that you can add dynamic aggregation to plotly figures with the   <code>FigureWidgetResampler</code> wrapper without needing to forward a port!</li> <li>In general, when using downsampling one should be aware of (possible) aliasing effects.   The <code>[R]</code> in the legend indicates when the corresponding trace is resampled (and thus possibly distorted).   The <code>~ delta</code> suffix in the legend represents the mean index delta for consecutive aggregated data points.</li> <li>The plotly autoscale event (triggered by the autoscale button or a double-click within the graph),   does not reset the axes but autoscales the current graph-view of plotly-resampler figures.   This design choice was made as it seemed more intuitive for the developers to support this behavior   with double-click than the default axes-reset behavior.   The graph axes can ofcourse be resetted by using the reset_axis button.   If you want to give feedback and discuss this further with the developers, see this issue #49.</li> </ul>"},{"location":"getting_started/#dynamically-adjusting-the-scatter-data","title":"Dynamically adjusting the scatter data \ud83d\udd29","text":"<p>The raw high-frequency trace data of plotly-resampler figures can be adjusted using the <code>hf_data</code> property.</p> <p>Working example \u2b07\ufe0f:</p> <pre><code>import plotly.graph_objects as go; import numpy as np\nfrom plotly_resampler import FigureResampler\n# Note: a FigureWidgetResampler can be used here as well\n\n# Construct the hf-data\nx = np.arange(1_000_000)\nsin = (3 + np.sin(x / 200) + np.random.randn(len(x)) / 10) * x / 1_000\n\nfig = FigureResampler(go.Figure())\nfig.add_trace(go.Scattergl(name='noisy sine', showlegend=True), hf_x=x, hf_y=sin)\nfig.show_dash(mode='inline')\n\n# After some time -&gt; update the hf_data y property of the trace\n# As we only have 1 trace, this needs to be mapped\nfig.hf_data[-1]['y'] = - sin ** 2\n</code></pre> <p>Note</p> <p>hf_data only withholds high-frequency traces (i.e., traces that are aggregated). To add non high-frequency traces (i.e., traces with fewer data points than max_n_samples), you need to set the <code>limit_to_view</code> argument to True when adding the corresponding trace with the <code>add_trace</code> function.</p> <p>Tip</p> <p>The <code>FigureWidgetResampler</code> graph will not be automatically redrawn after adjusting the fig its hf_data property. The redrawing can be triggered by manually calling either:</p> <ul> <li><code>FigureWidgetResampler.reload_data</code>, which keeps the current-graph range.</li> <li><code>FigureWidgetResampler.reset_axes</code>, which performs a graph update.</li> </ul>"},{"location":"getting_started/#plotly-resampler-not-high-frequency-traces","title":"Plotly-resampler &amp; not high-frequency traces \ud83d\udd0d","text":"<p>Tip</p> <p>In the Skin conductance example of the basic_example.ipynb, we deal with such low-frequency traces.</p> <p>The <code>add_trace</code> method allows configuring argument which allows us to deal with low-frequency traces.</p>"},{"location":"getting_started/#use-cases","title":"Use-cases","text":"<ul> <li> <p>not resampling trace data: To achieve this, set:</p> </li> <li> <p><code>max_n_samples = len(hf_x)</code></p> </li> <li> <p>not resampling trace data, but slicing to the view: To achieve this, set:</p> </li> <li><code>max_n_samples = len(hf_x)</code></li> <li><code>limit_to_view = True</code></li> </ul> <p>Note</p> <p>For, irregularly sampled traces which are filled (e.g. colored background signal quality trace of the skin conductance example), it is important that you set <code>gap_handler</code> to <code>NoGapHandler</code> for that trace.</p> <p>Otherwise, when you leave <code>gap_handler</code> to <code>MedDiffGapHandler</code>, you may get weird background shapes such as \u2b07\ufe0f: </p> <p>When <code>gap_handler</code> is set to <code>NoGapHandler</code> you get \u2b07\ufe0f: </p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>aggregation<ul> <li>aggregation_interface</li> <li>aggregators</li> <li>gap_handler_interface</li> <li>gap_handlers</li> <li>plotly_aggregator_parser</li> </ul> </li> <li>figure_resampler<ul> <li>figure_resampler</li> <li>figure_resampler_interface</li> <li>figurewidget_resampler</li> <li>jupyter_dash_persistent_inline_output</li> <li>utils</li> </ul> </li> <li>registering</li> </ul>"},{"location":"api/registering/","title":"registering","text":"<p>Register plotly-resampler to (un)wrap plotly-graph-objects.</p>"},{"location":"api/registering/#registering.register_plotly_resampler","title":"<code>register_plotly_resampler(mode='auto', **aggregator_kwargs)</code>","text":"<p>Register plotly-resampler to plotly.graph_objects.</p> <p>This function results in the use of plotly-resampler under the hood.</p> <p>Note</p> <p>We advise to use mode= <code>widget</code> when working in an IPython based environment as this will just behave as a <code>go.FigureWidget</code>, but with dynamic aggregation. When using mode= <code>auto</code> or <code>figure</code>; most figures will be wrapped as <code>FigureResampler</code>, on which <code>show_dash</code> needs to be called.</p> <p>Note</p> <p>This function is mostly useful for notebooks. For dash-apps, we advise to look at the dash app examples on GitHub</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode of the plotly-resampler. Possible values are: \u2018auto\u2019, \u2018figure\u2019, \u2018widget\u2019, None. If \u2018auto\u2019 is used, the mode is determined based on the environment; if it is in an IPython environment, the mode is \u2018widget\u2019, otherwise it is \u2018figure\u2019. If \u2018figure\u2019 is used, all plotly figures are wrapped as FigureResampler objects. If \u2018widget\u2019 is used, all plotly figure widgets are wrapped as FigureWidgetResampler objects (we advise to use this mode in IPython environment with a kernel). If None is used, wrapping is done as expected (go.Figure -&gt; FigureResampler, go.FigureWidget -&gt; FigureWidgetResampler).</p> <code>'auto'</code> <code>aggregator_kwargs</code> <code>dict</code> <p>The keyword arguments to pass to the plotly-resampler decorator its constructor. See more details in <code>FigureResampler</code> and <code>FigureWidgetResampler</code>.</p> <code>{}</code> Source code in <code>plotly_resampler/registering.py</code> <pre><code>def register_plotly_resampler(mode=\"auto\", **aggregator_kwargs):\n    \"\"\"Register plotly-resampler to plotly.graph_objects.\n\n    This function results in the use of plotly-resampler under the hood.\n\n    !!! note\n        We advise to use mode= ``widget`` when working in an IPython based environment\n        as this will just behave as a ``go.FigureWidget``, but with dynamic aggregation.\n        When using mode= ``auto`` or ``figure``; most figures will be wrapped as\n        [`FigureResampler`][figure_resampler.FigureResampler], on which\n        [`show_dash`][figure_resampler.FigureResampler.show_dash] needs to be called.\n\n    !!! note\n        This function is mostly useful for notebooks. For dash-apps, we advise to look\n        at the dash app examples on [GitHub](https://github.com/predict-idlab/plotly-resampler/tree/main/examples#2-dash-apps)\n\n    Parameters\n    ----------\n    mode : str, optional\n        The mode of the plotly-resampler.\n        Possible values are: 'auto', 'figure', 'widget', None.\n        If 'auto' is used, the mode is determined based on the environment; if it is in\n        an IPython environment, the mode is 'widget', otherwise it is 'figure'.\n        If 'figure' is used, all plotly figures are wrapped as FigureResampler objects.\n        If 'widget' is used, all plotly figure widgets are wrapped as\n        FigureWidgetResampler objects (we advise to use this mode in IPython environment\n        with a kernel).\n        If None is used, wrapping is done as expected (go.Figure -&gt; FigureResampler,\n        go.FigureWidget -&gt; FigureWidgetResampler).\n    aggregator_kwargs : dict, optional\n        The keyword arguments to pass to the plotly-resampler decorator its constructor.\n        See more details in [`FigureResampler`][figure_resampler.FigureResampler] and\n        [`FigureWidgetResampler`][figure_resampler.FigureWidgetResampler].\n\n    \"\"\"\n    for constr_name, pr_class in PLOTLY_CONSTRUCTOR_WRAPPER.items():\n        if (mode == \"auto\" and _is_ipython_env()) or mode == \"widget\":\n            pr_class = FigureWidgetResampler\n        elif mode == \"figure\":\n            pr_class = FigureResampler\n        # else: default mode -&gt; wrap according to PLOTLY_CONSTRUCTOR_WRAPPER\n\n        for module in PLOTLY_MODULES:\n            _register_wrapper(module, constr_name, pr_class, **aggregator_kwargs)\n</code></pre>"},{"location":"api/registering/#registering.unregister_plotly_resampler","title":"<code>unregister_plotly_resampler()</code>","text":"<p>Unregister plotly-resampler from plotly.graph_objects.</p> Source code in <code>plotly_resampler/registering.py</code> <pre><code>def unregister_plotly_resampler():\n    \"\"\"Unregister plotly-resampler from plotly.graph_objects.\"\"\"\n    for constr in PLOTLY_CONSTRUCTOR_WRAPPER.keys():\n        for module in PLOTLY_MODULES:\n            _unregister_wrapper(module, constr)\n</code></pre>"},{"location":"api/aggregation/","title":"aggregation","text":"<p>Compatible implementation for various downsample methods and open interface to  other downsample methods.</p>"},{"location":"api/aggregation/#aggregation.AbstractAggregator","title":"<code>AbstractAggregator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>class AbstractAggregator(ABC):\n    def __init__(\n        self,\n        x_dtype_regex_list: Optional[List[str]] = None,\n        y_dtype_regex_list: Optional[List[str]] = None,\n        **downsample_kwargs,\n    ):\n        \"\"\"Constructor of AbstractSeriesAggregator.\n\n        Parameters\n        ----------\n        x_dtype_regex_list: List[str], optional\n            List containing the regex matching the supported datatypes for the x array,\n            by default None.\n        y_dtype_regex_list: List[str], optional\n            List containing the regex matching the supported datatypes for the y array,\n            by default None.\n        downsample_kwargs: dict\n            Additional kwargs passed to the downsample method.\n\n        \"\"\"\n        self.x_dtype_regex_list = x_dtype_regex_list\n        self.y_dtype_regex_list = y_dtype_regex_list\n        self.downsample_kwargs = downsample_kwargs\n\n    @staticmethod\n    def _check_n_out(n_out: int) -&gt; None:\n        \"\"\"Check if the n_out is valid.\"\"\"\n        assert isinstance(n_out, (int, np.integer))\n        assert n_out &gt; 0\n\n    @staticmethod\n    def _process_args(*args) -&gt; Tuple[np.ndarray | None, np.ndarray]:\n        \"\"\"Process the args into the x and y arrays.\n\n        If only y is passed, x is set to None.\n        \"\"\"\n        assert len(args) in [1, 2], \"Must pass either 1 or 2 arrays\"\n        x, y = (None, args[0]) if len(args) == 1 else args\n        return x, y\n\n    @staticmethod\n    def _check_arr(arr: np.ndarray, regex_list: Optional[List[str]] = None):\n        \"\"\"Check if the array is valid.\"\"\"\n        assert isinstance(arr, np.ndarray), f\"Expected np.ndarray, got {type(arr)}\"\n        assert arr.ndim == 1\n        AbstractAggregator._supports_dtype(arr, regex_list)\n\n    def _check_x_y(self, x: np.ndarray | None, y: np.ndarray) -&gt; None:\n        \"\"\"Check if the x and y arrays are valid.\"\"\"\n        # Check x (if not None)\n        if x is not None:\n            self._check_arr(x, self.x_dtype_regex_list)\n            assert x.shape == y.shape, \"x and y must have the same shape\"\n        # Check y\n        self._check_arr(y, self.y_dtype_regex_list)\n\n    @staticmethod\n    def _supports_dtype(arr: np.ndarray, dtype_regex_list: Optional[List[str]] = None):\n        # base case\n        if dtype_regex_list is None:\n            return\n\n        for dtype_regex_str in dtype_regex_list:\n            m = re.compile(dtype_regex_str).match(str(arr.dtype))\n            if m is not None:  # a match is found\n                return\n        raise ValueError(\n            f\"{arr.dtype} doesn't match with any regex in {dtype_regex_list}\"\n        )\n</code></pre>"},{"location":"api/aggregation/#aggregation.AbstractAggregator.__init__","title":"<code>__init__(x_dtype_regex_list=None, y_dtype_regex_list=None, **downsample_kwargs)</code>","text":"<p>Constructor of AbstractSeriesAggregator.</p> <p>Parameters:</p> Name Type Description Default <code>x_dtype_regex_list</code> <code>Optional[List[str]]</code> <p>List containing the regex matching the supported datatypes for the x array, by default None.</p> <code>None</code> <code>y_dtype_regex_list</code> <code>Optional[List[str]]</code> <p>List containing the regex matching the supported datatypes for the y array, by default None.</p> <code>None</code> <code>downsample_kwargs</code> <p>Additional kwargs passed to the downsample method.</p> <code>{}</code> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>def __init__(\n    self,\n    x_dtype_regex_list: Optional[List[str]] = None,\n    y_dtype_regex_list: Optional[List[str]] = None,\n    **downsample_kwargs,\n):\n    \"\"\"Constructor of AbstractSeriesAggregator.\n\n    Parameters\n    ----------\n    x_dtype_regex_list: List[str], optional\n        List containing the regex matching the supported datatypes for the x array,\n        by default None.\n    y_dtype_regex_list: List[str], optional\n        List containing the regex matching the supported datatypes for the y array,\n        by default None.\n    downsample_kwargs: dict\n        Additional kwargs passed to the downsample method.\n\n    \"\"\"\n    self.x_dtype_regex_list = x_dtype_regex_list\n    self.y_dtype_regex_list = y_dtype_regex_list\n    self.downsample_kwargs = downsample_kwargs\n</code></pre>"},{"location":"api/aggregation/#aggregation.AbstractGapHandler","title":"<code>AbstractGapHandler</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>plotly_resampler/aggregation/gap_handler_interface.py</code> <pre><code>class AbstractGapHandler(ABC):\n    def __init__(self, fill_value: Optional[float] = None):\n        \"\"\"Constructor of AbstractGapHandler.\n\n        Parameters\n        ----------\n        fill_value: float, optional\n            The value to fill the gaps with, by default None.\n            Note that setting this value to 0 for filled area plots is particularly\n            useful.\n\n        \"\"\"\n        self.fill_value = fill_value\n\n    @abstractmethod\n    def _get_gap_mask(self, x_agg: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"Get a boolean mask indicating the indices where there are gaps.\n\n        If you require custom gap handling, you can implement this method to return a\n        boolean mask indicating the indices where there are gaps.\n\n        Parameters\n        ----------\n        x_agg: np.ndarray\n            The x array. This is used to determine the gaps.\n\n        Returns\n        -------\n        Optional[np.ndarray]\n            A boolean mask indicating the indices where there are gaps. If there are no\n            gaps, None is returned.\n\n        \"\"\"\n        pass\n\n    def insert_fill_value_between_gaps(\n        self,\n        x_agg: np.ndarray,\n        y_agg: np.ndarray,\n        idxs: np.ndarray,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Insert the fill_value in the y_agg array where there are gaps.\n\n        Gaps are determined by the x_agg array. The `_get_gap_mask` method is used to\n        determine a boolean mask indicating the indices where there are gaps.\n\n        Parameters\n        ----------\n        x_agg: np.ndarray\n            The x array. This is used to determine the gaps.\n        y_agg: np.ndarray\n            The y array. A copy of this array will be expanded with fill_values where\n            there are gaps.\n        idxs: np.ndarray\n            The index array. This is relevant aggregators that perform data point\n            selection (e.g., max, min, etc.) - this array will be expanded with the\n            same indices where there are gaps.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            The expanded y_agg array and the expanded idxs array respectively.\n\n        \"\"\"\n        gap_mask = self._get_gap_mask(x_agg)\n        if gap_mask is None:\n            # no gaps are found, nothing to do\n            return y_agg, idxs\n\n        # An array filled with 1s and 2s, where 2 indicates a large gap mask\n        # (i.e., that index will be repeated twice)\n        repeats = np.ones(x_agg.shape, dtype=\"int\") + gap_mask\n\n        # use the repeats to expand the idxs, and agg_y array\n        idx_exp_nan = np.repeat(idxs, repeats)\n        y_agg_exp_nan = np.repeat(y_agg, repeats)\n\n        # only float arrays can contain NaN values\n        if issubclass(y_agg_exp_nan.dtype.type, np.integer) or issubclass(\n            y_agg_exp_nan.dtype.type, np.bool_\n        ):\n            y_agg_exp_nan = y_agg_exp_nan.astype(\"float\")\n\n        # Set the NaN values\n        # We add the gap index offset (via the np.arange) to the indices to account for\n        # the repeats (i.e., expanded y_agg array).\n        y_agg_exp_nan[np.where(gap_mask)[0] + np.arange(gap_mask.sum())] = (\n            self.fill_value\n        )\n\n        return y_agg_exp_nan, idx_exp_nan\n</code></pre>"},{"location":"api/aggregation/#aggregation.AbstractGapHandler.__init__","title":"<code>__init__(fill_value=None)</code>","text":"<p>Constructor of AbstractGapHandler.</p> <p>Parameters:</p> Name Type Description Default <code>fill_value</code> <code>Optional[float]</code> <p>The value to fill the gaps with, by default None. Note that setting this value to 0 for filled area plots is particularly useful.</p> <code>None</code> Source code in <code>plotly_resampler/aggregation/gap_handler_interface.py</code> <pre><code>def __init__(self, fill_value: Optional[float] = None):\n    \"\"\"Constructor of AbstractGapHandler.\n\n    Parameters\n    ----------\n    fill_value: float, optional\n        The value to fill the gaps with, by default None.\n        Note that setting this value to 0 for filled area plots is particularly\n        useful.\n\n    \"\"\"\n    self.fill_value = fill_value\n</code></pre>"},{"location":"api/aggregation/#aggregation.AbstractGapHandler.insert_fill_value_between_gaps","title":"<code>insert_fill_value_between_gaps(x_agg, y_agg, idxs)</code>","text":"<p>Insert the fill_value in the y_agg array where there are gaps.</p> <p>Gaps are determined by the x_agg array. The <code>_get_gap_mask</code> method is used to determine a boolean mask indicating the indices where there are gaps.</p> <p>Parameters:</p> Name Type Description Default <code>x_agg</code> <code>ndarray</code> <p>The x array. This is used to determine the gaps.</p> required <code>y_agg</code> <code>ndarray</code> <p>The y array. A copy of this array will be expanded with fill_values where there are gaps.</p> required <code>idxs</code> <code>ndarray</code> <p>The index array. This is relevant aggregators that perform data point selection (e.g., max, min, etc.) - this array will be expanded with the same indices where there are gaps.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>The expanded y_agg array and the expanded idxs array respectively.</p> Source code in <code>plotly_resampler/aggregation/gap_handler_interface.py</code> <pre><code>def insert_fill_value_between_gaps(\n    self,\n    x_agg: np.ndarray,\n    y_agg: np.ndarray,\n    idxs: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Insert the fill_value in the y_agg array where there are gaps.\n\n    Gaps are determined by the x_agg array. The `_get_gap_mask` method is used to\n    determine a boolean mask indicating the indices where there are gaps.\n\n    Parameters\n    ----------\n    x_agg: np.ndarray\n        The x array. This is used to determine the gaps.\n    y_agg: np.ndarray\n        The y array. A copy of this array will be expanded with fill_values where\n        there are gaps.\n    idxs: np.ndarray\n        The index array. This is relevant aggregators that perform data point\n        selection (e.g., max, min, etc.) - this array will be expanded with the\n        same indices where there are gaps.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        The expanded y_agg array and the expanded idxs array respectively.\n\n    \"\"\"\n    gap_mask = self._get_gap_mask(x_agg)\n    if gap_mask is None:\n        # no gaps are found, nothing to do\n        return y_agg, idxs\n\n    # An array filled with 1s and 2s, where 2 indicates a large gap mask\n    # (i.e., that index will be repeated twice)\n    repeats = np.ones(x_agg.shape, dtype=\"int\") + gap_mask\n\n    # use the repeats to expand the idxs, and agg_y array\n    idx_exp_nan = np.repeat(idxs, repeats)\n    y_agg_exp_nan = np.repeat(y_agg, repeats)\n\n    # only float arrays can contain NaN values\n    if issubclass(y_agg_exp_nan.dtype.type, np.integer) or issubclass(\n        y_agg_exp_nan.dtype.type, np.bool_\n    ):\n        y_agg_exp_nan = y_agg_exp_nan.astype(\"float\")\n\n    # Set the NaN values\n    # We add the gap index offset (via the np.arange) to the indices to account for\n    # the repeats (i.e., expanded y_agg array).\n    y_agg_exp_nan[np.where(gap_mask)[0] + np.arange(gap_mask.sum())] = (\n        self.fill_value\n    )\n\n    return y_agg_exp_nan, idx_exp_nan\n</code></pre>"},{"location":"api/aggregation/#aggregation.EveryNthPoint","title":"<code>EveryNthPoint</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Naive (but fast) aggregator method which returns every N\u2019th point.</p> <p>Note</p> <p>This downsampler supports all dtypes.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class EveryNthPoint(DataPointSelector):\n    \"\"\"Naive (but fast) aggregator method which returns every N'th point.\n\n    !!! note\n        This downsampler supports all dtypes.\n    \"\"\"\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return EveryNthDownsampler().downsample(y, n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/#aggregation.FuncAggregator","title":"<code>FuncAggregator</code>","text":"<p>               Bases: <code>DataAggregator</code></p> <p>Aggregator instance which uses the passed aggregation func.</p> <p>Warning</p> <p>The user has total control which <code>aggregation_func</code> is passed to this method, hence the user should be careful to not make copies of the data, nor write to the data. Furthermore, the user should beware of performance issues when using more complex aggregation functions.</p> <p>Attention</p> <p>The user has total control which <code>aggregation_func</code> is passed to this method, hence it is the users\u2019 responsibility to handle categorical and bool-based data types.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class FuncAggregator(DataAggregator):\n    \"\"\"Aggregator instance which uses the passed aggregation func.\n\n    !!! warning\n\n        The user has total control which `aggregation_func` is passed to this method,\n        hence the user should be careful to not make copies of the data, nor write to\n        the data. Furthermore, the user should beware of performance issues when\n        using more complex aggregation functions.\n\n    !!! warning \"Attention\"\n\n        The user has total control which `aggregation_func` is passed to this method,\n        hence it is the users' responsibility to handle categorical and bool-based\n        data types.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        aggregation_func,\n        x_dtype_regex_list=None,\n        y_dtype_regex_list=None,\n        **downsample_kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        aggregation_func: Callable\n            The aggregation function which will be applied on each pin.\n\n        \"\"\"\n        self.aggregation_func = aggregation_func\n        super().__init__(x_dtype_regex_list, y_dtype_regex_list, **downsample_kwargs)\n\n    def _aggregate(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Aggregate the data using the object's aggregation function.\n\n        Parameters\n        ----------\n        x: np.ndarray | None\n            The x-values of the data. Can be None if no x-values are available.\n        y: np.ndarray\n            The y-values of the data.\n        n_out: int\n            The number of output data points.\n        **kwargs\n            Additional keyword arguments, which are passed to the aggregation function.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            The aggregated x &amp; y values.\n            If `x` is None, then the indices of the first element of each bin is\n            returned as x-values.\n\n        \"\"\"\n        # Create an index-estimation for real-time data\n        # Add one to the index so it's pointed at the end of the window\n        # Note: this can be adjusted to .5 to center the data\n        # Multiply it with the group size to get the real index-position\n        # TODO: add option to select start / middle / end as index\n        if x is None:\n            # equidistant index\n            idxs = np.linspace(0, len(y), n_out + 1).astype(int)\n        else:\n            xdt = x.dtype\n            if np.issubdtype(xdt, np.datetime64) or np.issubdtype(xdt, np.timedelta64):\n                x = x.view(\"int64\")\n            # Thanks to `linspace`, the data is evenly distributed over the index-range\n            # The searchsorted function returns the index positions\n            idxs = np.searchsorted(x, np.linspace(x[0], x[-1], n_out + 1))\n\n        y_agg = np.array(\n            [\n                self.aggregation_func(y[t0:t1], **self.downsample_kwargs)\n                for t0, t1 in zip(idxs[:-1], idxs[1:])\n            ]\n        )\n\n        if x is not None:\n            x_agg = x[idxs[:-1]]\n        else:\n            # x is None -&gt; return the indices of the first element of each bin\n            x_agg = idxs[:-1]\n\n        return x_agg, y_agg\n</code></pre>"},{"location":"api/aggregation/#aggregation.FuncAggregator.__init__","title":"<code>__init__(aggregation_func, x_dtype_regex_list=None, y_dtype_regex_list=None, **downsample_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>aggregation_func</code> <p>The aggregation function which will be applied on each pin.</p> required Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>def __init__(\n    self,\n    aggregation_func,\n    x_dtype_regex_list=None,\n    y_dtype_regex_list=None,\n    **downsample_kwargs,\n):\n    \"\"\"\n    Parameters\n    ----------\n    aggregation_func: Callable\n        The aggregation function which will be applied on each pin.\n\n    \"\"\"\n    self.aggregation_func = aggregation_func\n    super().__init__(x_dtype_regex_list, y_dtype_regex_list, **downsample_kwargs)\n</code></pre>"},{"location":"api/aggregation/#aggregation.LTTB","title":"<code>LTTB</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Largest Triangle Three Buckets (LTTB) aggregation method.</p> <p>This is arguably the most widely used aggregation method. It is based on the effective area of a triangle (inspired from the line simplification domain). The algorithm has $O(n)$ complexity, however, for large datasets, it can be much slower than other algorithms (e.g. MinMax) due to the higher cost of calculating the areas of triangles.</p> <p>Thesis: https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf  Details on visual representativeness &amp; stability: https://arxiv.org/abs/2304.00900</p> <p>Tip</p> <p><code>LTTB</code> doesn\u2019t scale super-well when moving to really large datasets, so when dealing with more than 1 million samples, you might consider using <code>MinMaxLTTB</code>.</p> <p>Note</p> <ul> <li>This class is mainly designed to operate on numerical data as LTTB calculates   distances on the values.    When dealing with categories, the data is encoded into its numeric codes,   these codes are the indices of the category array.</li> <li>To aggregate category data with LTTB, your <code>pd.Series</code> must be of dtype   \u2018category\u2019. </li> </ul> <p>tip:</p> <p>if there is an order in your categories, order them that way, LTTB uses   the ordered category codes values (see bullet above) to calculate distances and   make aggregation decisions.  code:     <pre><code>    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n    &gt;&gt;&gt; cat_type = pd.CategoricalDtype(categories=[\"b\", \"c\", \"a\"], ordered=True)\n    &gt;&gt;&gt; s_cat = s.astype(cat_type)\n</code></pre> * <code>LTTB</code> has no downsample kwargs, as it cannot be paralellized. Instead, you can   use the <code>MinMaxLTTB</code> downsampler, which performs   minmax preselection (in parallel if configured so), followed by LTTB.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class LTTB(DataPointSelector):\n    \"\"\"Largest Triangle Three Buckets (LTTB) aggregation method.\n\n    This is arguably the most widely used aggregation method. It is based on the\n    effective area of a triangle (inspired from the line simplification domain).\n    The algorithm has $O(n)$ complexity, however, for large datasets, it can be much\n    slower than other algorithms (e.g. MinMax) due to the higher cost of calculating\n    the areas of triangles.\n\n    Thesis: [https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf](https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf) &lt;br/&gt;\n    Details on visual representativeness &amp; stability: [https://arxiv.org/abs/2304.00900](https://arxiv.org/abs/2304.00900)\n\n    !!! tip\n\n        `LTTB` doesn't scale super-well when moving to really large datasets, so when\n        dealing with more than 1 million samples, you might consider using\n        [`MinMaxLTTB`][aggregation.aggregators.MinMaxLTTB].\n\n\n    !!! note\n\n        * This class is mainly designed to operate on numerical data as LTTB calculates\n          distances on the values. &lt;br/&gt;\n          When dealing with categories, the data is encoded into its numeric codes,\n          these codes are the indices of the category array.\n        * To aggregate category data with LTTB, your ``pd.Series`` must be of dtype\n          'category'. &lt;br/&gt;\n\n          **tip**:\n\n          if there is an order in your categories, order them that way, LTTB uses\n          the ordered category codes values (see bullet above) to calculate distances and\n          make aggregation decisions. &lt;br/&gt;\n          **code**:\n            ```python\n                &gt;&gt;&gt; import pandas as pd\n                &gt;&gt;&gt; s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n                &gt;&gt;&gt; cat_type = pd.CategoricalDtype(categories=[\"b\", \"c\", \"a\"], ordered=True)\n                &gt;&gt;&gt; s_cat = s.astype(cat_type)\n            ```\n        * `LTTB` has no downsample kwargs, as it cannot be paralellized. Instead, you can\n          use the [`MinMaxLTTB`][aggregation.aggregators.MinMaxLTTB] downsampler, which performs\n          minmax preselection (in parallel if configured so), followed by LTTB.\n\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            y_dtype_regex_list=[rf\"{dtype}\\d*\" for dtype in (\"float\", \"int\", \"uint\")]\n            + [\"category\", \"bool\"],\n        )\n        self.downsampler = LTTBDownsampler()\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return self.downsampler.downsample(*_to_tsdownsample_args(x, y), n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/#aggregation.MedDiffGapHandler","title":"<code>MedDiffGapHandler</code>","text":"<p>               Bases: <code>AbstractGapHandler</code></p> <p>Gap handling based on the median diff of the x_agg array.</p> Source code in <code>plotly_resampler/aggregation/gap_handlers.py</code> <pre><code>class MedDiffGapHandler(AbstractGapHandler):\n    \"\"\"Gap handling based on the median diff of the x_agg array.\"\"\"\n\n    def _calc_med_diff(self, x_agg: np.ndarray) -&gt; Tuple[float, np.ndarray]:\n        \"\"\"Calculate the median diff of the x_agg array.\n\n        As median is more robust to outliers than the mean, the median is used to define\n        the gap threshold.\n\n        This method performs a divide and conquer heuristic to calculate the median;\n        1. divide the array into `n_blocks` blocks (with `n_blocks` = 128)\n        2. calculate the mean of each block\n        3. calculate the median of the means\n        =&gt; This proves to be a good approximation of the median of the full array, while\n              being much faster than calculating the median of the full array.\n        \"\"\"\n        # remark: thanks to the prepend -&gt; x_diff.shape === len(s)\n        x_diff = np.diff(x_agg, prepend=x_agg[0])\n\n        # To do so - use an approach where we reshape the data\n        # into `n_blocks` blocks and calculate the mean and then the median on that\n        # Why use `median` instead of a global mean?\n        #   =&gt; when you have large gaps, they will be represented by a large diff\n        #      which will skew the mean way more than the median!\n        n_blocks = 128\n        if x_agg.shape[0] &gt; 5 * n_blocks:\n            blck_size = x_diff.shape[0] // n_blocks\n\n            # convert the index series index diff into a reshaped view (i.e., sid_v)\n            sid_v: np.ndarray = x_diff[: blck_size * n_blocks].reshape(n_blocks, -1)\n\n            # calculate the mean fore each block and then the median of those means\n            med_diff = np.median(np.mean(sid_v, axis=1))\n        else:\n            med_diff = np.median(x_diff)\n\n        return med_diff, x_diff\n\n    def _get_gap_mask(self, x_agg: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"Get a boolean mask indicating the indices where there are gaps.\n\n        If you require custom gap handling, you can implement this method to return a\n        boolean mask indicating the indices where there are gaps.\n\n        Parameters\n        ----------\n        x_agg: np.ndarray\n            The x array. This is used to determine the gaps.\n\n        Returns\n        -------\n        Optional[np.ndarray]\n            A boolean mask indicating the indices where there are gaps. If there are no\n            gaps, None is returned.\n\n        \"\"\"\n        med_diff, x_diff = self._calc_med_diff(x_agg)\n\n        # TODO: this 4 was revealed to me in a dream, but it seems to work well\n        # After some consideration, we altered this to a 4.1\n        gap_mask = x_diff &gt; 4.1 * med_diff\n        if not any(gap_mask):\n            return\n        return gap_mask\n</code></pre>"},{"location":"api/aggregation/#aggregation.MinMaxAggregator","title":"<code>MinMaxAggregator</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Aggregation method which performs binned min-max aggregation over fully overlapping windows.</p> <p>This is arguably the most computational efficient downsampling method, as it only performs (non-expensive) comparisons on the data in a single pass.</p> <p>Details on visual representativeness &amp; stability: https://arxiv.org/abs/2304.00900</p> <p>Note</p> <p>This method is rather efficient when scaling to large data sizes and can be used as a data-reduction step before feeding it to the <code>LTTB</code> algorithm, as <code>MinMaxLTTB</code> does with the <code>MinMaxOverlapAggregator</code>.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class MinMaxAggregator(DataPointSelector):\n    \"\"\"Aggregation method which performs binned min-max aggregation over fully\n    overlapping windows.\n\n    This is arguably the most computational efficient downsampling method, as it only\n    performs (non-expensive) comparisons on the data in a single pass.\n\n    Details on visual representativeness &amp; stability: [https://arxiv.org/abs/2304.00900](https://arxiv.org/abs/2304.00900)\n\n    !!! note\n\n        This method is rather efficient when scaling to large data sizes and can be used\n        as a data-reduction step before feeding it to the [`LTTB`][aggregation.aggregators.LTTB]\n        algorithm, as [`MinMaxLTTB`][aggregation.aggregators.MinMaxLTTB] does with the\n        [`MinMaxOverlapAggregator`][aggregation.aggregators.MinMaxOverlapAggregator].\n\n    \"\"\"\n\n    def __init__(self, nan_policy=\"omit\", **downsample_kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        **downsample_kwargs\n            Keyword arguments passed to the :class:`MinMaxDownsampler`.\n            - The `parallel` argument is set to False by default.\n        nan_policy: str, optional\n            The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n\n        \"\"\"\n        # this downsampler supports all dtypes\n        super().__init__(**downsample_kwargs)\n        if nan_policy not in (\"omit\", \"keep\"):\n            raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n        if nan_policy == \"omit\":\n            self.downsampler = MinMaxDownsampler()\n        else:\n            self.downsampler = NaNMinMaxDownsampler()\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return self.downsampler.downsample(\n            *_to_tsdownsample_args(x, y), n_out=n_out, **self.downsample_kwargs\n        )\n</code></pre>"},{"location":"api/aggregation/#aggregation.MinMaxAggregator.__init__","title":"<code>__init__(nan_policy='omit', **downsample_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>**downsample_kwargs</code> <p>Keyword arguments passed to the :class:<code>MinMaxDownsampler</code>. - The <code>parallel</code> argument is set to False by default.</p> <code>{}</code> <code>nan_policy</code> <p>The policy to handle NaNs. Can be \u2018omit\u2019 or \u2018keep\u2019. By default, \u2018omit\u2019.</p> <code>'omit'</code> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>def __init__(self, nan_policy=\"omit\", **downsample_kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    **downsample_kwargs\n        Keyword arguments passed to the :class:`MinMaxDownsampler`.\n        - The `parallel` argument is set to False by default.\n    nan_policy: str, optional\n        The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n\n    \"\"\"\n    # this downsampler supports all dtypes\n    super().__init__(**downsample_kwargs)\n    if nan_policy not in (\"omit\", \"keep\"):\n        raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n    if nan_policy == \"omit\":\n        self.downsampler = MinMaxDownsampler()\n    else:\n        self.downsampler = NaNMinMaxDownsampler()\n</code></pre>"},{"location":"api/aggregation/#aggregation.MinMaxLTTB","title":"<code>MinMaxLTTB</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Efficient version off LTTB by first reducing really large datasets with the <code>MinMaxAggregator</code> and then further aggregating the reduced result with <code>LTTB</code>.</p> <p>Starting from 10M data points, this method performs the MinMax-prefetching of data points to enhance computational efficiency.</p> <p>Inventors: Jonas &amp; Jeroen Van Der Donckt - 2022</p> <p>Paper: https://arxiv.org/pdf/2305.00332.pdf</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class MinMaxLTTB(DataPointSelector):\n    \"\"\"Efficient version off LTTB by first reducing really large datasets with\n    the [`MinMaxAggregator`][aggregation.aggregators.MinMaxAggregator] and then further aggregating the\n    reduced result with [`LTTB`][aggregation.aggregators.LTTB].\n\n    Starting from 10M data points, this method performs the MinMax-prefetching of data\n    points to enhance computational efficiency.\n\n    Inventors: Jonas &amp; Jeroen Van Der Donckt - 2022\n\n    Paper: [https://arxiv.org/pdf/2305.00332.pdf](https://arxiv.org/pdf/2305.00332.pdf)\n    \"\"\"\n\n    def __init__(\n        self, minmax_ratio: int = 4, nan_policy: str = \"omit\", **downsample_kwargs\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        minmax_ratio: int, optional\n            The ratio between the number of data points in the MinMax-prefetching and\n            the number of data points that will be outputted by LTTB. By default, 4.\n        nan_policy: str, optional\n            The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n        **downsample_kwargs\n            Keyword arguments passed to the `MinMaxLTTBDownsampler`.\n            - The `parallel` argument is set to False by default.\n            - The `minmax_ratio` argument is set to 4 by default, which was empirically\n              proven to be a good default.\n\n        \"\"\"\n        if nan_policy not in (\"omit\", \"keep\"):\n            raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n        if nan_policy == \"omit\":\n            self.minmaxlttb = MinMaxLTTBDownsampler()\n        else:\n            self.minmaxlttb = NaNMinMaxLTTBDownsampler()\n\n        self.minmax_ratio = minmax_ratio\n\n        super().__init__(\n            y_dtype_regex_list=[rf\"{dtype}\\d*\" for dtype in (\"float\", \"int\", \"uint\")]\n            + [\"category\", \"bool\"],\n            **downsample_kwargs,\n        )\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return self.minmaxlttb.downsample(\n            *_to_tsdownsample_args(x, y),\n            n_out=n_out,\n            minmax_ratio=self.minmax_ratio,\n            **self.downsample_kwargs,\n        )\n</code></pre>"},{"location":"api/aggregation/#aggregation.MinMaxLTTB.__init__","title":"<code>__init__(minmax_ratio=4, nan_policy='omit', **downsample_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>minmax_ratio</code> <code>int</code> <p>The ratio between the number of data points in the MinMax-prefetching and the number of data points that will be outputted by LTTB. By default, 4.</p> <code>4</code> <code>nan_policy</code> <code>str</code> <p>The policy to handle NaNs. Can be \u2018omit\u2019 or \u2018keep\u2019. By default, \u2018omit\u2019.</p> <code>'omit'</code> <code>**downsample_kwargs</code> <p>Keyword arguments passed to the <code>MinMaxLTTBDownsampler</code>. - The <code>parallel</code> argument is set to False by default. - The <code>minmax_ratio</code> argument is set to 4 by default, which was empirically   proven to be a good default.</p> <code>{}</code> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>def __init__(\n    self, minmax_ratio: int = 4, nan_policy: str = \"omit\", **downsample_kwargs\n):\n    \"\"\"\n    Parameters\n    ----------\n    minmax_ratio: int, optional\n        The ratio between the number of data points in the MinMax-prefetching and\n        the number of data points that will be outputted by LTTB. By default, 4.\n    nan_policy: str, optional\n        The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n    **downsample_kwargs\n        Keyword arguments passed to the `MinMaxLTTBDownsampler`.\n        - The `parallel` argument is set to False by default.\n        - The `minmax_ratio` argument is set to 4 by default, which was empirically\n          proven to be a good default.\n\n    \"\"\"\n    if nan_policy not in (\"omit\", \"keep\"):\n        raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n    if nan_policy == \"omit\":\n        self.minmaxlttb = MinMaxLTTBDownsampler()\n    else:\n        self.minmaxlttb = NaNMinMaxLTTBDownsampler()\n\n    self.minmax_ratio = minmax_ratio\n\n    super().__init__(\n        y_dtype_regex_list=[rf\"{dtype}\\d*\" for dtype in (\"float\", \"int\", \"uint\")]\n        + [\"category\", \"bool\"],\n        **downsample_kwargs,\n    )\n</code></pre>"},{"location":"api/aggregation/#aggregation.MinMaxOverlapAggregator","title":"<code>MinMaxOverlapAggregator</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Aggregation method which performs binned min-max aggregation over 50% overlapping windows.</p> <p></p> <p>In the above image, bin_size: represents the size of (len(series) / n_out). As the windows have 50% overlap and are consecutive, the min &amp; max values are calculated on a windows with size (2x bin-size).</p> <p>This is very similar to the MinMaxAggregator, emperical results showed no observable difference between both approaches.</p> <p>Note</p> <p>This method is implemented in Python (leveraging numpy for vecotrization), but is significantly slower than the MinMaxAggregator (which is implemented in the tsdownsample toolkit in Rust).  As such, this class does not support any downsample kwargs.</p> <p>Note</p> <p>This downsampler supports all dtypes.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class MinMaxOverlapAggregator(DataPointSelector):\n    \"\"\"Aggregation method which performs binned min-max aggregation over 50% overlapping\n    windows.\n\n    ![minmax operator image](https://github.com/predict-idlab/plotly-resampler/blob/main/mkdocs/static/minmax_operator.png)\n\n    In the above image, **bin_size**: represents the size of *(len(series) / n_out)*.\n    As the windows have 50% overlap and are consecutive, the min &amp; max values are\n    calculated on a windows with size (2x bin-size).\n\n    This is *very* similar to the MinMaxAggregator, emperical results showed no\n    observable difference between both approaches.\n\n    !!! note\n\n        This method is implemented in Python (leveraging numpy for vecotrization), but\n        is **significantly slower than the MinMaxAggregator** (which is implemented in\n        the tsdownsample toolkit in Rust). &lt;br/&gt;\n        As such, this class does not support any downsample kwargs.\n\n    !!! note\n\n        This downsampler supports all dtypes.\n\n    \"\"\"\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        # The block size 2x the bin size we also perform the ceil-operation\n        # to ensure that the block_size * n_out / 2 &lt; len(x)\n        block_size = math.ceil(y.shape[0] / (n_out + 1) * 2)\n        argmax_offset = block_size // 2\n\n        # Calculate the offset range which will be added to the argmin and argmax pos\n        offset = np.arange(\n            0, stop=y.shape[0] - block_size - argmax_offset, step=block_size\n        )\n\n        # Calculate the argmin &amp; argmax on the reshaped view of `y` &amp;\n        # add the corresponding offset\n        argmin = (\n            y[: block_size * offset.shape[0]].reshape(-1, block_size).argmin(axis=1)\n            + offset\n        )\n        argmax = (\n            y[argmax_offset : block_size * offset.shape[0] + argmax_offset]\n            .reshape(-1, block_size)\n            .argmax(axis=1)\n            + offset\n            + argmax_offset\n        )\n\n        # Sort the argmin &amp; argmax (where we append the first and last index item)\n        return np.unique(np.concatenate((argmin, argmax, [0, y.shape[0] - 1])))\n</code></pre>"},{"location":"api/aggregation/#aggregation.NoGapHandler","title":"<code>NoGapHandler</code>","text":"<p>               Bases: <code>AbstractGapHandler</code></p> <p>No gap handling.</p> Source code in <code>plotly_resampler/aggregation/gap_handlers.py</code> <pre><code>class NoGapHandler(AbstractGapHandler):\n    \"\"\"No gap handling.\"\"\"\n\n    def _get_gap_mask(self, x_agg: np.ndarray) -&gt; Optional[np.ndarray]:\n        return\n</code></pre>"},{"location":"api/aggregation/#aggregation.PlotlyAggregatorParser","title":"<code>PlotlyAggregatorParser</code>","text":"Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>class PlotlyAggregatorParser:\n    @staticmethod\n    def parse_hf_data(\n        hf_data: np.ndarray | pd.Categorical | pd.Series | pd.Index,\n    ) -&gt; np.ndarray | pd.Categorical:\n        \"\"\"Parse the high-frequency data to a numpy array.\"\"\"\n        # Categorical data (pandas)\n        #   - pd.Series with categorical dtype -&gt; calling .values will returns a\n        #       pd.Categorical\n        #   - pd.CategoricalIndex -&gt; calling .values returns a pd.Categorical\n        #   - pd.Categorical: has no .values attribute -&gt; will not be parsed\n        if isinstance(hf_data, pd.RangeIndex):\n            return None\n        if isinstance(hf_data, (pd.Series, pd.Index)):\n            return hf_data.values\n        return hf_data\n\n    @staticmethod\n    def to_same_tz(\n        ts: Union[pd.Timestamp, None], reference_tz: Union[pytz.BaseTzInfo, None]\n    ) -&gt; Union[pd.Timestamp, None]:\n        \"\"\"Adjust `ts` its timezone to the `reference_tz`.\"\"\"\n        if ts is None:\n            return None\n        elif reference_tz is not None:\n            if ts.tz is not None:\n                # compare if these two have the same timezone / offset\n                try:\n                    assert ts.tz.__str__() == reference_tz.__str__()\n                except AssertionError:\n                    assert ts.utcoffset() == reference_tz.utcoffset(ts.tz_convert(None))\n                return ts\n            else:  # localize -&gt; time remains the same\n                return ts.tz_localize(reference_tz)\n        elif reference_tz is None and ts.tz is not None:\n            return ts.tz_localize(None)\n        return ts\n\n    @staticmethod\n    def get_start_end_indices(hf_trace_data, axis_type, start, end) -&gt; Tuple[int, int]:\n        \"\"\"Get the start &amp; end indices of the high-frequency data.\"\"\"\n        # Base case: no hf data, or both start &amp; end are None\n        if not len(hf_trace_data[\"x\"]):\n            return 0, 0\n        elif start is None and end is None:\n            return 0, len(hf_trace_data[\"x\"])\n\n        # NOTE: as we use bisect right for the end index, we do not need to add a\n        #      small epsilon to the end value\n        start = hf_trace_data[\"x\"][0] if start is None else start\n        end = hf_trace_data[\"x\"][-1] if end is None else end\n\n        # NOTE: we must verify this before check if the x is a range-index\n        if axis_type == \"log\":\n            start, end = 10**start, 10**end\n\n        # We can compute the start &amp; end indices directly when it is a RangeIndex\n        if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n            x_start = hf_trace_data[\"x\"].start\n            x_step = hf_trace_data[\"x\"].step\n            start_idx = int(max((start - x_start) // x_step, 0))\n            end_idx = int((end - x_start) // x_step)\n            return start_idx, end_idx\n        # TODO: this can be performed as-well for a fixed frequency range-index w/ freq\n\n        if axis_type == \"date\":\n            start, end = pd.to_datetime(start), pd.to_datetime(end)\n            # convert start &amp; end to the same timezone\n            if isinstance(hf_trace_data[\"x\"], pd.DatetimeIndex):\n                tz = hf_trace_data[\"x\"].tz\n                try:\n                    assert start.tz.__str__() == end.tz.__str__()\n                except (TypeError, AssertionError):\n                    # This fix is needed for DST (when the timezone is not fixed)\n                    assert start.tz_localize(None) == start.tz_convert(tz).tz_localize(\n                        None\n                    )\n                    assert end.tz_localize(None) == end.tz_convert(tz).tz_localize(None)\n\n                start = PlotlyAggregatorParser.to_same_tz(start, tz)\n                end = PlotlyAggregatorParser.to_same_tz(end, tz)\n\n        # Search the index-positions\n        start_idx = bisect.bisect_left(hf_trace_data[\"x\"], start)\n        end_idx = bisect.bisect_right(hf_trace_data[\"x\"], end)\n        return start_idx, end_idx\n\n    @staticmethod\n    def _handle_gaps(\n        hf_trace_data: dict,\n        hf_x: np.ndarray,\n        agg_x: np.ndarray,\n        agg_y: np.ndarray,\n        indices: np.ndarray,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Handle the gaps in the aggregated data.\n\n        Returns:\n            - agg_x: the aggregated x-values\n            - agg_y: the aggregated y-values\n            - indices: the indices of the hf_data data that were aggregated\n\n        \"\"\"\n        gap_handler: AbstractGapHandler = hf_trace_data[\"gap_handler\"]\n        downsampler = hf_trace_data[\"downsampler\"]\n\n        # TODO check for trace mode (markers, lines, etc.) and only perform the\n        # gap insertion methodology when the mode is lines.\n        # if trace.get(\"connectgaps\") != True and\n        if (\n            isinstance(gap_handler, NoGapHandler)\n            # rangeIndex | datetimeIndex with freq -&gt; equally spaced x; so no gaps\n            or isinstance(hf_trace_data[\"x\"], pd.RangeIndex)\n            or (\n                isinstance(hf_trace_data[\"x\"], pd.DatetimeIndex)\n                and hf_trace_data[\"x\"].freq is not None\n            )\n        ):\n            return agg_x, agg_y, indices\n\n        # Interleave the gaps\n        # View the data as an int64 when we have a DatetimeIndex\n        # We only want to detect gaps, so we only want to compare values.\n        agg_x_parsed = PlotlyAggregatorParser.parse_hf_data(agg_x)\n        xdt = agg_x_parsed.dtype\n        if np.issubdtype(xdt, np.timedelta64) or np.issubdtype(xdt, np.datetime64):\n            agg_x_parsed = agg_x_parsed.view(\"int64\")\n\n        agg_y, indices = gap_handler.insert_fill_value_between_gaps(\n            agg_x_parsed, agg_y, indices\n        )\n        if isinstance(downsampler, DataPointSelector):\n            agg_x = hf_x[indices]\n        elif isinstance(downsampler, DataAggregator):\n            # The indices are in this case a repeat\n            agg_x = agg_x[indices]\n\n        return agg_x, agg_y, indices\n\n    @staticmethod\n    def aggregate(\n        hf_trace_data: dict,\n        start_idx: int,\n        end_idx: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Aggregate the data in `hf_trace_data` between `start_idx` and `end_idx`.\n\n        Returns:\n            - x: the aggregated x-values\n            - y: the aggregated y-values\n            - indices: the indices of the hf_data data that were aggregated\n\n            These indices are useful to select the corresponding hf_data from\n            non `x` and `y` data (e.g. `text`, `marker_size`, `marker_color`).\n\n        \"\"\"\n        hf_x = hf_trace_data[\"x\"][start_idx:end_idx]\n        hf_y = hf_trace_data[\"y\"][start_idx:end_idx]\n\n        # No downsampling needed ; we show the raw data as is, but with gap-detection\n        if (end_idx - start_idx) &lt;= hf_trace_data[\"max_n_samples\"]:\n            indices = np.arange(len(hf_y))  # no downsampling - all values are selected\n            if len(indices):\n                return PlotlyAggregatorParser._handle_gaps(\n                    hf_trace_data, hf_x=hf_x, agg_x=hf_x, agg_y=hf_y, indices=indices\n                )\n            else:\n                return hf_x, hf_y, indices\n\n        downsampler = hf_trace_data[\"downsampler\"]\n\n        hf_x_parsed = PlotlyAggregatorParser.parse_hf_data(hf_x)\n        hf_y_parsed = PlotlyAggregatorParser.parse_hf_data(hf_y)\n\n        if isinstance(downsampler, DataPointSelector):\n            s_v = hf_y_parsed\n            if isinstance(s_v, pd.Categorical):  # pd.Categorical (has no .values)\n                s_v = s_v.codes\n            indices = downsampler.arg_downsample(\n                hf_x_parsed,\n                s_v,\n                n_out=hf_trace_data[\"max_n_samples\"],\n                **hf_trace_data.get(\"downsampler_kwargs\", {}),\n            )\n            if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n                # we avoid slicing the default pd.RangeIndex (as this is not an\n                # in-memory array) - this proves to be faster than slicing the index.\n                agg_x = (\n                    start_idx\n                    + hf_trace_data[\"x\"].start\n                    + indices.astype(hf_trace_data[\"x\"].dtype) * hf_trace_data[\"x\"].step\n                )\n            else:\n                agg_x = hf_x[indices]\n            agg_y = hf_y[indices]\n        elif isinstance(downsampler, DataAggregator):\n            agg_x, agg_y = downsampler.aggregate(\n                hf_x_parsed,\n                hf_y_parsed,\n                n_out=hf_trace_data[\"max_n_samples\"],\n                **hf_trace_data.get(\"downsampler_kwargs\", {}),\n            )\n            if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n                # we avoid slicing the default pd.RangeIndex (as this is not an\n                # in-memory array) - this proves to be faster than slicing the index.\n                agg_x = (\n                    start_idx\n                    + hf_trace_data[\"x\"].start\n                    + agg_x * hf_trace_data[\"x\"].step\n                )\n            # The indices are just the range of the aggregated data\n            indices = np.arange(len(agg_x))\n        else:\n            raise ValueError(\n                \"Invalid downsampler instance, must be either a \"\n                + f\"DataAggregator or a DataPointSelector, got {type(downsampler)}\"\n            )\n\n        return PlotlyAggregatorParser._handle_gaps(\n            hf_trace_data, hf_x=hf_x, agg_x=agg_x, agg_y=agg_y, indices=indices\n        )\n</code></pre>"},{"location":"api/aggregation/#aggregation.PlotlyAggregatorParser.aggregate","title":"<code>aggregate(hf_trace_data, start_idx, end_idx)</code>  <code>staticmethod</code>","text":"<p>Aggregate the data in <code>hf_trace_data</code> between <code>start_idx</code> and <code>end_idx</code>.</p> <p>Returns:     - x: the aggregated x-values     - y: the aggregated y-values     - indices: the indices of the hf_data data that were aggregated</p> <pre><code>These indices are useful to select the corresponding hf_data from\nnon `x` and `y` data (e.g. `text`, `marker_size`, `marker_color`).\n</code></pre> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef aggregate(\n    hf_trace_data: dict,\n    start_idx: int,\n    end_idx: int,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Aggregate the data in `hf_trace_data` between `start_idx` and `end_idx`.\n\n    Returns:\n        - x: the aggregated x-values\n        - y: the aggregated y-values\n        - indices: the indices of the hf_data data that were aggregated\n\n        These indices are useful to select the corresponding hf_data from\n        non `x` and `y` data (e.g. `text`, `marker_size`, `marker_color`).\n\n    \"\"\"\n    hf_x = hf_trace_data[\"x\"][start_idx:end_idx]\n    hf_y = hf_trace_data[\"y\"][start_idx:end_idx]\n\n    # No downsampling needed ; we show the raw data as is, but with gap-detection\n    if (end_idx - start_idx) &lt;= hf_trace_data[\"max_n_samples\"]:\n        indices = np.arange(len(hf_y))  # no downsampling - all values are selected\n        if len(indices):\n            return PlotlyAggregatorParser._handle_gaps(\n                hf_trace_data, hf_x=hf_x, agg_x=hf_x, agg_y=hf_y, indices=indices\n            )\n        else:\n            return hf_x, hf_y, indices\n\n    downsampler = hf_trace_data[\"downsampler\"]\n\n    hf_x_parsed = PlotlyAggregatorParser.parse_hf_data(hf_x)\n    hf_y_parsed = PlotlyAggregatorParser.parse_hf_data(hf_y)\n\n    if isinstance(downsampler, DataPointSelector):\n        s_v = hf_y_parsed\n        if isinstance(s_v, pd.Categorical):  # pd.Categorical (has no .values)\n            s_v = s_v.codes\n        indices = downsampler.arg_downsample(\n            hf_x_parsed,\n            s_v,\n            n_out=hf_trace_data[\"max_n_samples\"],\n            **hf_trace_data.get(\"downsampler_kwargs\", {}),\n        )\n        if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n            # we avoid slicing the default pd.RangeIndex (as this is not an\n            # in-memory array) - this proves to be faster than slicing the index.\n            agg_x = (\n                start_idx\n                + hf_trace_data[\"x\"].start\n                + indices.astype(hf_trace_data[\"x\"].dtype) * hf_trace_data[\"x\"].step\n            )\n        else:\n            agg_x = hf_x[indices]\n        agg_y = hf_y[indices]\n    elif isinstance(downsampler, DataAggregator):\n        agg_x, agg_y = downsampler.aggregate(\n            hf_x_parsed,\n            hf_y_parsed,\n            n_out=hf_trace_data[\"max_n_samples\"],\n            **hf_trace_data.get(\"downsampler_kwargs\", {}),\n        )\n        if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n            # we avoid slicing the default pd.RangeIndex (as this is not an\n            # in-memory array) - this proves to be faster than slicing the index.\n            agg_x = (\n                start_idx\n                + hf_trace_data[\"x\"].start\n                + agg_x * hf_trace_data[\"x\"].step\n            )\n        # The indices are just the range of the aggregated data\n        indices = np.arange(len(agg_x))\n    else:\n        raise ValueError(\n            \"Invalid downsampler instance, must be either a \"\n            + f\"DataAggregator or a DataPointSelector, got {type(downsampler)}\"\n        )\n\n    return PlotlyAggregatorParser._handle_gaps(\n        hf_trace_data, hf_x=hf_x, agg_x=agg_x, agg_y=agg_y, indices=indices\n    )\n</code></pre>"},{"location":"api/aggregation/#aggregation.PlotlyAggregatorParser.get_start_end_indices","title":"<code>get_start_end_indices(hf_trace_data, axis_type, start, end)</code>  <code>staticmethod</code>","text":"<p>Get the start &amp; end indices of the high-frequency data.</p> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef get_start_end_indices(hf_trace_data, axis_type, start, end) -&gt; Tuple[int, int]:\n    \"\"\"Get the start &amp; end indices of the high-frequency data.\"\"\"\n    # Base case: no hf data, or both start &amp; end are None\n    if not len(hf_trace_data[\"x\"]):\n        return 0, 0\n    elif start is None and end is None:\n        return 0, len(hf_trace_data[\"x\"])\n\n    # NOTE: as we use bisect right for the end index, we do not need to add a\n    #      small epsilon to the end value\n    start = hf_trace_data[\"x\"][0] if start is None else start\n    end = hf_trace_data[\"x\"][-1] if end is None else end\n\n    # NOTE: we must verify this before check if the x is a range-index\n    if axis_type == \"log\":\n        start, end = 10**start, 10**end\n\n    # We can compute the start &amp; end indices directly when it is a RangeIndex\n    if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n        x_start = hf_trace_data[\"x\"].start\n        x_step = hf_trace_data[\"x\"].step\n        start_idx = int(max((start - x_start) // x_step, 0))\n        end_idx = int((end - x_start) // x_step)\n        return start_idx, end_idx\n    # TODO: this can be performed as-well for a fixed frequency range-index w/ freq\n\n    if axis_type == \"date\":\n        start, end = pd.to_datetime(start), pd.to_datetime(end)\n        # convert start &amp; end to the same timezone\n        if isinstance(hf_trace_data[\"x\"], pd.DatetimeIndex):\n            tz = hf_trace_data[\"x\"].tz\n            try:\n                assert start.tz.__str__() == end.tz.__str__()\n            except (TypeError, AssertionError):\n                # This fix is needed for DST (when the timezone is not fixed)\n                assert start.tz_localize(None) == start.tz_convert(tz).tz_localize(\n                    None\n                )\n                assert end.tz_localize(None) == end.tz_convert(tz).tz_localize(None)\n\n            start = PlotlyAggregatorParser.to_same_tz(start, tz)\n            end = PlotlyAggregatorParser.to_same_tz(end, tz)\n\n    # Search the index-positions\n    start_idx = bisect.bisect_left(hf_trace_data[\"x\"], start)\n    end_idx = bisect.bisect_right(hf_trace_data[\"x\"], end)\n    return start_idx, end_idx\n</code></pre>"},{"location":"api/aggregation/#aggregation.PlotlyAggregatorParser.parse_hf_data","title":"<code>parse_hf_data(hf_data)</code>  <code>staticmethod</code>","text":"<p>Parse the high-frequency data to a numpy array.</p> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef parse_hf_data(\n    hf_data: np.ndarray | pd.Categorical | pd.Series | pd.Index,\n) -&gt; np.ndarray | pd.Categorical:\n    \"\"\"Parse the high-frequency data to a numpy array.\"\"\"\n    # Categorical data (pandas)\n    #   - pd.Series with categorical dtype -&gt; calling .values will returns a\n    #       pd.Categorical\n    #   - pd.CategoricalIndex -&gt; calling .values returns a pd.Categorical\n    #   - pd.Categorical: has no .values attribute -&gt; will not be parsed\n    if isinstance(hf_data, pd.RangeIndex):\n        return None\n    if isinstance(hf_data, (pd.Series, pd.Index)):\n        return hf_data.values\n    return hf_data\n</code></pre>"},{"location":"api/aggregation/#aggregation.PlotlyAggregatorParser.to_same_tz","title":"<code>to_same_tz(ts, reference_tz)</code>  <code>staticmethod</code>","text":"<p>Adjust <code>ts</code> its timezone to the <code>reference_tz</code>.</p> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef to_same_tz(\n    ts: Union[pd.Timestamp, None], reference_tz: Union[pytz.BaseTzInfo, None]\n) -&gt; Union[pd.Timestamp, None]:\n    \"\"\"Adjust `ts` its timezone to the `reference_tz`.\"\"\"\n    if ts is None:\n        return None\n    elif reference_tz is not None:\n        if ts.tz is not None:\n            # compare if these two have the same timezone / offset\n            try:\n                assert ts.tz.__str__() == reference_tz.__str__()\n            except AssertionError:\n                assert ts.utcoffset() == reference_tz.utcoffset(ts.tz_convert(None))\n            return ts\n        else:  # localize -&gt; time remains the same\n            return ts.tz_localize(reference_tz)\n    elif reference_tz is None and ts.tz is not None:\n        return ts.tz_localize(None)\n    return ts\n</code></pre>"},{"location":"api/aggregation/aggregation_interface/","title":"aggregation_interface","text":"<p>AbstractAggregator interface-class, subclassed by concrete aggregators.</p>"},{"location":"api/aggregation/aggregation_interface/#aggregation.aggregation_interface.AbstractAggregator","title":"<code>AbstractAggregator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>class AbstractAggregator(ABC):\n    def __init__(\n        self,\n        x_dtype_regex_list: Optional[List[str]] = None,\n        y_dtype_regex_list: Optional[List[str]] = None,\n        **downsample_kwargs,\n    ):\n        \"\"\"Constructor of AbstractSeriesAggregator.\n\n        Parameters\n        ----------\n        x_dtype_regex_list: List[str], optional\n            List containing the regex matching the supported datatypes for the x array,\n            by default None.\n        y_dtype_regex_list: List[str], optional\n            List containing the regex matching the supported datatypes for the y array,\n            by default None.\n        downsample_kwargs: dict\n            Additional kwargs passed to the downsample method.\n\n        \"\"\"\n        self.x_dtype_regex_list = x_dtype_regex_list\n        self.y_dtype_regex_list = y_dtype_regex_list\n        self.downsample_kwargs = downsample_kwargs\n\n    @staticmethod\n    def _check_n_out(n_out: int) -&gt; None:\n        \"\"\"Check if the n_out is valid.\"\"\"\n        assert isinstance(n_out, (int, np.integer))\n        assert n_out &gt; 0\n\n    @staticmethod\n    def _process_args(*args) -&gt; Tuple[np.ndarray | None, np.ndarray]:\n        \"\"\"Process the args into the x and y arrays.\n\n        If only y is passed, x is set to None.\n        \"\"\"\n        assert len(args) in [1, 2], \"Must pass either 1 or 2 arrays\"\n        x, y = (None, args[0]) if len(args) == 1 else args\n        return x, y\n\n    @staticmethod\n    def _check_arr(arr: np.ndarray, regex_list: Optional[List[str]] = None):\n        \"\"\"Check if the array is valid.\"\"\"\n        assert isinstance(arr, np.ndarray), f\"Expected np.ndarray, got {type(arr)}\"\n        assert arr.ndim == 1\n        AbstractAggregator._supports_dtype(arr, regex_list)\n\n    def _check_x_y(self, x: np.ndarray | None, y: np.ndarray) -&gt; None:\n        \"\"\"Check if the x and y arrays are valid.\"\"\"\n        # Check x (if not None)\n        if x is not None:\n            self._check_arr(x, self.x_dtype_regex_list)\n            assert x.shape == y.shape, \"x and y must have the same shape\"\n        # Check y\n        self._check_arr(y, self.y_dtype_regex_list)\n\n    @staticmethod\n    def _supports_dtype(arr: np.ndarray, dtype_regex_list: Optional[List[str]] = None):\n        # base case\n        if dtype_regex_list is None:\n            return\n\n        for dtype_regex_str in dtype_regex_list:\n            m = re.compile(dtype_regex_str).match(str(arr.dtype))\n            if m is not None:  # a match is found\n                return\n        raise ValueError(\n            f\"{arr.dtype} doesn't match with any regex in {dtype_regex_list}\"\n        )\n</code></pre>"},{"location":"api/aggregation/aggregation_interface/#aggregation.aggregation_interface.AbstractAggregator.__init__","title":"<code>__init__(x_dtype_regex_list=None, y_dtype_regex_list=None, **downsample_kwargs)</code>","text":"<p>Constructor of AbstractSeriesAggregator.</p> <p>Parameters:</p> Name Type Description Default <code>x_dtype_regex_list</code> <code>Optional[List[str]]</code> <p>List containing the regex matching the supported datatypes for the x array, by default None.</p> <code>None</code> <code>y_dtype_regex_list</code> <code>Optional[List[str]]</code> <p>List containing the regex matching the supported datatypes for the y array, by default None.</p> <code>None</code> <code>downsample_kwargs</code> <p>Additional kwargs passed to the downsample method.</p> <code>{}</code> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>def __init__(\n    self,\n    x_dtype_regex_list: Optional[List[str]] = None,\n    y_dtype_regex_list: Optional[List[str]] = None,\n    **downsample_kwargs,\n):\n    \"\"\"Constructor of AbstractSeriesAggregator.\n\n    Parameters\n    ----------\n    x_dtype_regex_list: List[str], optional\n        List containing the regex matching the supported datatypes for the x array,\n        by default None.\n    y_dtype_regex_list: List[str], optional\n        List containing the regex matching the supported datatypes for the y array,\n        by default None.\n    downsample_kwargs: dict\n        Additional kwargs passed to the downsample method.\n\n    \"\"\"\n    self.x_dtype_regex_list = x_dtype_regex_list\n    self.y_dtype_regex_list = y_dtype_regex_list\n    self.downsample_kwargs = downsample_kwargs\n</code></pre>"},{"location":"api/aggregation/aggregation_interface/#aggregation.aggregation_interface.DataAggregator","title":"<code>DataAggregator</code>","text":"<p>               Bases: <code>AbstractAggregator</code>, <code>ABC</code></p> <p>Implementation of the AbstractAggregator interface for data aggregation.</p> <p>DataAggregator differs from DataPointSelector in that it doesn\u2019t select data points, but rather aggregates the data (e.g., mean). As such, the <code>_aggregate</code> method is responsible for aggregating the data, and thus returns a tuple of the aggregated x and y values.</p> <p>Concrete implementations of this class must implement the <code>_aggregate</code> method, and have full responsibility on how they deal with other high-frequency properties, such as <code>hovertext</code>, <code>marker_size</code>, \u2018marker_color`, etc \u2026</p> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>class DataAggregator(AbstractAggregator, ABC):\n    \"\"\"Implementation of the AbstractAggregator interface for data aggregation.\n\n    DataAggregator differs from DataPointSelector in that it doesn't select data points,\n    but rather aggregates the data (e.g., mean).\n    As such, the `_aggregate` method is responsible for aggregating the data, and thus\n    returns a tuple of the aggregated x and y values.\n\n    Concrete implementations of this class must implement the `_aggregate` method, and\n    have full responsibility on how they deal with other high-frequency properties, such\n    as `hovertext`, `marker_size`, 'marker_color`, etc ...\n    \"\"\"\n\n    @abstractmethod\n    def _aggregate(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        raise NotImplementedError\n\n    def aggregate(\n        self,\n        *args,\n        n_out: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Aggregate the data.\n\n        Parameters\n        ----------\n        x, y: np.ndarray\n            The x and y data of the to-be-aggregated series.\n            The x array is optional (i.e., if only 1 array is passed, it is assumed to\n            be the y array).\n            The array(s) must be 1-dimensional, and have the same length (if x is\n            passed).\n            These cannot be passed as keyword arguments, as they are positional-only.\n        n_out: int\n            The number of samples which the downsampled series should contain.\n            This should be passed as a keyword argument.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            The aggregated x and y data, respectively.\n\n        \"\"\"\n        # Check n_out\n        assert n_out is not None\n\n        # Get x and y\n        x, y = DataPointSelector._process_args(*args)\n\n        # Check x and y\n        self._check_x_y(x, y)\n\n        return self._aggregate(x=x, y=y, n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/aggregation_interface/#aggregation.aggregation_interface.DataAggregator.aggregate","title":"<code>aggregate(*args, n_out)</code>","text":"<p>Aggregate the data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The x and y data of the to-be-aggregated series. The x array is optional (i.e., if only 1 array is passed, it is assumed to be the y array). The array(s) must be 1-dimensional, and have the same length (if x is passed). These cannot be passed as keyword arguments, as they are positional-only.</p> required <code>y</code> <p>The x and y data of the to-be-aggregated series. The x array is optional (i.e., if only 1 array is passed, it is assumed to be the y array). The array(s) must be 1-dimensional, and have the same length (if x is passed). These cannot be passed as keyword arguments, as they are positional-only.</p> required <code>n_out</code> <code>int</code> <p>The number of samples which the downsampled series should contain. This should be passed as a keyword argument.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>The aggregated x and y data, respectively.</p> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>def aggregate(\n    self,\n    *args,\n    n_out: int,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Aggregate the data.\n\n    Parameters\n    ----------\n    x, y: np.ndarray\n        The x and y data of the to-be-aggregated series.\n        The x array is optional (i.e., if only 1 array is passed, it is assumed to\n        be the y array).\n        The array(s) must be 1-dimensional, and have the same length (if x is\n        passed).\n        These cannot be passed as keyword arguments, as they are positional-only.\n    n_out: int\n        The number of samples which the downsampled series should contain.\n        This should be passed as a keyword argument.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        The aggregated x and y data, respectively.\n\n    \"\"\"\n    # Check n_out\n    assert n_out is not None\n\n    # Get x and y\n    x, y = DataPointSelector._process_args(*args)\n\n    # Check x and y\n    self._check_x_y(x, y)\n\n    return self._aggregate(x=x, y=y, n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/aggregation_interface/#aggregation.aggregation_interface.DataPointSelector","title":"<code>DataPointSelector</code>","text":"<p>               Bases: <code>AbstractAggregator</code>, <code>ABC</code></p> <p>Implementation of the AbstractAggregator interface for data point selection.</p> <p>DataPointSelector differs from DataAggregator in that they don\u2019t aggregate the data (e.g., mean) but instead select data points (e.g., first, last, min, max, etc \u2026). As such, the <code>_arg_downsample</code> method returns the index positions of the selected data points.</p> <p>This class utilizes the <code>arg_downsample</code> method to compute the index positions.</p> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>class DataPointSelector(AbstractAggregator, ABC):\n    \"\"\"Implementation of the AbstractAggregator interface for data point selection.\n\n    DataPointSelector differs from DataAggregator in that they don't aggregate the data\n    (e.g., mean) but instead select data points (e.g., first, last, min, max, etc ...).\n    As such, the `_arg_downsample` method returns the index positions of the selected\n    data points.\n\n    This class utilizes the `arg_downsample` method to compute the index positions.\n    \"\"\"\n\n    @abstractmethod\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        # Note: this method can utilize the self.downsample_kwargs property\n        raise NotImplementedError\n\n    def arg_downsample(\n        self,\n        *args,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the index positions for the downsampled representation.\n\n        Parameters\n        ----------\n        x, y: np.ndarray\n            The x and y data of the to-be-aggregated series.\n            The x array is optional (i.e., if only 1 array is passed, it is assumed to\n            be the y array).\n            The array(s) must be 1-dimensional, and have the same length (if x is\n            passed).\n            These cannot be passed as keyword arguments, as they are positional-only.\n        n_out: int\n            The number of samples which the downsampled series should contain.\n            This should be passed as a keyword argument.\n\n        Returns\n        -------\n        np.ndarray\n            The index positions of the selected data points.\n\n        \"\"\"\n        # Check n_out\n        DataPointSelector._check_n_out(n_out)\n\n        # Get x and y\n        x, y = DataPointSelector._process_args(*args)\n\n        # Check x and y\n        self._check_x_y(x, y)\n\n        if len(y) &lt;= n_out:\n            # Fewer samples than n_out -&gt; return all indices\n            return np.arange(len(y))\n\n        # More samples that n_out -&gt; perform data aggregation\n        return self._arg_downsample(x=x, y=y, n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/aggregation_interface/#aggregation.aggregation_interface.DataPointSelector.arg_downsample","title":"<code>arg_downsample(*args, n_out)</code>","text":"<p>Compute the index positions for the downsampled representation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The x and y data of the to-be-aggregated series. The x array is optional (i.e., if only 1 array is passed, it is assumed to be the y array). The array(s) must be 1-dimensional, and have the same length (if x is passed). These cannot be passed as keyword arguments, as they are positional-only.</p> required <code>y</code> <p>The x and y data of the to-be-aggregated series. The x array is optional (i.e., if only 1 array is passed, it is assumed to be the y array). The array(s) must be 1-dimensional, and have the same length (if x is passed). These cannot be passed as keyword arguments, as they are positional-only.</p> required <code>n_out</code> <code>int</code> <p>The number of samples which the downsampled series should contain. This should be passed as a keyword argument.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The index positions of the selected data points.</p> Source code in <code>plotly_resampler/aggregation/aggregation_interface.py</code> <pre><code>def arg_downsample(\n    self,\n    *args,\n    n_out: int,\n) -&gt; np.ndarray:\n    \"\"\"Compute the index positions for the downsampled representation.\n\n    Parameters\n    ----------\n    x, y: np.ndarray\n        The x and y data of the to-be-aggregated series.\n        The x array is optional (i.e., if only 1 array is passed, it is assumed to\n        be the y array).\n        The array(s) must be 1-dimensional, and have the same length (if x is\n        passed).\n        These cannot be passed as keyword arguments, as they are positional-only.\n    n_out: int\n        The number of samples which the downsampled series should contain.\n        This should be passed as a keyword argument.\n\n    Returns\n    -------\n    np.ndarray\n        The index positions of the selected data points.\n\n    \"\"\"\n    # Check n_out\n    DataPointSelector._check_n_out(n_out)\n\n    # Get x and y\n    x, y = DataPointSelector._process_args(*args)\n\n    # Check x and y\n    self._check_x_y(x, y)\n\n    if len(y) &lt;= n_out:\n        # Fewer samples than n_out -&gt; return all indices\n        return np.arange(len(y))\n\n    # More samples that n_out -&gt; perform data aggregation\n    return self._arg_downsample(x=x, y=y, n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/aggregators/","title":"aggregators","text":"<p>Compatible implementation for various aggregation/downsample methods.</p>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.EveryNthPoint","title":"<code>EveryNthPoint</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Naive (but fast) aggregator method which returns every N\u2019th point.</p> <p>Note</p> <p>This downsampler supports all dtypes.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class EveryNthPoint(DataPointSelector):\n    \"\"\"Naive (but fast) aggregator method which returns every N'th point.\n\n    !!! note\n        This downsampler supports all dtypes.\n    \"\"\"\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return EveryNthDownsampler().downsample(y, n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.FuncAggregator","title":"<code>FuncAggregator</code>","text":"<p>               Bases: <code>DataAggregator</code></p> <p>Aggregator instance which uses the passed aggregation func.</p> <p>Warning</p> <p>The user has total control which <code>aggregation_func</code> is passed to this method, hence the user should be careful to not make copies of the data, nor write to the data. Furthermore, the user should beware of performance issues when using more complex aggregation functions.</p> <p>Attention</p> <p>The user has total control which <code>aggregation_func</code> is passed to this method, hence it is the users\u2019 responsibility to handle categorical and bool-based data types.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class FuncAggregator(DataAggregator):\n    \"\"\"Aggregator instance which uses the passed aggregation func.\n\n    !!! warning\n\n        The user has total control which `aggregation_func` is passed to this method,\n        hence the user should be careful to not make copies of the data, nor write to\n        the data. Furthermore, the user should beware of performance issues when\n        using more complex aggregation functions.\n\n    !!! warning \"Attention\"\n\n        The user has total control which `aggregation_func` is passed to this method,\n        hence it is the users' responsibility to handle categorical and bool-based\n        data types.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        aggregation_func,\n        x_dtype_regex_list=None,\n        y_dtype_regex_list=None,\n        **downsample_kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        aggregation_func: Callable\n            The aggregation function which will be applied on each pin.\n\n        \"\"\"\n        self.aggregation_func = aggregation_func\n        super().__init__(x_dtype_regex_list, y_dtype_regex_list, **downsample_kwargs)\n\n    def _aggregate(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Aggregate the data using the object's aggregation function.\n\n        Parameters\n        ----------\n        x: np.ndarray | None\n            The x-values of the data. Can be None if no x-values are available.\n        y: np.ndarray\n            The y-values of the data.\n        n_out: int\n            The number of output data points.\n        **kwargs\n            Additional keyword arguments, which are passed to the aggregation function.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            The aggregated x &amp; y values.\n            If `x` is None, then the indices of the first element of each bin is\n            returned as x-values.\n\n        \"\"\"\n        # Create an index-estimation for real-time data\n        # Add one to the index so it's pointed at the end of the window\n        # Note: this can be adjusted to .5 to center the data\n        # Multiply it with the group size to get the real index-position\n        # TODO: add option to select start / middle / end as index\n        if x is None:\n            # equidistant index\n            idxs = np.linspace(0, len(y), n_out + 1).astype(int)\n        else:\n            xdt = x.dtype\n            if np.issubdtype(xdt, np.datetime64) or np.issubdtype(xdt, np.timedelta64):\n                x = x.view(\"int64\")\n            # Thanks to `linspace`, the data is evenly distributed over the index-range\n            # The searchsorted function returns the index positions\n            idxs = np.searchsorted(x, np.linspace(x[0], x[-1], n_out + 1))\n\n        y_agg = np.array(\n            [\n                self.aggregation_func(y[t0:t1], **self.downsample_kwargs)\n                for t0, t1 in zip(idxs[:-1], idxs[1:])\n            ]\n        )\n\n        if x is not None:\n            x_agg = x[idxs[:-1]]\n        else:\n            # x is None -&gt; return the indices of the first element of each bin\n            x_agg = idxs[:-1]\n\n        return x_agg, y_agg\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.FuncAggregator.__init__","title":"<code>__init__(aggregation_func, x_dtype_regex_list=None, y_dtype_regex_list=None, **downsample_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>aggregation_func</code> <p>The aggregation function which will be applied on each pin.</p> required Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>def __init__(\n    self,\n    aggregation_func,\n    x_dtype_regex_list=None,\n    y_dtype_regex_list=None,\n    **downsample_kwargs,\n):\n    \"\"\"\n    Parameters\n    ----------\n    aggregation_func: Callable\n        The aggregation function which will be applied on each pin.\n\n    \"\"\"\n    self.aggregation_func = aggregation_func\n    super().__init__(x_dtype_regex_list, y_dtype_regex_list, **downsample_kwargs)\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.LTTB","title":"<code>LTTB</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Largest Triangle Three Buckets (LTTB) aggregation method.</p> <p>This is arguably the most widely used aggregation method. It is based on the effective area of a triangle (inspired from the line simplification domain). The algorithm has $O(n)$ complexity, however, for large datasets, it can be much slower than other algorithms (e.g. MinMax) due to the higher cost of calculating the areas of triangles.</p> <p>Thesis: https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf  Details on visual representativeness &amp; stability: https://arxiv.org/abs/2304.00900</p> <p>Tip</p> <p><code>LTTB</code> doesn\u2019t scale super-well when moving to really large datasets, so when dealing with more than 1 million samples, you might consider using <code>MinMaxLTTB</code>.</p> <p>Note</p> <ul> <li>This class is mainly designed to operate on numerical data as LTTB calculates   distances on the values.    When dealing with categories, the data is encoded into its numeric codes,   these codes are the indices of the category array.</li> <li>To aggregate category data with LTTB, your <code>pd.Series</code> must be of dtype   \u2018category\u2019. </li> </ul> <p>tip:</p> <p>if there is an order in your categories, order them that way, LTTB uses   the ordered category codes values (see bullet above) to calculate distances and   make aggregation decisions.  code:     <pre><code>    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n    &gt;&gt;&gt; cat_type = pd.CategoricalDtype(categories=[\"b\", \"c\", \"a\"], ordered=True)\n    &gt;&gt;&gt; s_cat = s.astype(cat_type)\n</code></pre> * <code>LTTB</code> has no downsample kwargs, as it cannot be paralellized. Instead, you can   use the <code>MinMaxLTTB</code> downsampler, which performs   minmax preselection (in parallel if configured so), followed by LTTB.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class LTTB(DataPointSelector):\n    \"\"\"Largest Triangle Three Buckets (LTTB) aggregation method.\n\n    This is arguably the most widely used aggregation method. It is based on the\n    effective area of a triangle (inspired from the line simplification domain).\n    The algorithm has $O(n)$ complexity, however, for large datasets, it can be much\n    slower than other algorithms (e.g. MinMax) due to the higher cost of calculating\n    the areas of triangles.\n\n    Thesis: [https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf](https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf) &lt;br/&gt;\n    Details on visual representativeness &amp; stability: [https://arxiv.org/abs/2304.00900](https://arxiv.org/abs/2304.00900)\n\n    !!! tip\n\n        `LTTB` doesn't scale super-well when moving to really large datasets, so when\n        dealing with more than 1 million samples, you might consider using\n        [`MinMaxLTTB`][aggregation.aggregators.MinMaxLTTB].\n\n\n    !!! note\n\n        * This class is mainly designed to operate on numerical data as LTTB calculates\n          distances on the values. &lt;br/&gt;\n          When dealing with categories, the data is encoded into its numeric codes,\n          these codes are the indices of the category array.\n        * To aggregate category data with LTTB, your ``pd.Series`` must be of dtype\n          'category'. &lt;br/&gt;\n\n          **tip**:\n\n          if there is an order in your categories, order them that way, LTTB uses\n          the ordered category codes values (see bullet above) to calculate distances and\n          make aggregation decisions. &lt;br/&gt;\n          **code**:\n            ```python\n                &gt;&gt;&gt; import pandas as pd\n                &gt;&gt;&gt; s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n                &gt;&gt;&gt; cat_type = pd.CategoricalDtype(categories=[\"b\", \"c\", \"a\"], ordered=True)\n                &gt;&gt;&gt; s_cat = s.astype(cat_type)\n            ```\n        * `LTTB` has no downsample kwargs, as it cannot be paralellized. Instead, you can\n          use the [`MinMaxLTTB`][aggregation.aggregators.MinMaxLTTB] downsampler, which performs\n          minmax preselection (in parallel if configured so), followed by LTTB.\n\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            y_dtype_regex_list=[rf\"{dtype}\\d*\" for dtype in (\"float\", \"int\", \"uint\")]\n            + [\"category\", \"bool\"],\n        )\n        self.downsampler = LTTBDownsampler()\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return self.downsampler.downsample(*_to_tsdownsample_args(x, y), n_out=n_out)\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.MinMaxAggregator","title":"<code>MinMaxAggregator</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Aggregation method which performs binned min-max aggregation over fully overlapping windows.</p> <p>This is arguably the most computational efficient downsampling method, as it only performs (non-expensive) comparisons on the data in a single pass.</p> <p>Details on visual representativeness &amp; stability: https://arxiv.org/abs/2304.00900</p> <p>Note</p> <p>This method is rather efficient when scaling to large data sizes and can be used as a data-reduction step before feeding it to the <code>LTTB</code> algorithm, as <code>MinMaxLTTB</code> does with the <code>MinMaxOverlapAggregator</code>.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class MinMaxAggregator(DataPointSelector):\n    \"\"\"Aggregation method which performs binned min-max aggregation over fully\n    overlapping windows.\n\n    This is arguably the most computational efficient downsampling method, as it only\n    performs (non-expensive) comparisons on the data in a single pass.\n\n    Details on visual representativeness &amp; stability: [https://arxiv.org/abs/2304.00900](https://arxiv.org/abs/2304.00900)\n\n    !!! note\n\n        This method is rather efficient when scaling to large data sizes and can be used\n        as a data-reduction step before feeding it to the [`LTTB`][aggregation.aggregators.LTTB]\n        algorithm, as [`MinMaxLTTB`][aggregation.aggregators.MinMaxLTTB] does with the\n        [`MinMaxOverlapAggregator`][aggregation.aggregators.MinMaxOverlapAggregator].\n\n    \"\"\"\n\n    def __init__(self, nan_policy=\"omit\", **downsample_kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        **downsample_kwargs\n            Keyword arguments passed to the :class:`MinMaxDownsampler`.\n            - The `parallel` argument is set to False by default.\n        nan_policy: str, optional\n            The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n\n        \"\"\"\n        # this downsampler supports all dtypes\n        super().__init__(**downsample_kwargs)\n        if nan_policy not in (\"omit\", \"keep\"):\n            raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n        if nan_policy == \"omit\":\n            self.downsampler = MinMaxDownsampler()\n        else:\n            self.downsampler = NaNMinMaxDownsampler()\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return self.downsampler.downsample(\n            *_to_tsdownsample_args(x, y), n_out=n_out, **self.downsample_kwargs\n        )\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.MinMaxAggregator.__init__","title":"<code>__init__(nan_policy='omit', **downsample_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>**downsample_kwargs</code> <p>Keyword arguments passed to the :class:<code>MinMaxDownsampler</code>. - The <code>parallel</code> argument is set to False by default.</p> <code>{}</code> <code>nan_policy</code> <p>The policy to handle NaNs. Can be \u2018omit\u2019 or \u2018keep\u2019. By default, \u2018omit\u2019.</p> <code>'omit'</code> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>def __init__(self, nan_policy=\"omit\", **downsample_kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    **downsample_kwargs\n        Keyword arguments passed to the :class:`MinMaxDownsampler`.\n        - The `parallel` argument is set to False by default.\n    nan_policy: str, optional\n        The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n\n    \"\"\"\n    # this downsampler supports all dtypes\n    super().__init__(**downsample_kwargs)\n    if nan_policy not in (\"omit\", \"keep\"):\n        raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n    if nan_policy == \"omit\":\n        self.downsampler = MinMaxDownsampler()\n    else:\n        self.downsampler = NaNMinMaxDownsampler()\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.MinMaxLTTB","title":"<code>MinMaxLTTB</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Efficient version off LTTB by first reducing really large datasets with the <code>MinMaxAggregator</code> and then further aggregating the reduced result with <code>LTTB</code>.</p> <p>Starting from 10M data points, this method performs the MinMax-prefetching of data points to enhance computational efficiency.</p> <p>Inventors: Jonas &amp; Jeroen Van Der Donckt - 2022</p> <p>Paper: https://arxiv.org/pdf/2305.00332.pdf</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class MinMaxLTTB(DataPointSelector):\n    \"\"\"Efficient version off LTTB by first reducing really large datasets with\n    the [`MinMaxAggregator`][aggregation.aggregators.MinMaxAggregator] and then further aggregating the\n    reduced result with [`LTTB`][aggregation.aggregators.LTTB].\n\n    Starting from 10M data points, this method performs the MinMax-prefetching of data\n    points to enhance computational efficiency.\n\n    Inventors: Jonas &amp; Jeroen Van Der Donckt - 2022\n\n    Paper: [https://arxiv.org/pdf/2305.00332.pdf](https://arxiv.org/pdf/2305.00332.pdf)\n    \"\"\"\n\n    def __init__(\n        self, minmax_ratio: int = 4, nan_policy: str = \"omit\", **downsample_kwargs\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        minmax_ratio: int, optional\n            The ratio between the number of data points in the MinMax-prefetching and\n            the number of data points that will be outputted by LTTB. By default, 4.\n        nan_policy: str, optional\n            The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n        **downsample_kwargs\n            Keyword arguments passed to the `MinMaxLTTBDownsampler`.\n            - The `parallel` argument is set to False by default.\n            - The `minmax_ratio` argument is set to 4 by default, which was empirically\n              proven to be a good default.\n\n        \"\"\"\n        if nan_policy not in (\"omit\", \"keep\"):\n            raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n        if nan_policy == \"omit\":\n            self.minmaxlttb = MinMaxLTTBDownsampler()\n        else:\n            self.minmaxlttb = NaNMinMaxLTTBDownsampler()\n\n        self.minmax_ratio = minmax_ratio\n\n        super().__init__(\n            y_dtype_regex_list=[rf\"{dtype}\\d*\" for dtype in (\"float\", \"int\", \"uint\")]\n            + [\"category\", \"bool\"],\n            **downsample_kwargs,\n        )\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        return self.minmaxlttb.downsample(\n            *_to_tsdownsample_args(x, y),\n            n_out=n_out,\n            minmax_ratio=self.minmax_ratio,\n            **self.downsample_kwargs,\n        )\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.MinMaxLTTB.__init__","title":"<code>__init__(minmax_ratio=4, nan_policy='omit', **downsample_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>minmax_ratio</code> <code>int</code> <p>The ratio between the number of data points in the MinMax-prefetching and the number of data points that will be outputted by LTTB. By default, 4.</p> <code>4</code> <code>nan_policy</code> <code>str</code> <p>The policy to handle NaNs. Can be \u2018omit\u2019 or \u2018keep\u2019. By default, \u2018omit\u2019.</p> <code>'omit'</code> <code>**downsample_kwargs</code> <p>Keyword arguments passed to the <code>MinMaxLTTBDownsampler</code>. - The <code>parallel</code> argument is set to False by default. - The <code>minmax_ratio</code> argument is set to 4 by default, which was empirically   proven to be a good default.</p> <code>{}</code> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>def __init__(\n    self, minmax_ratio: int = 4, nan_policy: str = \"omit\", **downsample_kwargs\n):\n    \"\"\"\n    Parameters\n    ----------\n    minmax_ratio: int, optional\n        The ratio between the number of data points in the MinMax-prefetching and\n        the number of data points that will be outputted by LTTB. By default, 4.\n    nan_policy: str, optional\n        The policy to handle NaNs. Can be 'omit' or 'keep'. By default, 'omit'.\n    **downsample_kwargs\n        Keyword arguments passed to the `MinMaxLTTBDownsampler`.\n        - The `parallel` argument is set to False by default.\n        - The `minmax_ratio` argument is set to 4 by default, which was empirically\n          proven to be a good default.\n\n    \"\"\"\n    if nan_policy not in (\"omit\", \"keep\"):\n        raise ValueError(\"nan_policy must be either 'omit' or 'keep'\")\n    if nan_policy == \"omit\":\n        self.minmaxlttb = MinMaxLTTBDownsampler()\n    else:\n        self.minmaxlttb = NaNMinMaxLTTBDownsampler()\n\n    self.minmax_ratio = minmax_ratio\n\n    super().__init__(\n        y_dtype_regex_list=[rf\"{dtype}\\d*\" for dtype in (\"float\", \"int\", \"uint\")]\n        + [\"category\", \"bool\"],\n        **downsample_kwargs,\n    )\n</code></pre>"},{"location":"api/aggregation/aggregators/#aggregation.aggregators.MinMaxOverlapAggregator","title":"<code>MinMaxOverlapAggregator</code>","text":"<p>               Bases: <code>DataPointSelector</code></p> <p>Aggregation method which performs binned min-max aggregation over 50% overlapping windows.</p> <p></p> <p>In the above image, bin_size: represents the size of (len(series) / n_out). As the windows have 50% overlap and are consecutive, the min &amp; max values are calculated on a windows with size (2x bin-size).</p> <p>This is very similar to the MinMaxAggregator, emperical results showed no observable difference between both approaches.</p> <p>Note</p> <p>This method is implemented in Python (leveraging numpy for vecotrization), but is significantly slower than the MinMaxAggregator (which is implemented in the tsdownsample toolkit in Rust).  As such, this class does not support any downsample kwargs.</p> <p>Note</p> <p>This downsampler supports all dtypes.</p> Source code in <code>plotly_resampler/aggregation/aggregators.py</code> <pre><code>class MinMaxOverlapAggregator(DataPointSelector):\n    \"\"\"Aggregation method which performs binned min-max aggregation over 50% overlapping\n    windows.\n\n    ![minmax operator image](https://github.com/predict-idlab/plotly-resampler/blob/main/mkdocs/static/minmax_operator.png)\n\n    In the above image, **bin_size**: represents the size of *(len(series) / n_out)*.\n    As the windows have 50% overlap and are consecutive, the min &amp; max values are\n    calculated on a windows with size (2x bin-size).\n\n    This is *very* similar to the MinMaxAggregator, emperical results showed no\n    observable difference between both approaches.\n\n    !!! note\n\n        This method is implemented in Python (leveraging numpy for vecotrization), but\n        is **significantly slower than the MinMaxAggregator** (which is implemented in\n        the tsdownsample toolkit in Rust). &lt;br/&gt;\n        As such, this class does not support any downsample kwargs.\n\n    !!! note\n\n        This downsampler supports all dtypes.\n\n    \"\"\"\n\n    def _arg_downsample(\n        self,\n        x: np.ndarray | None,\n        y: np.ndarray,\n        n_out: int,\n    ) -&gt; np.ndarray:\n        # The block size 2x the bin size we also perform the ceil-operation\n        # to ensure that the block_size * n_out / 2 &lt; len(x)\n        block_size = math.ceil(y.shape[0] / (n_out + 1) * 2)\n        argmax_offset = block_size // 2\n\n        # Calculate the offset range which will be added to the argmin and argmax pos\n        offset = np.arange(\n            0, stop=y.shape[0] - block_size - argmax_offset, step=block_size\n        )\n\n        # Calculate the argmin &amp; argmax on the reshaped view of `y` &amp;\n        # add the corresponding offset\n        argmin = (\n            y[: block_size * offset.shape[0]].reshape(-1, block_size).argmin(axis=1)\n            + offset\n        )\n        argmax = (\n            y[argmax_offset : block_size * offset.shape[0] + argmax_offset]\n            .reshape(-1, block_size)\n            .argmax(axis=1)\n            + offset\n            + argmax_offset\n        )\n\n        # Sort the argmin &amp; argmax (where we append the first and last index item)\n        return np.unique(np.concatenate((argmin, argmax, [0, y.shape[0] - 1])))\n</code></pre>"},{"location":"api/aggregation/gap_handler_interface/","title":"gap_handler_interface","text":"<p>AbstractGapHandler interface-class, subclassed by concrete gap handlers.</p>"},{"location":"api/aggregation/gap_handler_interface/#aggregation.gap_handler_interface.AbstractGapHandler","title":"<code>AbstractGapHandler</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>plotly_resampler/aggregation/gap_handler_interface.py</code> <pre><code>class AbstractGapHandler(ABC):\n    def __init__(self, fill_value: Optional[float] = None):\n        \"\"\"Constructor of AbstractGapHandler.\n\n        Parameters\n        ----------\n        fill_value: float, optional\n            The value to fill the gaps with, by default None.\n            Note that setting this value to 0 for filled area plots is particularly\n            useful.\n\n        \"\"\"\n        self.fill_value = fill_value\n\n    @abstractmethod\n    def _get_gap_mask(self, x_agg: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"Get a boolean mask indicating the indices where there are gaps.\n\n        If you require custom gap handling, you can implement this method to return a\n        boolean mask indicating the indices where there are gaps.\n\n        Parameters\n        ----------\n        x_agg: np.ndarray\n            The x array. This is used to determine the gaps.\n\n        Returns\n        -------\n        Optional[np.ndarray]\n            A boolean mask indicating the indices where there are gaps. If there are no\n            gaps, None is returned.\n\n        \"\"\"\n        pass\n\n    def insert_fill_value_between_gaps(\n        self,\n        x_agg: np.ndarray,\n        y_agg: np.ndarray,\n        idxs: np.ndarray,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Insert the fill_value in the y_agg array where there are gaps.\n\n        Gaps are determined by the x_agg array. The `_get_gap_mask` method is used to\n        determine a boolean mask indicating the indices where there are gaps.\n\n        Parameters\n        ----------\n        x_agg: np.ndarray\n            The x array. This is used to determine the gaps.\n        y_agg: np.ndarray\n            The y array. A copy of this array will be expanded with fill_values where\n            there are gaps.\n        idxs: np.ndarray\n            The index array. This is relevant aggregators that perform data point\n            selection (e.g., max, min, etc.) - this array will be expanded with the\n            same indices where there are gaps.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            The expanded y_agg array and the expanded idxs array respectively.\n\n        \"\"\"\n        gap_mask = self._get_gap_mask(x_agg)\n        if gap_mask is None:\n            # no gaps are found, nothing to do\n            return y_agg, idxs\n\n        # An array filled with 1s and 2s, where 2 indicates a large gap mask\n        # (i.e., that index will be repeated twice)\n        repeats = np.ones(x_agg.shape, dtype=\"int\") + gap_mask\n\n        # use the repeats to expand the idxs, and agg_y array\n        idx_exp_nan = np.repeat(idxs, repeats)\n        y_agg_exp_nan = np.repeat(y_agg, repeats)\n\n        # only float arrays can contain NaN values\n        if issubclass(y_agg_exp_nan.dtype.type, np.integer) or issubclass(\n            y_agg_exp_nan.dtype.type, np.bool_\n        ):\n            y_agg_exp_nan = y_agg_exp_nan.astype(\"float\")\n\n        # Set the NaN values\n        # We add the gap index offset (via the np.arange) to the indices to account for\n        # the repeats (i.e., expanded y_agg array).\n        y_agg_exp_nan[np.where(gap_mask)[0] + np.arange(gap_mask.sum())] = (\n            self.fill_value\n        )\n\n        return y_agg_exp_nan, idx_exp_nan\n</code></pre>"},{"location":"api/aggregation/gap_handler_interface/#aggregation.gap_handler_interface.AbstractGapHandler.__init__","title":"<code>__init__(fill_value=None)</code>","text":"<p>Constructor of AbstractGapHandler.</p> <p>Parameters:</p> Name Type Description Default <code>fill_value</code> <code>Optional[float]</code> <p>The value to fill the gaps with, by default None. Note that setting this value to 0 for filled area plots is particularly useful.</p> <code>None</code> Source code in <code>plotly_resampler/aggregation/gap_handler_interface.py</code> <pre><code>def __init__(self, fill_value: Optional[float] = None):\n    \"\"\"Constructor of AbstractGapHandler.\n\n    Parameters\n    ----------\n    fill_value: float, optional\n        The value to fill the gaps with, by default None.\n        Note that setting this value to 0 for filled area plots is particularly\n        useful.\n\n    \"\"\"\n    self.fill_value = fill_value\n</code></pre>"},{"location":"api/aggregation/gap_handler_interface/#aggregation.gap_handler_interface.AbstractGapHandler.insert_fill_value_between_gaps","title":"<code>insert_fill_value_between_gaps(x_agg, y_agg, idxs)</code>","text":"<p>Insert the fill_value in the y_agg array where there are gaps.</p> <p>Gaps are determined by the x_agg array. The <code>_get_gap_mask</code> method is used to determine a boolean mask indicating the indices where there are gaps.</p> <p>Parameters:</p> Name Type Description Default <code>x_agg</code> <code>ndarray</code> <p>The x array. This is used to determine the gaps.</p> required <code>y_agg</code> <code>ndarray</code> <p>The y array. A copy of this array will be expanded with fill_values where there are gaps.</p> required <code>idxs</code> <code>ndarray</code> <p>The index array. This is relevant aggregators that perform data point selection (e.g., max, min, etc.) - this array will be expanded with the same indices where there are gaps.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>The expanded y_agg array and the expanded idxs array respectively.</p> Source code in <code>plotly_resampler/aggregation/gap_handler_interface.py</code> <pre><code>def insert_fill_value_between_gaps(\n    self,\n    x_agg: np.ndarray,\n    y_agg: np.ndarray,\n    idxs: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Insert the fill_value in the y_agg array where there are gaps.\n\n    Gaps are determined by the x_agg array. The `_get_gap_mask` method is used to\n    determine a boolean mask indicating the indices where there are gaps.\n\n    Parameters\n    ----------\n    x_agg: np.ndarray\n        The x array. This is used to determine the gaps.\n    y_agg: np.ndarray\n        The y array. A copy of this array will be expanded with fill_values where\n        there are gaps.\n    idxs: np.ndarray\n        The index array. This is relevant aggregators that perform data point\n        selection (e.g., max, min, etc.) - this array will be expanded with the\n        same indices where there are gaps.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        The expanded y_agg array and the expanded idxs array respectively.\n\n    \"\"\"\n    gap_mask = self._get_gap_mask(x_agg)\n    if gap_mask is None:\n        # no gaps are found, nothing to do\n        return y_agg, idxs\n\n    # An array filled with 1s and 2s, where 2 indicates a large gap mask\n    # (i.e., that index will be repeated twice)\n    repeats = np.ones(x_agg.shape, dtype=\"int\") + gap_mask\n\n    # use the repeats to expand the idxs, and agg_y array\n    idx_exp_nan = np.repeat(idxs, repeats)\n    y_agg_exp_nan = np.repeat(y_agg, repeats)\n\n    # only float arrays can contain NaN values\n    if issubclass(y_agg_exp_nan.dtype.type, np.integer) or issubclass(\n        y_agg_exp_nan.dtype.type, np.bool_\n    ):\n        y_agg_exp_nan = y_agg_exp_nan.astype(\"float\")\n\n    # Set the NaN values\n    # We add the gap index offset (via the np.arange) to the indices to account for\n    # the repeats (i.e., expanded y_agg array).\n    y_agg_exp_nan[np.where(gap_mask)[0] + np.arange(gap_mask.sum())] = (\n        self.fill_value\n    )\n\n    return y_agg_exp_nan, idx_exp_nan\n</code></pre>"},{"location":"api/aggregation/gap_handlers/","title":"gap_handlers","text":"<p>Compatible implementation for various gap handling methods.</p>"},{"location":"api/aggregation/gap_handlers/#aggregation.gap_handlers.MedDiffGapHandler","title":"<code>MedDiffGapHandler</code>","text":"<p>               Bases: <code>AbstractGapHandler</code></p> <p>Gap handling based on the median diff of the x_agg array.</p> Source code in <code>plotly_resampler/aggregation/gap_handlers.py</code> <pre><code>class MedDiffGapHandler(AbstractGapHandler):\n    \"\"\"Gap handling based on the median diff of the x_agg array.\"\"\"\n\n    def _calc_med_diff(self, x_agg: np.ndarray) -&gt; Tuple[float, np.ndarray]:\n        \"\"\"Calculate the median diff of the x_agg array.\n\n        As median is more robust to outliers than the mean, the median is used to define\n        the gap threshold.\n\n        This method performs a divide and conquer heuristic to calculate the median;\n        1. divide the array into `n_blocks` blocks (with `n_blocks` = 128)\n        2. calculate the mean of each block\n        3. calculate the median of the means\n        =&gt; This proves to be a good approximation of the median of the full array, while\n              being much faster than calculating the median of the full array.\n        \"\"\"\n        # remark: thanks to the prepend -&gt; x_diff.shape === len(s)\n        x_diff = np.diff(x_agg, prepend=x_agg[0])\n\n        # To do so - use an approach where we reshape the data\n        # into `n_blocks` blocks and calculate the mean and then the median on that\n        # Why use `median` instead of a global mean?\n        #   =&gt; when you have large gaps, they will be represented by a large diff\n        #      which will skew the mean way more than the median!\n        n_blocks = 128\n        if x_agg.shape[0] &gt; 5 * n_blocks:\n            blck_size = x_diff.shape[0] // n_blocks\n\n            # convert the index series index diff into a reshaped view (i.e., sid_v)\n            sid_v: np.ndarray = x_diff[: blck_size * n_blocks].reshape(n_blocks, -1)\n\n            # calculate the mean fore each block and then the median of those means\n            med_diff = np.median(np.mean(sid_v, axis=1))\n        else:\n            med_diff = np.median(x_diff)\n\n        return med_diff, x_diff\n\n    def _get_gap_mask(self, x_agg: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"Get a boolean mask indicating the indices where there are gaps.\n\n        If you require custom gap handling, you can implement this method to return a\n        boolean mask indicating the indices where there are gaps.\n\n        Parameters\n        ----------\n        x_agg: np.ndarray\n            The x array. This is used to determine the gaps.\n\n        Returns\n        -------\n        Optional[np.ndarray]\n            A boolean mask indicating the indices where there are gaps. If there are no\n            gaps, None is returned.\n\n        \"\"\"\n        med_diff, x_diff = self._calc_med_diff(x_agg)\n\n        # TODO: this 4 was revealed to me in a dream, but it seems to work well\n        # After some consideration, we altered this to a 4.1\n        gap_mask = x_diff &gt; 4.1 * med_diff\n        if not any(gap_mask):\n            return\n        return gap_mask\n</code></pre>"},{"location":"api/aggregation/gap_handlers/#aggregation.gap_handlers.NoGapHandler","title":"<code>NoGapHandler</code>","text":"<p>               Bases: <code>AbstractGapHandler</code></p> <p>No gap handling.</p> Source code in <code>plotly_resampler/aggregation/gap_handlers.py</code> <pre><code>class NoGapHandler(AbstractGapHandler):\n    \"\"\"No gap handling.\"\"\"\n\n    def _get_gap_mask(self, x_agg: np.ndarray) -&gt; Optional[np.ndarray]:\n        return\n</code></pre>"},{"location":"api/aggregation/plotly_aggregator_parser/","title":"plotly_aggregator_parser","text":""},{"location":"api/aggregation/plotly_aggregator_parser/#aggregation.plotly_aggregator_parser.PlotlyAggregatorParser","title":"<code>PlotlyAggregatorParser</code>","text":"Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>class PlotlyAggregatorParser:\n    @staticmethod\n    def parse_hf_data(\n        hf_data: np.ndarray | pd.Categorical | pd.Series | pd.Index,\n    ) -&gt; np.ndarray | pd.Categorical:\n        \"\"\"Parse the high-frequency data to a numpy array.\"\"\"\n        # Categorical data (pandas)\n        #   - pd.Series with categorical dtype -&gt; calling .values will returns a\n        #       pd.Categorical\n        #   - pd.CategoricalIndex -&gt; calling .values returns a pd.Categorical\n        #   - pd.Categorical: has no .values attribute -&gt; will not be parsed\n        if isinstance(hf_data, pd.RangeIndex):\n            return None\n        if isinstance(hf_data, (pd.Series, pd.Index)):\n            return hf_data.values\n        return hf_data\n\n    @staticmethod\n    def to_same_tz(\n        ts: Union[pd.Timestamp, None], reference_tz: Union[pytz.BaseTzInfo, None]\n    ) -&gt; Union[pd.Timestamp, None]:\n        \"\"\"Adjust `ts` its timezone to the `reference_tz`.\"\"\"\n        if ts is None:\n            return None\n        elif reference_tz is not None:\n            if ts.tz is not None:\n                # compare if these two have the same timezone / offset\n                try:\n                    assert ts.tz.__str__() == reference_tz.__str__()\n                except AssertionError:\n                    assert ts.utcoffset() == reference_tz.utcoffset(ts.tz_convert(None))\n                return ts\n            else:  # localize -&gt; time remains the same\n                return ts.tz_localize(reference_tz)\n        elif reference_tz is None and ts.tz is not None:\n            return ts.tz_localize(None)\n        return ts\n\n    @staticmethod\n    def get_start_end_indices(hf_trace_data, axis_type, start, end) -&gt; Tuple[int, int]:\n        \"\"\"Get the start &amp; end indices of the high-frequency data.\"\"\"\n        # Base case: no hf data, or both start &amp; end are None\n        if not len(hf_trace_data[\"x\"]):\n            return 0, 0\n        elif start is None and end is None:\n            return 0, len(hf_trace_data[\"x\"])\n\n        # NOTE: as we use bisect right for the end index, we do not need to add a\n        #      small epsilon to the end value\n        start = hf_trace_data[\"x\"][0] if start is None else start\n        end = hf_trace_data[\"x\"][-1] if end is None else end\n\n        # NOTE: we must verify this before check if the x is a range-index\n        if axis_type == \"log\":\n            start, end = 10**start, 10**end\n\n        # We can compute the start &amp; end indices directly when it is a RangeIndex\n        if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n            x_start = hf_trace_data[\"x\"].start\n            x_step = hf_trace_data[\"x\"].step\n            start_idx = int(max((start - x_start) // x_step, 0))\n            end_idx = int((end - x_start) // x_step)\n            return start_idx, end_idx\n        # TODO: this can be performed as-well for a fixed frequency range-index w/ freq\n\n        if axis_type == \"date\":\n            start, end = pd.to_datetime(start), pd.to_datetime(end)\n            # convert start &amp; end to the same timezone\n            if isinstance(hf_trace_data[\"x\"], pd.DatetimeIndex):\n                tz = hf_trace_data[\"x\"].tz\n                try:\n                    assert start.tz.__str__() == end.tz.__str__()\n                except (TypeError, AssertionError):\n                    # This fix is needed for DST (when the timezone is not fixed)\n                    assert start.tz_localize(None) == start.tz_convert(tz).tz_localize(\n                        None\n                    )\n                    assert end.tz_localize(None) == end.tz_convert(tz).tz_localize(None)\n\n                start = PlotlyAggregatorParser.to_same_tz(start, tz)\n                end = PlotlyAggregatorParser.to_same_tz(end, tz)\n\n        # Search the index-positions\n        start_idx = bisect.bisect_left(hf_trace_data[\"x\"], start)\n        end_idx = bisect.bisect_right(hf_trace_data[\"x\"], end)\n        return start_idx, end_idx\n\n    @staticmethod\n    def _handle_gaps(\n        hf_trace_data: dict,\n        hf_x: np.ndarray,\n        agg_x: np.ndarray,\n        agg_y: np.ndarray,\n        indices: np.ndarray,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Handle the gaps in the aggregated data.\n\n        Returns:\n            - agg_x: the aggregated x-values\n            - agg_y: the aggregated y-values\n            - indices: the indices of the hf_data data that were aggregated\n\n        \"\"\"\n        gap_handler: AbstractGapHandler = hf_trace_data[\"gap_handler\"]\n        downsampler = hf_trace_data[\"downsampler\"]\n\n        # TODO check for trace mode (markers, lines, etc.) and only perform the\n        # gap insertion methodology when the mode is lines.\n        # if trace.get(\"connectgaps\") != True and\n        if (\n            isinstance(gap_handler, NoGapHandler)\n            # rangeIndex | datetimeIndex with freq -&gt; equally spaced x; so no gaps\n            or isinstance(hf_trace_data[\"x\"], pd.RangeIndex)\n            or (\n                isinstance(hf_trace_data[\"x\"], pd.DatetimeIndex)\n                and hf_trace_data[\"x\"].freq is not None\n            )\n        ):\n            return agg_x, agg_y, indices\n\n        # Interleave the gaps\n        # View the data as an int64 when we have a DatetimeIndex\n        # We only want to detect gaps, so we only want to compare values.\n        agg_x_parsed = PlotlyAggregatorParser.parse_hf_data(agg_x)\n        xdt = agg_x_parsed.dtype\n        if np.issubdtype(xdt, np.timedelta64) or np.issubdtype(xdt, np.datetime64):\n            agg_x_parsed = agg_x_parsed.view(\"int64\")\n\n        agg_y, indices = gap_handler.insert_fill_value_between_gaps(\n            agg_x_parsed, agg_y, indices\n        )\n        if isinstance(downsampler, DataPointSelector):\n            agg_x = hf_x[indices]\n        elif isinstance(downsampler, DataAggregator):\n            # The indices are in this case a repeat\n            agg_x = agg_x[indices]\n\n        return agg_x, agg_y, indices\n\n    @staticmethod\n    def aggregate(\n        hf_trace_data: dict,\n        start_idx: int,\n        end_idx: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Aggregate the data in `hf_trace_data` between `start_idx` and `end_idx`.\n\n        Returns:\n            - x: the aggregated x-values\n            - y: the aggregated y-values\n            - indices: the indices of the hf_data data that were aggregated\n\n            These indices are useful to select the corresponding hf_data from\n            non `x` and `y` data (e.g. `text`, `marker_size`, `marker_color`).\n\n        \"\"\"\n        hf_x = hf_trace_data[\"x\"][start_idx:end_idx]\n        hf_y = hf_trace_data[\"y\"][start_idx:end_idx]\n\n        # No downsampling needed ; we show the raw data as is, but with gap-detection\n        if (end_idx - start_idx) &lt;= hf_trace_data[\"max_n_samples\"]:\n            indices = np.arange(len(hf_y))  # no downsampling - all values are selected\n            if len(indices):\n                return PlotlyAggregatorParser._handle_gaps(\n                    hf_trace_data, hf_x=hf_x, agg_x=hf_x, agg_y=hf_y, indices=indices\n                )\n            else:\n                return hf_x, hf_y, indices\n\n        downsampler = hf_trace_data[\"downsampler\"]\n\n        hf_x_parsed = PlotlyAggregatorParser.parse_hf_data(hf_x)\n        hf_y_parsed = PlotlyAggregatorParser.parse_hf_data(hf_y)\n\n        if isinstance(downsampler, DataPointSelector):\n            s_v = hf_y_parsed\n            if isinstance(s_v, pd.Categorical):  # pd.Categorical (has no .values)\n                s_v = s_v.codes\n            indices = downsampler.arg_downsample(\n                hf_x_parsed,\n                s_v,\n                n_out=hf_trace_data[\"max_n_samples\"],\n                **hf_trace_data.get(\"downsampler_kwargs\", {}),\n            )\n            if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n                # we avoid slicing the default pd.RangeIndex (as this is not an\n                # in-memory array) - this proves to be faster than slicing the index.\n                agg_x = (\n                    start_idx\n                    + hf_trace_data[\"x\"].start\n                    + indices.astype(hf_trace_data[\"x\"].dtype) * hf_trace_data[\"x\"].step\n                )\n            else:\n                agg_x = hf_x[indices]\n            agg_y = hf_y[indices]\n        elif isinstance(downsampler, DataAggregator):\n            agg_x, agg_y = downsampler.aggregate(\n                hf_x_parsed,\n                hf_y_parsed,\n                n_out=hf_trace_data[\"max_n_samples\"],\n                **hf_trace_data.get(\"downsampler_kwargs\", {}),\n            )\n            if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n                # we avoid slicing the default pd.RangeIndex (as this is not an\n                # in-memory array) - this proves to be faster than slicing the index.\n                agg_x = (\n                    start_idx\n                    + hf_trace_data[\"x\"].start\n                    + agg_x * hf_trace_data[\"x\"].step\n                )\n            # The indices are just the range of the aggregated data\n            indices = np.arange(len(agg_x))\n        else:\n            raise ValueError(\n                \"Invalid downsampler instance, must be either a \"\n                + f\"DataAggregator or a DataPointSelector, got {type(downsampler)}\"\n            )\n\n        return PlotlyAggregatorParser._handle_gaps(\n            hf_trace_data, hf_x=hf_x, agg_x=agg_x, agg_y=agg_y, indices=indices\n        )\n</code></pre>"},{"location":"api/aggregation/plotly_aggregator_parser/#aggregation.plotly_aggregator_parser.PlotlyAggregatorParser.aggregate","title":"<code>aggregate(hf_trace_data, start_idx, end_idx)</code>  <code>staticmethod</code>","text":"<p>Aggregate the data in <code>hf_trace_data</code> between <code>start_idx</code> and <code>end_idx</code>.</p> <p>Returns:     - x: the aggregated x-values     - y: the aggregated y-values     - indices: the indices of the hf_data data that were aggregated</p> <pre><code>These indices are useful to select the corresponding hf_data from\nnon `x` and `y` data (e.g. `text`, `marker_size`, `marker_color`).\n</code></pre> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef aggregate(\n    hf_trace_data: dict,\n    start_idx: int,\n    end_idx: int,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Aggregate the data in `hf_trace_data` between `start_idx` and `end_idx`.\n\n    Returns:\n        - x: the aggregated x-values\n        - y: the aggregated y-values\n        - indices: the indices of the hf_data data that were aggregated\n\n        These indices are useful to select the corresponding hf_data from\n        non `x` and `y` data (e.g. `text`, `marker_size`, `marker_color`).\n\n    \"\"\"\n    hf_x = hf_trace_data[\"x\"][start_idx:end_idx]\n    hf_y = hf_trace_data[\"y\"][start_idx:end_idx]\n\n    # No downsampling needed ; we show the raw data as is, but with gap-detection\n    if (end_idx - start_idx) &lt;= hf_trace_data[\"max_n_samples\"]:\n        indices = np.arange(len(hf_y))  # no downsampling - all values are selected\n        if len(indices):\n            return PlotlyAggregatorParser._handle_gaps(\n                hf_trace_data, hf_x=hf_x, agg_x=hf_x, agg_y=hf_y, indices=indices\n            )\n        else:\n            return hf_x, hf_y, indices\n\n    downsampler = hf_trace_data[\"downsampler\"]\n\n    hf_x_parsed = PlotlyAggregatorParser.parse_hf_data(hf_x)\n    hf_y_parsed = PlotlyAggregatorParser.parse_hf_data(hf_y)\n\n    if isinstance(downsampler, DataPointSelector):\n        s_v = hf_y_parsed\n        if isinstance(s_v, pd.Categorical):  # pd.Categorical (has no .values)\n            s_v = s_v.codes\n        indices = downsampler.arg_downsample(\n            hf_x_parsed,\n            s_v,\n            n_out=hf_trace_data[\"max_n_samples\"],\n            **hf_trace_data.get(\"downsampler_kwargs\", {}),\n        )\n        if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n            # we avoid slicing the default pd.RangeIndex (as this is not an\n            # in-memory array) - this proves to be faster than slicing the index.\n            agg_x = (\n                start_idx\n                + hf_trace_data[\"x\"].start\n                + indices.astype(hf_trace_data[\"x\"].dtype) * hf_trace_data[\"x\"].step\n            )\n        else:\n            agg_x = hf_x[indices]\n        agg_y = hf_y[indices]\n    elif isinstance(downsampler, DataAggregator):\n        agg_x, agg_y = downsampler.aggregate(\n            hf_x_parsed,\n            hf_y_parsed,\n            n_out=hf_trace_data[\"max_n_samples\"],\n            **hf_trace_data.get(\"downsampler_kwargs\", {}),\n        )\n        if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n            # we avoid slicing the default pd.RangeIndex (as this is not an\n            # in-memory array) - this proves to be faster than slicing the index.\n            agg_x = (\n                start_idx\n                + hf_trace_data[\"x\"].start\n                + agg_x * hf_trace_data[\"x\"].step\n            )\n        # The indices are just the range of the aggregated data\n        indices = np.arange(len(agg_x))\n    else:\n        raise ValueError(\n            \"Invalid downsampler instance, must be either a \"\n            + f\"DataAggregator or a DataPointSelector, got {type(downsampler)}\"\n        )\n\n    return PlotlyAggregatorParser._handle_gaps(\n        hf_trace_data, hf_x=hf_x, agg_x=agg_x, agg_y=agg_y, indices=indices\n    )\n</code></pre>"},{"location":"api/aggregation/plotly_aggregator_parser/#aggregation.plotly_aggregator_parser.PlotlyAggregatorParser.get_start_end_indices","title":"<code>get_start_end_indices(hf_trace_data, axis_type, start, end)</code>  <code>staticmethod</code>","text":"<p>Get the start &amp; end indices of the high-frequency data.</p> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef get_start_end_indices(hf_trace_data, axis_type, start, end) -&gt; Tuple[int, int]:\n    \"\"\"Get the start &amp; end indices of the high-frequency data.\"\"\"\n    # Base case: no hf data, or both start &amp; end are None\n    if not len(hf_trace_data[\"x\"]):\n        return 0, 0\n    elif start is None and end is None:\n        return 0, len(hf_trace_data[\"x\"])\n\n    # NOTE: as we use bisect right for the end index, we do not need to add a\n    #      small epsilon to the end value\n    start = hf_trace_data[\"x\"][0] if start is None else start\n    end = hf_trace_data[\"x\"][-1] if end is None else end\n\n    # NOTE: we must verify this before check if the x is a range-index\n    if axis_type == \"log\":\n        start, end = 10**start, 10**end\n\n    # We can compute the start &amp; end indices directly when it is a RangeIndex\n    if isinstance(hf_trace_data[\"x\"], pd.RangeIndex):\n        x_start = hf_trace_data[\"x\"].start\n        x_step = hf_trace_data[\"x\"].step\n        start_idx = int(max((start - x_start) // x_step, 0))\n        end_idx = int((end - x_start) // x_step)\n        return start_idx, end_idx\n    # TODO: this can be performed as-well for a fixed frequency range-index w/ freq\n\n    if axis_type == \"date\":\n        start, end = pd.to_datetime(start), pd.to_datetime(end)\n        # convert start &amp; end to the same timezone\n        if isinstance(hf_trace_data[\"x\"], pd.DatetimeIndex):\n            tz = hf_trace_data[\"x\"].tz\n            try:\n                assert start.tz.__str__() == end.tz.__str__()\n            except (TypeError, AssertionError):\n                # This fix is needed for DST (when the timezone is not fixed)\n                assert start.tz_localize(None) == start.tz_convert(tz).tz_localize(\n                    None\n                )\n                assert end.tz_localize(None) == end.tz_convert(tz).tz_localize(None)\n\n            start = PlotlyAggregatorParser.to_same_tz(start, tz)\n            end = PlotlyAggregatorParser.to_same_tz(end, tz)\n\n    # Search the index-positions\n    start_idx = bisect.bisect_left(hf_trace_data[\"x\"], start)\n    end_idx = bisect.bisect_right(hf_trace_data[\"x\"], end)\n    return start_idx, end_idx\n</code></pre>"},{"location":"api/aggregation/plotly_aggregator_parser/#aggregation.plotly_aggregator_parser.PlotlyAggregatorParser.parse_hf_data","title":"<code>parse_hf_data(hf_data)</code>  <code>staticmethod</code>","text":"<p>Parse the high-frequency data to a numpy array.</p> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef parse_hf_data(\n    hf_data: np.ndarray | pd.Categorical | pd.Series | pd.Index,\n) -&gt; np.ndarray | pd.Categorical:\n    \"\"\"Parse the high-frequency data to a numpy array.\"\"\"\n    # Categorical data (pandas)\n    #   - pd.Series with categorical dtype -&gt; calling .values will returns a\n    #       pd.Categorical\n    #   - pd.CategoricalIndex -&gt; calling .values returns a pd.Categorical\n    #   - pd.Categorical: has no .values attribute -&gt; will not be parsed\n    if isinstance(hf_data, pd.RangeIndex):\n        return None\n    if isinstance(hf_data, (pd.Series, pd.Index)):\n        return hf_data.values\n    return hf_data\n</code></pre>"},{"location":"api/aggregation/plotly_aggregator_parser/#aggregation.plotly_aggregator_parser.PlotlyAggregatorParser.to_same_tz","title":"<code>to_same_tz(ts, reference_tz)</code>  <code>staticmethod</code>","text":"<p>Adjust <code>ts</code> its timezone to the <code>reference_tz</code>.</p> Source code in <code>plotly_resampler/aggregation/plotly_aggregator_parser.py</code> <pre><code>@staticmethod\ndef to_same_tz(\n    ts: Union[pd.Timestamp, None], reference_tz: Union[pytz.BaseTzInfo, None]\n) -&gt; Union[pd.Timestamp, None]:\n    \"\"\"Adjust `ts` its timezone to the `reference_tz`.\"\"\"\n    if ts is None:\n        return None\n    elif reference_tz is not None:\n        if ts.tz is not None:\n            # compare if these two have the same timezone / offset\n            try:\n                assert ts.tz.__str__() == reference_tz.__str__()\n            except AssertionError:\n                assert ts.utcoffset() == reference_tz.utcoffset(ts.tz_convert(None))\n            return ts\n        else:  # localize -&gt; time remains the same\n            return ts.tz_localize(reference_tz)\n    elif reference_tz is None and ts.tz is not None:\n        return ts.tz_localize(None)\n    return ts\n</code></pre>"},{"location":"api/figure_resampler/","title":"figure_resampler","text":"<p>Module withholding wrappers for the plotly <code>go.Figure</code> and <code>go.FigureWidget</code> class  which allows bookkeeping and back-end based resampling of high-frequency sequential data.</p> <p>Tip</p> <p>The term <code>high-frequency</code> actually refers very large amounts of sequential data.</p>"},{"location":"api/figure_resampler/#figure_resampler.FigureResampler","title":"<code>FigureResampler</code>","text":"<p>               Bases: <code>AbstractFigureAggregator</code>, <code>Figure</code></p> <p>Data aggregation functionality for <code>go.Figures</code>.</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>class FigureResampler(AbstractFigureAggregator, go.Figure):\n    \"\"\"Data aggregation functionality for ``go.Figures``.\"\"\"\n\n    def __init__(\n        self,\n        figure: BaseFigure | dict = None,\n        convert_existing_traces: bool = True,\n        default_n_shown_samples: int = 1000,\n        default_downsampler: AbstractAggregator = MinMaxLTTB(),\n        default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n        resampled_trace_prefix_suffix: Tuple[str, str] = (\n            '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n            \"\",\n        ),\n        show_mean_aggregation_size: bool = True,\n        convert_traces_kwargs: dict | None = None,\n        create_overview: bool = False,\n        overview_row_idxs: list = None,\n        overview_kwargs: dict = {},\n        verbose: bool = False,\n        show_dash_kwargs: dict | None = None,\n    ):\n        \"\"\"Initialize a dynamic aggregation data mirror using a dash web app.\n\n        Parameters\n        ----------\n        figure: BaseFigure\n            The figure that will be decorated. Can be either an empty figure\n            (e.g., ``go.Figure()``, ``make_subplots()``, ``go.FigureWidget``) or an\n            existing figure.\n        convert_existing_traces: bool\n            A bool indicating whether the high-frequency traces of the passed ``figure``\n            should be resampled, by default True. Hence, when set to False, the\n            high-frequency traces of the passed ``figure`` will not be resampled.\n        default_n_shown_samples: int, optional\n            The default number of samples that will be shown for each trace,\n            by default 1000.\\n\n            !!! note\n                - This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n                - If a trace withholds fewer datapoints than this parameter,\n                  the data will *not* be aggregated.\n        default_downsampler: AbstractAggregator, optional\n            An instance which implements the AbstractAggregator interface and\n            will be used as default downsampler, by default ``MinMaxLTTB`` with\n            ``MinMaxLTTB`` is a heuristic to the LTTB algorithm that uses pre-selection\n            of min-max values (default 4 per bin) to speed up LTTB (as now only 4 values\n            per bin are considered by LTTB). This min-max ratio of 4 can be changed by\n            initializing ``MinMaxLTTB`` with a different value for the ``minmax_ratio``\n            parameter. \\n\n            !!! note\n                This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n        default_gap_handler: AbstractGapHandler, optional\n            An instance which implements the AbstractGapHandler interface and\n            will be used as default gap handler, by default ``MedDiffGapHandler``.\n            ``MedDiffGapHandler`` will determine gaps by first calculating the median\n            aggregated x difference and then thresholding the aggregated x delta on a\n            multiple of this median difference.  \\n\n            !!! note\n                This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n        resampled_trace_prefix_suffix: str, optional\n            A tuple which contains the ``prefix`` and ``suffix``, respectively, which\n            will be added to the trace its legend-name when a resampled version of the\n            trace is shown. By default a bold, orange ``[R]`` is shown as prefix\n            (no suffix is shown).\n        show_mean_aggregation_size: bool, optional\n            Whether the mean aggregation bin size will be added as a suffix to the trace\n            its legend-name, by default True.\n        convert_traces_kwargs: dict, optional\n            A dict of kwargs that will be passed to the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method and\n            will be used to convert the existing traces. \\n\n            !!! note\n                This argument is only used when the passed ``figure`` contains data and\n                ``convert_existing_traces`` is set to True.\n        create_overview: bool, optional\n            Whether an overview will be added to the figure (also known as rangeslider),\n            by default False. An overview is a bidirectionally linked figure that is\n            placed below the FigureResampler figure and shows a coarse version on which\n            the current view of the FigureResampler figure is highlighted. The overview\n            can be used to quickly navigate through the data by dragging the selection\n            box.\n            !!! note\n                - In the case of subplots, the overview will be created for each subplot\n                  column. Only a single subplot row can be captured in the overview,\n                  this is by default the first row. If you want to customize this\n                  behavior, you can use the `overview_row_idxs` argument.\n                - This functionality is not yet extensively validated. Please report any\n                  issues you encounter on GitHub.\n        overview_row_idxs: list, optional\n            A list of integers corresponding to the row indices (START AT 0) of the\n            subplots columns that should be linked with the column its corresponding\n            overview. By default None, which will result in the first row being utilized\n            for each column.\n        overview_kwargs: dict, optional\n            A dict of kwargs that will be passed to the `update_layout` method of the\n            overview figure, by default {}, which will result in utilizing the\n            [`default`][_DEFAULT_OVERVIEW_LAYOUT_KWARGS] overview layout kwargs.\n        verbose: bool, optional\n            Whether some verbose messages will be printed or not, by default False.\n        show_dash_kwargs: dict, optional\n            A dict that will be used as default kwargs for the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method.\n            !!! note\n                The passed kwargs to the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method will take precedence over these defaults.\n\n        \"\"\"\n        # Parse the figure input before calling `super`\n        if is_figure(figure) and not is_fr(figure):\n            # A go.Figure\n            # =&gt; base case: the figure does not need to be adjusted\n            f = figure\n        else:\n            # Create a new figure object and make sure that the trace uid will not get\n            # adjusted when they are added.\n            f = self._get_figure_class(go.Figure)()\n            f._data_validator.set_uid = False\n\n            if isinstance(figure, BaseFigure):\n                # A base figure object, can be;\n                # - a go.FigureWidget\n                # - a plotly-resampler figure: subclass of AbstractFigureAggregator\n                # =&gt; we first copy the layout, grid_str and grid ref\n                f.layout = figure.layout\n                f._grid_str = figure._grid_str\n                f._grid_ref = figure._grid_ref\n                f.add_traces(figure.data)\n            elif isinstance(figure, dict) and (\n                \"data\" in figure or \"layout\" in figure  # or \"frames\" in figure  # TODO\n            ):\n                # A figure as a dict, can be;\n                # - a plotly figure as a dict (after calling `fig.to_dict()`)\n                # - a pickled (plotly-resampler) figure (after loading a pickled figure)\n                # =&gt; we first copy the layout, grid_str and grid ref\n                f.layout = figure.get(\"layout\")\n                f._grid_str = figure.get(\"_grid_str\")\n                f._grid_ref = figure.get(\"_grid_ref\")\n                f.add_traces(figure.get(\"data\"))\n                # `pr_props` is not None when loading a pickled plotly-resampler figure\n                f._pr_props = figure.get(\"pr_props\")\n                # `f._pr_props`` is an attribute to store properties of a\n                # plotly-resampler figure. This attribute is only used to pass\n                # information to the super() constructor. Once the super constructor is\n                # called, the attribute is removed.\n\n                # f.add_frames(figure.get(\"frames\")) TODO\n            elif isinstance(figure, (dict, list)):\n                # A single trace dict or a list of traces\n                f.add_traces(figure)\n\n        self._show_dash_kwargs = (\n            show_dash_kwargs if show_dash_kwargs is not None else {}\n        )\n\n        super().__init__(\n            f,\n            convert_existing_traces,\n            default_n_shown_samples,\n            default_downsampler,\n            default_gap_handler,\n            resampled_trace_prefix_suffix,\n            show_mean_aggregation_size,\n            convert_traces_kwargs,\n            verbose,\n        )\n\n        if isinstance(figure, AbstractFigureAggregator):\n            # Copy the `_hf_data` if the previous figure was an AbstractFigureAggregator\n            # and adjust the default `max_n_samples` and `downsampler`\n            self._hf_data.update(\n                self._copy_hf_data(figure._hf_data, adjust_default_values=True)\n            )\n\n            # Note: This hack ensures that the this figure object initially uses\n            # data of the whole view. More concretely; we create a dict\n            # serialization figure and adjust the hf-traces to the whole view\n            # with the check-update method (by passing no range / filter args)\n            with self.batch_update():\n                graph_dict: dict = self._get_current_graph()\n                update_indices = self._check_update_figure_dict(graph_dict)\n                for idx in update_indices:\n                    self.data[idx].update(graph_dict[\"data\"][idx])\n\n        self._create_overview = create_overview\n        # update the overview layout\n        overview_layout_kwargs = _DEFAULT_OVERVIEW_LAYOUT_KWARGS.copy()\n        overview_layout_kwargs.update(overview_kwargs)\n        self._overview_layout_kwargs = overview_layout_kwargs\n\n        # array representing the row indices per column (START AT 0) of the subplot\n        # that should be linked with the columns corresponding overview.\n        # By default, the first row (i.e. index 0) will be utilized for each column\n        self._overview_row_idxs = self._parse_subplot_row_indices(overview_row_idxs)\n\n        # The FigureResampler needs a dash app\n        self._app: dash.Dash | None = None\n        self._port: int | None = None\n        self._host: str | None = None\n        # Certain functions will be different when using persistent inline\n        # (namely `show_dash` and `stop_callback`)\n\n    def _get_subplot_rows_and_cols_from_grid(self) -&gt; Tuple[int, int]:\n        \"\"\"Get the number of rows and columns of the figure's grid.\n\n        Returns\n        -------\n        Tuple[int, int]\n            The number of rows and columns of the figure's grid, respectively.\n        \"\"\"\n        if self._grid_ref is None:  # case: go.Figure (no subplots)\n            return (1, 1)\n        # TODO: not 100% sure whether this is correct\n        return (len(self._grid_ref), len(self._grid_ref[0]))\n\n    def _parse_subplot_row_indices(self, row_indices: list = None) -&gt; List[int]:\n        \"\"\"Verify whether the passed row indices are valid.\n\n        Parameters\n        ----------\n        row_indices: list, optional\n            A list of integers representing the row indices for which the overview\n            should be created. The length of the list should be equal to the number of\n            columns of the figure. Each element of the list should be smaller than the\n            number of rows of the figure (thus note that the row indices start at 0). By\n            default None, which will result in the first row being utilized for each\n            column.\n            !!! note\n                When you do not want to use an overview of a certain column (because\n                a certain subplot spans more than 1 column), you can specify this by\n                setting that respecive row_index value to `None`.\n\n                For instance, the sbuplot on row 2, col 1 spans two coloms. So when you\n                intend to utilize that subplot within the overview, you want to specify\n                the row_indices as: `[1, None, ...]`\n\n        Returns\n        -------\n        List[int]\n            A list of integers representing the row indices per subplot column.\n\n        \"\"\"\n        n_rows, n_cols = self._get_subplot_rows_and_cols_from_grid()\n\n        # By default, the first row is utilized to set the row indices\n        if row_indices is None:\n            return [0] * n_cols\n\n        # perform some checks on the row indices\n        assert isinstance(row_indices, list), \"row indices must be a list\"\n        assert (\n            len(row_indices) == n_cols\n        ), \"the number of row indices must be equal to the number of columns\"\n        assert all(\n            [(li is None) or (0 &lt;= li &lt; n_rows) for li in row_indices]\n        ), \"row indices must be smaller than the number of rows\"\n\n        return row_indices\n\n    # determines which subplot data to take from main and put into coarse\n    def _remove_other_axes_for_coarse(self) -&gt; go.Figure:\n        # base case: no rows and cols to filter\n        if self._grid_ref is None:  # case: go.Figure (no subplots)\n            return self\n\n        # Create the grid specification for the overview figure (in `reduced_grid_ref`)\n        # The trace_list and the 2 axis lists are 1D arrays holding track of the traces\n        # and axes to track.\n        reduced_grid_ref = [[]]\n\n        # Store the xaxis keys (e.g., x2) of the traces to keep\n        trace_list = []\n        # Store the xaxis and yaxis layout keys of the traces to keep (e.g., xaxis2)\n        layout_xaxis_list, layout_yaxis_list = [], []\n        for col_idx, row_idx in enumerate(self._overview_row_idxs):\n            if row_idx is None:  # skip None value\n                continue\n\n            overview_grid_ref = self._grid_ref[row_idx][col_idx]\n            reduced_grid_ref[0].append(overview_grid_ref)  # [0] bc 1 row in overview\n            for subplot in overview_grid_ref:\n                trace_list.append(subplot.trace_kwargs[\"xaxis\"])\n\n                # store the layout keys so that we can retain the exact layout\n                xaxis_key, yaxis_key = subplot.layout_keys\n                layout_yaxis_list.append(yaxis_key)\n                layout_xaxis_list.append(xaxis_key)\n        # print(\"layout_list\", l_xaxis_list, l_yaxis_list)\n        # print(\"trace_list\", trace_list)\n\n        fig_dict = self._get_current_graph()  # a copy of the current graph\n\n        # copy the data from the relevant overview subplots\n        reduced_fig_dict = {\n            \"data\": [],\n            \"layout\": {\"template\": fig_dict[\"layout\"][\"template\"]},\n        }\n        # NOTE: we enumerate over the data of the full figure so that we can utilize the\n        # trace index to mimic the colorway.\n        for i, trace in enumerate(fig_dict[\"data\"]):\n            # NOTE: the interplay between line_color and marker_color seems to work in\n            # this implementation - a more thorough investigation might be needed\n            if trace.get(\"xaxis\", \"x\") in trace_list:\n                if \"line\" not in trace:\n                    trace[\"line\"] = {}\n                # Ensure that the same color is utilized\n                trace[\"line\"][\"color\"] = (\n                    self._layout_obj.template.layout.colorway[i]\n                    if self.data[i].line.color is None\n                    else self.data[i].line.color\n                )\n                # add the trace to the reduced figure\n                reduced_fig_dict[\"data\"].append(trace)\n\n        # Add the relevant layout keys to the reduced figure\n        for k, v in fig_dict[\"layout\"].items():\n            if k in layout_xaxis_list:\n                reduced_fig_dict[\"layout\"][k] = v\n            elif k in layout_yaxis_list:\n                v = v.copy()\n                # set the domain to [0, 1] to ensure that the overview figure has the\n                # global y-axis range\n                v.update({\"domain\": [0, 1]})\n                reduced_fig_dict[\"layout\"][k] = v\n\n        # Create a figure object using the reduced figure dict\n        reduced_fig = go.Figure(layout=reduced_fig_dict[\"layout\"])\n        reduced_fig._grid_ref = reduced_grid_ref\n        # Ensure that the trace uid is not adjusted, this must be set prior to adding\n        # the trace data. Otherwise, data aggregation will not work.\n        reduced_fig._data_validator.set_uid = False\n        reduced_fig.add_traces(reduced_fig_dict[\"data\"])\n        return reduced_fig\n\n    def _create_overview_figure(self) -&gt; go.Figure:\n        # create a new coarse fig\n        reduced_fig = self._remove_other_axes_for_coarse()\n\n        # Resample the coarse figure using 3x the default aggregation size to ensure\n        # that it contains sufficient details\n        coarse_fig_hf = FigureResampler(\n            reduced_fig,\n            default_n_shown_samples=3 * self._global_n_shown_samples,\n        )\n\n        # NOTE: this way we can alter props without altering the original hf data\n        # NOTE: this also copies the default aggregation functionality to the coarse figure\n        coarse_fig_hf._hf_data = {uid: trc.copy() for uid, trc in self._hf_data.items()}\n        for trace in coarse_fig_hf.hf_data:\n            trace[\"max_n_samples\"] *= 3\n\n        coarse_fig_dict = coarse_fig_hf._get_current_graph()\n        # add the 3x max_n_samples coarse figure data to the coarse_fig_dict\n        coarse_fig_hf._check_update_figure_dict(coarse_fig_dict)\n        del coarse_fig_hf\n\n        coarse_fig = go.Figure(layout=coarse_fig_dict[\"layout\"])\n        coarse_fig._grid_ref = reduced_fig._grid_ref\n        coarse_fig._data_validator.set_uid = False\n        coarse_fig.add_traces(coarse_fig_dict[\"data\"])\n        # remove any update menus for the coarse figure\n        coarse_fig.layout.pop(\"updatemenus\", None)\n        # remove the `rangeselector` options for all 'axis' keys in the layout of the\n        # coarse figure\n        for k, v in coarse_fig.layout._props.items():\n            if \"axis\" in k:\n                v.pop(\"rangeselector\", None)\n\n        # height of the overview scales with the height of the dynamic view\n        coarse_fig.update_layout(\n            **self._overview_layout_kwargs,\n            hovermode=False,\n            clickmode=\"event+select\",\n            dragmode=\"select\",\n        )\n        # Hide the grid\n        hide_kwrgs = dict(\n            showgrid=False,\n            showticklabels=False,\n            zeroline=False,\n            title_text=None,\n            mirror=True,\n            ticks=\"\",\n            showline=False,\n            linecolor=\"black\",\n        )\n        coarse_fig.update_yaxes(**hide_kwrgs)\n        coarse_fig.update_xaxes(**hide_kwrgs)\n\n        vrect_props = dict(\n            **dict(line_width=0, x0=0, x1=1),\n            **dict(fillcolor=\"lightblue\", opacity=0.25, layer=\"above\"),\n        )\n\n        if self._grid_ref is None:  # case: go.Figure (no subplots)\n            # set the fixed range to True\n            coarse_fig[\"layout\"][\"xaxis\"][\"fixedrange\"] = True\n            coarse_fig[\"layout\"][\"yaxis\"][\"fixedrange\"] = True\n\n            # add a shading to the overview\n            coarse_fig.add_vrect(xref=\"x domain\", **vrect_props)\n            return coarse_fig\n\n        col_idx_overview = 0\n        for col_idx, row_idx in enumerate(self._overview_row_idxs):\n            if row_idx is None:  # skip the None value\n                continue\n\n            # we will only use the first grid-ref (as we will otherwise have multiple\n            # overlapping selection boxes)\n            for subplot in self._grid_ref[row_idx][col_idx][:1]:\n                xaxis_key, yaxis_key = subplot.layout_keys\n\n                # set the fixed range to True\n                coarse_fig[\"layout\"][xaxis_key][\"fixedrange\"] = True\n                coarse_fig[\"layout\"][yaxis_key][\"fixedrange\"] = True\n\n                # add a shading to the overview\n                coarse_fig.add_vrect(\n                    col=col_idx_overview + 1,\n                    xref=f\"{subplot.trace_kwargs['xaxis']} domain\",\n                    **vrect_props,\n                )\n\n            col_idx_overview += 1  # only increase the index when not None\n\n        return coarse_fig\n\n    def show_dash(\n        self,\n        mode=None,\n        config: dict | None = None,\n        init_dash_kwargs: dict | None = None,\n        graph_properties: dict | None = None,\n        **kwargs,\n    ):\n        \"\"\"Registers the `update_graph` callback &amp; show the figure in a dash app.\n\n        Parameters\n        ----------\n        mode: str, optional\n            Display mode. One of:\\n\n              * ``\"external\"``: The URL of the app will be displayed in the notebook\n                output cell. Clicking this URL will open the app in the default\n                web browser.\n              * ``\"inline\"``: The app will be displayed inline in the notebook output\n                cell in an iframe.\n              * ``\"inline_persistent\"``: The app will be displayed inline in the\n                notebook output cell in an iframe, if the app is not reachable a static\n                image of the figure is shown. Hence this is a persistent version of the\n                ``\"inline\"`` mode, allowing users to see a static figure in other\n                environments, browsers, etc.\n\n                !!! note\n\n                    This mode requires the ``kaleido`` and ``flask_cors`` package.\n                    Install them : ``pip install plotly_resampler[inline_persistent]``\n                    or ``pip install kaleido flask_cors``.\n\n              * ``\"jupyterlab\"``: The app will be displayed in a dedicated tab in the\n                JupyterLab interface. Requires JupyterLab and the ``jupyterlab-dash``\n                extension.\n            By default None, which will result in the same behavior as ``\"external\"``.\n        config: dict, optional\n            The configuration options for displaying this figure, by default None.\n            This ``config`` parameter is the same as the dict that you would pass as\n            ``config`` argument to the `show` method.\n            See more [https://plotly.com/python/configuration-options/](https://plotly.com/python/configuration-options/)\n        init_dash_kwargs: dict, optional\n            Keyword arguments for the Dash app constructor.\n            !!! note\n                This variable is of special interest when working in a jupyterhub +\n                kubernetes environment. In this case, user notebook servers are spawned\n                as separate pods and user access to those servers are proxied via\n                jupyterhub. Dash requires the `requests_pathname_prefix` to be set on\n                __init__ - which can be done via this `init_dash_kwargs` argument.\n                Note that you should also pass the `jupyter_server_url` to the\n                `show_dash` method.\n                More details: https://github.com/predict-idlab/plotly-resampler/issues/265\n        graph_properties: dict, optional\n            Dictionary of (keyword, value) for the properties that should be passed to\n            the dcc.Graph, by default None.\n            e.g.: `{\"style\": {\"width\": \"50%\"}}`\n            Note: \"config\" is not allowed as key in this dict, as there is a distinct\n            ``config`` parameter for this property in this method.\n            See more [https://dash.plotly.com/dash-core-components/graph](https://dash.plotly.com/dash-core-components/graph)\n        **kwargs: dict\n            kwargs for the ``app.run_server()`` method, e.g., port=8037.\n            !!! note\n                These kwargs take precedence over the ones that are passed to the\n                constructor via the ``show_dash_kwargs`` argument.\n\n        \"\"\"\n        available_modes = list(dash._jupyter.JupyterDisplayMode.__args__) + [\n            \"inline_persistent\"\n        ]\n        assert (\n            mode is None or mode in available_modes\n        ), f\"mode must be one of {available_modes}\"\n        graph_properties = {} if graph_properties is None else graph_properties\n        assert \"config\" not in graph_properties  # There is a param for config\n        if self[\"layout\"][\"autosize\"] is True and self[\"layout\"][\"height\"] is None:\n            graph_properties.setdefault(\"style\", {}).update({\"height\": \"100%\"})\n\n        # 0. Check if the traces need to be updated when there is a xrange set\n        # This will be the case when the users has set a xrange (via the `update_layout`\n        # or `update_xaxes` methods`)\n        relayout_dict = {}\n        for xaxis_str in self._xaxis_list:\n            x_range = self.layout[xaxis_str].range\n            if x_range:  # when not None\n                relayout_dict[f\"{xaxis_str}.range[0]\"] = x_range[0]\n                relayout_dict[f\"{xaxis_str}.range[1]\"] = x_range[1]\n        if relayout_dict:  # when not empty\n            update_data = self._construct_update_data(relayout_dict)\n\n            if not self._is_no_update(update_data):  # when there is an update\n                with self.batch_update():\n                    # First update the layout (first item of update_data)\n                    self.layout.update(self._parse_relayout(update_data[0]))\n\n                    # Then update the data\n                    for updated_trace in update_data[1:]:\n                        trace_idx = updated_trace.pop(\"index\")\n                        self.data[trace_idx].update(updated_trace)\n\n        # 1. Construct the Dash app layout\n        init_dash_kwargs = {} if init_dash_kwargs is None else init_dash_kwargs\n        if self._create_overview:\n            # fmt: off\n            # Add the assets folder to the init_dash_kwargs\n            init_dash_kwargs[\"assets_folder\"] = os.path.relpath(ASSETS_FOLDER, os.getcwd())\n            # Also include the lodash script, as the client-side callbacks uses this\n            init_dash_kwargs[\"external_scripts\"] = [\"https://cdn.jsdelivr.net/npm/lodash/lodash.min.js\" ]\n            # fmt: on\n\n        # fmt: off\n        div = dash.html.Div(\n            children=[\n                dash.dcc.Graph(\n                    id=\"resample-figure\", figure=self, config=config, **graph_properties\n                )\n            ],\n            style={\n                \"display\": \"flex\", \"flex-flow\": \"column\",\n                \"height\": \"95vh\", \"width\": \"100%\",\n            },\n        )\n        # fmt: on\n        if self._create_overview:\n            overview_config = config.copy() if config is not None else {}\n            overview_config[\"displayModeBar\"] = False\n            coarse_fig = self._create_overview_figure()\n            div.children += [\n                dash.dcc.Graph(\n                    id=\"overview-figure\",\n                    figure=coarse_fig,\n                    config=overview_config,\n                    **graph_properties,\n                ),\n            ]\n\n        # Create the app, populate the layout and register the resample callback\n        app = dash.Dash(\"local_app\", **init_dash_kwargs)\n        app.layout = div\n        self.register_update_graph_callback(\n            app,\n            \"resample-figure\",\n            \"overview-figure\" if self._create_overview else None,\n        )\n\n        # 2. Run the app\n        height_param = \"height\" if mode == \"inline_persistent\" else \"jupyter_height\"\n        if \"inline\" in mode and height_param not in kwargs:\n            # If app height is not specified -&gt; re-use figure height for inline dash app\n            #  Note: default layout height is 450 (whereas default app height is 650)\n            #  See: https://plotly.com/python/reference/layout/#layout-height\n            fig_height = self.layout.height if self.layout.height is not None else 450\n            kwargs[height_param] = fig_height + 18\n\n        # kwargs take precedence over the show_dash_kwargs\n        kwargs = {**self._show_dash_kwargs, **kwargs}\n\n        # Store the app information, so it can be killed\n        self._app = app\n        self._host = kwargs.get(\"host\", \"127.0.0.1\")\n        self._port = kwargs.get(\"port\", \"8050\")\n\n        # function signatures are slightly different for the (Jupyter)Dash and the\n        # JupyterDashInlinePersistent implementations\n        if mode == \"inline_persistent\":\n            from .jupyter_dash_persistent_inline_output import (\n                JupyterDashPersistentInlineOutput,\n            )\n\n            jpi = JupyterDashPersistentInlineOutput(self)\n            jpi.run_app(app=app, **kwargs)\n        else:\n            app.run(jupyter_mode=mode, **kwargs)\n\n    def stop_server(self, warn: bool = True):\n        \"\"\"Stop the running dash-app.\n\n        Parameters\n        ----------\n        warn: bool\n            Whether a warning message will be shown or  not, by default True.\n\n        !!! warning\n\n            This only works if the dash-app was started with [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash].\n        \"\"\"\n        if self._app is not None:\n            servers_dict = dash.jupyter_dash._servers\n            old_server = servers_dict.get((self._host, self._port))\n            if old_server:\n                old_server.shutdown()\n            del servers_dict[(self._host, self._port)]\n        elif warn:\n            warnings.warn(\n                \"Could not stop the server, either the \\n\"\n                + \"\\t- 'show-dash' method was not called, or \\n\"\n                + \"\\t- the dash-server wasn't started with 'show_dash'\"\n            )\n\n    def construct_update_data_patch(\n        self, relayout_data: dict\n    ) -&gt; Union[dash.Patch, dash.no_update]:\n        \"\"\"Construct the Patch of the to-be-updated front-end data, based on the layout\n        change.\n\n        Attention\n        ---------\n        This method is tightly coupled with Dash app callbacks. It takes the front-end\n        figure its ``relayoutData`` as input and returns the ``dash.Patch`` which needs\n        to be sent to the ``figure`` property for the corresponding ``dcc.Graph``.\n\n        Parameters\n        ----------\n        relayout_data: dict\n            A dict containing the ``relayoutData`` (i.e., the changed layout data) of\n            the corresponding front-end graph.\n\n        Returns\n        -------\n        dash.Patch:\n            The Patch object containing the figure updates which needs to be sent to\n            the front-end.\n\n        \"\"\"\n        update_data = self._construct_update_data(relayout_data)\n        if not isinstance(update_data, list) or len(update_data) &lt;= 1:\n            return dash.no_update\n\n        patched_figure = dash.Patch()  # create patch\n        for trace in update_data[1:]:  # skip first item as it contains the relayout\n            trace_index = trace.pop(\"index\")  # the index of the corresponding trace\n            # All the other items are the trace properties which needs to be updated\n            for k, v in trace.items():\n                # NOTE: we need to use the `patched_figure` as a dict, and not\n                # `patched_figure.data` as the latter will replace **all** the\n                # data for the corresponding trace, and we just want to update the\n                # specific trace its properties.\n                patched_figure[\"data\"][trace_index][k] = v\n        return patched_figure\n\n    def register_update_graph_callback(\n        self,\n        app: dash.Dash,\n        graph_id: str,\n        coarse_graph_id: Optional[str] = None,\n    ):\n        \"\"\"Register the [`construct_update_data_patch`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.construct_update_data_patch]\n        method as callback function to the passed dash-app.\n\n        Parameters\n        ----------\n        app: Union[dash.Dash, JupyterDash]\n            The app in which the callback will be registered.\n        graph_id:\n            The id of the ``dcc.Graph``-component which withholds the to-be resampled\n            Figure.\n        coarse_graph_id: str, optional\n            The id of the ``dcc.Graph``-component which withholds the coarse overview\n            Figure, by default None.\n\n        \"\"\"\n        # As we use the figure again as output, we need to set: allow_duplicate=True\n\n        if coarse_graph_id is not None:\n            # update pr graph range with overview selection\n            app.clientside_callback(\n                dash.ClientsideFunction(\n                    namespace=\"clientside\", function_name=\"coarse_to_main\"\n                ),\n                dash.Output(graph_id, \"id\", allow_duplicate=True),\n                dash.Input(coarse_graph_id, \"selectedData\"),\n                dash.State(graph_id, \"id\"),\n                dash.State(coarse_graph_id, \"id\"),\n                prevent_initial_call=True,\n            )\n\n            # update selectbox with clientside callback\n            app.clientside_callback(\n                dash.ClientsideFunction(\n                    namespace=\"clientside\", function_name=\"main_to_coarse\"\n                ),\n                dash.Output(coarse_graph_id, \"id\", allow_duplicate=True),\n                dash.Input(graph_id, \"relayoutData\"),\n                dash.State(coarse_graph_id, \"id\"),\n                dash.State(graph_id, \"id\"),\n                prevent_initial_call=True,\n            )\n\n        app.callback(\n            dash.Output(graph_id, \"figure\", allow_duplicate=True),\n            dash.Input(graph_id, \"relayoutData\"),\n            prevent_initial_call=True,\n        )(self.construct_update_data_patch)\n\n    def _get_pr_props_keys(self) -&gt; List[str]:\n        # Add the additional plotly-resampler properties of this class\n        return super()._get_pr_props_keys() + [\"_show_dash_kwargs\"]\n\n    def _ipython_display_(self):\n        # To display the figure inline as a dash app\n        self.show_dash(mode=\"inline\")\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureResampler.__init__","title":"<code>__init__(figure=None, convert_existing_traces=True, default_n_shown_samples=1000, default_downsampler=MinMaxLTTB(), default_gap_handler=MedDiffGapHandler(), resampled_trace_prefix_suffix=('&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ', ''), show_mean_aggregation_size=True, convert_traces_kwargs=None, create_overview=False, overview_row_idxs=None, overview_kwargs={}, verbose=False, show_dash_kwargs=None)</code>","text":"<p>Initialize a dynamic aggregation data mirror using a dash web app.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>BaseFigure | dict</code> <p>The figure that will be decorated. Can be either an empty figure (e.g., <code>go.Figure()</code>, <code>make_subplots()</code>, <code>go.FigureWidget</code>) or an existing figure.</p> <code>None</code> <code>convert_existing_traces</code> <code>bool</code> <p>A bool indicating whether the high-frequency traces of the passed <code>figure</code> should be resampled, by default True. Hence, when set to False, the high-frequency traces of the passed <code>figure</code> will not be resampled.</p> <code>True</code> <code>default_n_shown_samples</code> <code>int</code> <p>The default number of samples that will be shown for each trace, by default 1000.</p> <p>Note</p> <ul> <li>This can be overridden within the <code>add_trace</code> method.</li> <li>If a trace withholds fewer datapoints than this parameter,   the data will not be aggregated.</li> </ul> <code>1000</code> <code>default_downsampler</code> <code>AbstractAggregator</code> <p>An instance which implements the AbstractAggregator interface and will be used as default downsampler, by default <code>MinMaxLTTB</code> with <code>MinMaxLTTB</code> is a heuristic to the LTTB algorithm that uses pre-selection of min-max values (default 4 per bin) to speed up LTTB (as now only 4 values per bin are considered by LTTB). This min-max ratio of 4 can be changed by initializing <code>MinMaxLTTB</code> with a different value for the <code>minmax_ratio</code> parameter. </p> <p>Note</p> <p>This can be overridden within the <code>add_trace</code> method.</p> <code>MinMaxLTTB()</code> <code>default_gap_handler</code> <code>AbstractGapHandler</code> <p>An instance which implements the AbstractGapHandler interface and will be used as default gap handler, by default <code>MedDiffGapHandler</code>. <code>MedDiffGapHandler</code> will determine gaps by first calculating the median aggregated x difference and then thresholding the aggregated x delta on a multiple of this median difference.  </p> <p>Note</p> <p>This can be overridden within the <code>add_trace</code> method.</p> <code>MedDiffGapHandler()</code> <code>resampled_trace_prefix_suffix</code> <code>Tuple[str, str]</code> <p>A tuple which contains the <code>prefix</code> and <code>suffix</code>, respectively, which will be added to the trace its legend-name when a resampled version of the trace is shown. By default a bold, orange <code>[R]</code> is shown as prefix (no suffix is shown).</p> <code>('&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ', '')</code> <code>show_mean_aggregation_size</code> <code>bool</code> <p>Whether the mean aggregation bin size will be added as a suffix to the trace its legend-name, by default True.</p> <code>True</code> <code>convert_traces_kwargs</code> <code>dict | None</code> <p>A dict of kwargs that will be passed to the <code>add_trace</code> method and will be used to convert the existing traces. </p> <p>Note</p> <p>This argument is only used when the passed <code>figure</code> contains data and <code>convert_existing_traces</code> is set to True.</p> <code>None</code> <code>create_overview</code> <code>bool</code> <p>Whether an overview will be added to the figure (also known as rangeslider), by default False. An overview is a bidirectionally linked figure that is placed below the FigureResampler figure and shows a coarse version on which the current view of the FigureResampler figure is highlighted. The overview can be used to quickly navigate through the data by dragging the selection box.</p> <p>Note</p> <ul> <li>In the case of subplots, the overview will be created for each subplot   column. Only a single subplot row can be captured in the overview,   this is by default the first row. If you want to customize this   behavior, you can use the <code>overview_row_idxs</code> argument.</li> <li>This functionality is not yet extensively validated. Please report any   issues you encounter on GitHub.</li> </ul> <code>False</code> <code>overview_row_idxs</code> <code>list</code> <p>A list of integers corresponding to the row indices (START AT 0) of the subplots columns that should be linked with the column its corresponding overview. By default None, which will result in the first row being utilized for each column.</p> <code>None</code> <code>overview_kwargs</code> <code>dict</code> <p>A dict of kwargs that will be passed to the <code>update_layout</code> method of the overview figure, by default {}, which will result in utilizing the <code>default</code> overview layout kwargs.</p> <code>{}</code> <code>verbose</code> <code>bool</code> <p>Whether some verbose messages will be printed or not, by default False.</p> <code>False</code> <code>show_dash_kwargs</code> <code>dict | None</code> <p>A dict that will be used as default kwargs for the <code>show_dash</code> method.</p> <p>Note</p> <p>The passed kwargs to the <code>show_dash</code> method will take precedence over these defaults.</p> <code>None</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def __init__(\n    self,\n    figure: BaseFigure | dict = None,\n    convert_existing_traces: bool = True,\n    default_n_shown_samples: int = 1000,\n    default_downsampler: AbstractAggregator = MinMaxLTTB(),\n    default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n    resampled_trace_prefix_suffix: Tuple[str, str] = (\n        '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n        \"\",\n    ),\n    show_mean_aggregation_size: bool = True,\n    convert_traces_kwargs: dict | None = None,\n    create_overview: bool = False,\n    overview_row_idxs: list = None,\n    overview_kwargs: dict = {},\n    verbose: bool = False,\n    show_dash_kwargs: dict | None = None,\n):\n    \"\"\"Initialize a dynamic aggregation data mirror using a dash web app.\n\n    Parameters\n    ----------\n    figure: BaseFigure\n        The figure that will be decorated. Can be either an empty figure\n        (e.g., ``go.Figure()``, ``make_subplots()``, ``go.FigureWidget``) or an\n        existing figure.\n    convert_existing_traces: bool\n        A bool indicating whether the high-frequency traces of the passed ``figure``\n        should be resampled, by default True. Hence, when set to False, the\n        high-frequency traces of the passed ``figure`` will not be resampled.\n    default_n_shown_samples: int, optional\n        The default number of samples that will be shown for each trace,\n        by default 1000.\\n\n        !!! note\n            - This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n            - If a trace withholds fewer datapoints than this parameter,\n              the data will *not* be aggregated.\n    default_downsampler: AbstractAggregator, optional\n        An instance which implements the AbstractAggregator interface and\n        will be used as default downsampler, by default ``MinMaxLTTB`` with\n        ``MinMaxLTTB`` is a heuristic to the LTTB algorithm that uses pre-selection\n        of min-max values (default 4 per bin) to speed up LTTB (as now only 4 values\n        per bin are considered by LTTB). This min-max ratio of 4 can be changed by\n        initializing ``MinMaxLTTB`` with a different value for the ``minmax_ratio``\n        parameter. \\n\n        !!! note\n            This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n    default_gap_handler: AbstractGapHandler, optional\n        An instance which implements the AbstractGapHandler interface and\n        will be used as default gap handler, by default ``MedDiffGapHandler``.\n        ``MedDiffGapHandler`` will determine gaps by first calculating the median\n        aggregated x difference and then thresholding the aggregated x delta on a\n        multiple of this median difference.  \\n\n        !!! note\n            This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n    resampled_trace_prefix_suffix: str, optional\n        A tuple which contains the ``prefix`` and ``suffix``, respectively, which\n        will be added to the trace its legend-name when a resampled version of the\n        trace is shown. By default a bold, orange ``[R]`` is shown as prefix\n        (no suffix is shown).\n    show_mean_aggregation_size: bool, optional\n        Whether the mean aggregation bin size will be added as a suffix to the trace\n        its legend-name, by default True.\n    convert_traces_kwargs: dict, optional\n        A dict of kwargs that will be passed to the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method and\n        will be used to convert the existing traces. \\n\n        !!! note\n            This argument is only used when the passed ``figure`` contains data and\n            ``convert_existing_traces`` is set to True.\n    create_overview: bool, optional\n        Whether an overview will be added to the figure (also known as rangeslider),\n        by default False. An overview is a bidirectionally linked figure that is\n        placed below the FigureResampler figure and shows a coarse version on which\n        the current view of the FigureResampler figure is highlighted. The overview\n        can be used to quickly navigate through the data by dragging the selection\n        box.\n        !!! note\n            - In the case of subplots, the overview will be created for each subplot\n              column. Only a single subplot row can be captured in the overview,\n              this is by default the first row. If you want to customize this\n              behavior, you can use the `overview_row_idxs` argument.\n            - This functionality is not yet extensively validated. Please report any\n              issues you encounter on GitHub.\n    overview_row_idxs: list, optional\n        A list of integers corresponding to the row indices (START AT 0) of the\n        subplots columns that should be linked with the column its corresponding\n        overview. By default None, which will result in the first row being utilized\n        for each column.\n    overview_kwargs: dict, optional\n        A dict of kwargs that will be passed to the `update_layout` method of the\n        overview figure, by default {}, which will result in utilizing the\n        [`default`][_DEFAULT_OVERVIEW_LAYOUT_KWARGS] overview layout kwargs.\n    verbose: bool, optional\n        Whether some verbose messages will be printed or not, by default False.\n    show_dash_kwargs: dict, optional\n        A dict that will be used as default kwargs for the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method.\n        !!! note\n            The passed kwargs to the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method will take precedence over these defaults.\n\n    \"\"\"\n    # Parse the figure input before calling `super`\n    if is_figure(figure) and not is_fr(figure):\n        # A go.Figure\n        # =&gt; base case: the figure does not need to be adjusted\n        f = figure\n    else:\n        # Create a new figure object and make sure that the trace uid will not get\n        # adjusted when they are added.\n        f = self._get_figure_class(go.Figure)()\n        f._data_validator.set_uid = False\n\n        if isinstance(figure, BaseFigure):\n            # A base figure object, can be;\n            # - a go.FigureWidget\n            # - a plotly-resampler figure: subclass of AbstractFigureAggregator\n            # =&gt; we first copy the layout, grid_str and grid ref\n            f.layout = figure.layout\n            f._grid_str = figure._grid_str\n            f._grid_ref = figure._grid_ref\n            f.add_traces(figure.data)\n        elif isinstance(figure, dict) and (\n            \"data\" in figure or \"layout\" in figure  # or \"frames\" in figure  # TODO\n        ):\n            # A figure as a dict, can be;\n            # - a plotly figure as a dict (after calling `fig.to_dict()`)\n            # - a pickled (plotly-resampler) figure (after loading a pickled figure)\n            # =&gt; we first copy the layout, grid_str and grid ref\n            f.layout = figure.get(\"layout\")\n            f._grid_str = figure.get(\"_grid_str\")\n            f._grid_ref = figure.get(\"_grid_ref\")\n            f.add_traces(figure.get(\"data\"))\n            # `pr_props` is not None when loading a pickled plotly-resampler figure\n            f._pr_props = figure.get(\"pr_props\")\n            # `f._pr_props`` is an attribute to store properties of a\n            # plotly-resampler figure. This attribute is only used to pass\n            # information to the super() constructor. Once the super constructor is\n            # called, the attribute is removed.\n\n            # f.add_frames(figure.get(\"frames\")) TODO\n        elif isinstance(figure, (dict, list)):\n            # A single trace dict or a list of traces\n            f.add_traces(figure)\n\n    self._show_dash_kwargs = (\n        show_dash_kwargs if show_dash_kwargs is not None else {}\n    )\n\n    super().__init__(\n        f,\n        convert_existing_traces,\n        default_n_shown_samples,\n        default_downsampler,\n        default_gap_handler,\n        resampled_trace_prefix_suffix,\n        show_mean_aggregation_size,\n        convert_traces_kwargs,\n        verbose,\n    )\n\n    if isinstance(figure, AbstractFigureAggregator):\n        # Copy the `_hf_data` if the previous figure was an AbstractFigureAggregator\n        # and adjust the default `max_n_samples` and `downsampler`\n        self._hf_data.update(\n            self._copy_hf_data(figure._hf_data, adjust_default_values=True)\n        )\n\n        # Note: This hack ensures that the this figure object initially uses\n        # data of the whole view. More concretely; we create a dict\n        # serialization figure and adjust the hf-traces to the whole view\n        # with the check-update method (by passing no range / filter args)\n        with self.batch_update():\n            graph_dict: dict = self._get_current_graph()\n            update_indices = self._check_update_figure_dict(graph_dict)\n            for idx in update_indices:\n                self.data[idx].update(graph_dict[\"data\"][idx])\n\n    self._create_overview = create_overview\n    # update the overview layout\n    overview_layout_kwargs = _DEFAULT_OVERVIEW_LAYOUT_KWARGS.copy()\n    overview_layout_kwargs.update(overview_kwargs)\n    self._overview_layout_kwargs = overview_layout_kwargs\n\n    # array representing the row indices per column (START AT 0) of the subplot\n    # that should be linked with the columns corresponding overview.\n    # By default, the first row (i.e. index 0) will be utilized for each column\n    self._overview_row_idxs = self._parse_subplot_row_indices(overview_row_idxs)\n\n    # The FigureResampler needs a dash app\n    self._app: dash.Dash | None = None\n    self._port: int | None = None\n    self._host: str | None = None\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureResampler.construct_update_data_patch","title":"<code>construct_update_data_patch(relayout_data)</code>","text":"<p>Construct the Patch of the to-be-updated front-end data, based on the layout change.</p> Attention <p>This method is tightly coupled with Dash app callbacks. It takes the front-end figure its <code>relayoutData</code> as input and returns the <code>dash.Patch</code> which needs to be sent to the <code>figure</code> property for the corresponding <code>dcc.Graph</code>.</p> <p>Parameters:</p> Name Type Description Default <code>relayout_data</code> <code>dict</code> <p>A dict containing the <code>relayoutData</code> (i.e., the changed layout data) of the corresponding front-end graph.</p> required <p>Returns:</p> Type Description <code>dash.Patch:</code> <p>The Patch object containing the figure updates which needs to be sent to the front-end.</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def construct_update_data_patch(\n    self, relayout_data: dict\n) -&gt; Union[dash.Patch, dash.no_update]:\n    \"\"\"Construct the Patch of the to-be-updated front-end data, based on the layout\n    change.\n\n    Attention\n    ---------\n    This method is tightly coupled with Dash app callbacks. It takes the front-end\n    figure its ``relayoutData`` as input and returns the ``dash.Patch`` which needs\n    to be sent to the ``figure`` property for the corresponding ``dcc.Graph``.\n\n    Parameters\n    ----------\n    relayout_data: dict\n        A dict containing the ``relayoutData`` (i.e., the changed layout data) of\n        the corresponding front-end graph.\n\n    Returns\n    -------\n    dash.Patch:\n        The Patch object containing the figure updates which needs to be sent to\n        the front-end.\n\n    \"\"\"\n    update_data = self._construct_update_data(relayout_data)\n    if not isinstance(update_data, list) or len(update_data) &lt;= 1:\n        return dash.no_update\n\n    patched_figure = dash.Patch()  # create patch\n    for trace in update_data[1:]:  # skip first item as it contains the relayout\n        trace_index = trace.pop(\"index\")  # the index of the corresponding trace\n        # All the other items are the trace properties which needs to be updated\n        for k, v in trace.items():\n            # NOTE: we need to use the `patched_figure` as a dict, and not\n            # `patched_figure.data` as the latter will replace **all** the\n            # data for the corresponding trace, and we just want to update the\n            # specific trace its properties.\n            patched_figure[\"data\"][trace_index][k] = v\n    return patched_figure\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureResampler.register_update_graph_callback","title":"<code>register_update_graph_callback(app, graph_id, coarse_graph_id=None)</code>","text":"<p>Register the <code>construct_update_data_patch</code> method as callback function to the passed dash-app.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Dash</code> <p>The app in which the callback will be registered.</p> required <code>graph_id</code> <code>str</code> <p>The id of the <code>dcc.Graph</code>-component which withholds the to-be resampled Figure.</p> required <code>coarse_graph_id</code> <code>Optional[str]</code> <p>The id of the <code>dcc.Graph</code>-component which withholds the coarse overview Figure, by default None.</p> <code>None</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def register_update_graph_callback(\n    self,\n    app: dash.Dash,\n    graph_id: str,\n    coarse_graph_id: Optional[str] = None,\n):\n    \"\"\"Register the [`construct_update_data_patch`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.construct_update_data_patch]\n    method as callback function to the passed dash-app.\n\n    Parameters\n    ----------\n    app: Union[dash.Dash, JupyterDash]\n        The app in which the callback will be registered.\n    graph_id:\n        The id of the ``dcc.Graph``-component which withholds the to-be resampled\n        Figure.\n    coarse_graph_id: str, optional\n        The id of the ``dcc.Graph``-component which withholds the coarse overview\n        Figure, by default None.\n\n    \"\"\"\n    # As we use the figure again as output, we need to set: allow_duplicate=True\n\n    if coarse_graph_id is not None:\n        # update pr graph range with overview selection\n        app.clientside_callback(\n            dash.ClientsideFunction(\n                namespace=\"clientside\", function_name=\"coarse_to_main\"\n            ),\n            dash.Output(graph_id, \"id\", allow_duplicate=True),\n            dash.Input(coarse_graph_id, \"selectedData\"),\n            dash.State(graph_id, \"id\"),\n            dash.State(coarse_graph_id, \"id\"),\n            prevent_initial_call=True,\n        )\n\n        # update selectbox with clientside callback\n        app.clientside_callback(\n            dash.ClientsideFunction(\n                namespace=\"clientside\", function_name=\"main_to_coarse\"\n            ),\n            dash.Output(coarse_graph_id, \"id\", allow_duplicate=True),\n            dash.Input(graph_id, \"relayoutData\"),\n            dash.State(coarse_graph_id, \"id\"),\n            dash.State(graph_id, \"id\"),\n            prevent_initial_call=True,\n        )\n\n    app.callback(\n        dash.Output(graph_id, \"figure\", allow_duplicate=True),\n        dash.Input(graph_id, \"relayoutData\"),\n        prevent_initial_call=True,\n    )(self.construct_update_data_patch)\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureResampler.show_dash","title":"<code>show_dash(mode=None, config=None, init_dash_kwargs=None, graph_properties=None, **kwargs)</code>","text":"<p>Registers the <code>update_graph</code> callback &amp; show the figure in a dash app.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <p>Display mode. One of:</p> <ul> <li><code>\"external\"</code>: The URL of the app will be displayed in the notebook     output cell. Clicking this URL will open the app in the default     web browser.</li> <li><code>\"inline\"</code>: The app will be displayed inline in the notebook output     cell in an iframe.</li> <li> <p><code>\"inline_persistent\"</code>: The app will be displayed inline in the     notebook output cell in an iframe, if the app is not reachable a static     image of the figure is shown. Hence this is a persistent version of the     <code>\"inline\"</code> mode, allowing users to see a static figure in other     environments, browsers, etc.</p> <p>Note</p> <p>This mode requires the <code>kaleido</code> and <code>flask_cors</code> package. Install them : <code>pip install plotly_resampler[inline_persistent]</code> or <code>pip install kaleido flask_cors</code>.</p> </li> <li> <p><code>\"jupyterlab\"</code>: The app will be displayed in a dedicated tab in the     JupyterLab interface. Requires JupyterLab and the <code>jupyterlab-dash</code>     extension. By default None, which will result in the same behavior as <code>\"external\"</code>.</p> </li> </ul> <code>None</code> <code>config</code> <code>dict | None</code> <p>The configuration options for displaying this figure, by default None. This <code>config</code> parameter is the same as the dict that you would pass as <code>config</code> argument to the <code>show</code> method. See more https://plotly.com/python/configuration-options/</p> <code>None</code> <code>init_dash_kwargs</code> <code>dict | None</code> <p>Keyword arguments for the Dash app constructor.</p> <p>Note</p> <p>This variable is of special interest when working in a jupyterhub + kubernetes environment. In this case, user notebook servers are spawned as separate pods and user access to those servers are proxied via jupyterhub. Dash requires the <code>requests_pathname_prefix</code> to be set on init - which can be done via this <code>init_dash_kwargs</code> argument. Note that you should also pass the <code>jupyter_server_url</code> to the <code>show_dash</code> method. More details: https://github.com/predict-idlab/plotly-resampler/issues/265</p> <code>None</code> <code>graph_properties</code> <code>dict | None</code> <p>Dictionary of (keyword, value) for the properties that should be passed to the dcc.Graph, by default None. e.g.: <code>{\"style\": {\"width\": \"50%\"}}</code> Note: \u201cconfig\u201d is not allowed as key in this dict, as there is a distinct <code>config</code> parameter for this property in this method. See more https://dash.plotly.com/dash-core-components/graph</p> <code>None</code> <code>**kwargs</code> <p>kwargs for the <code>app.run_server()</code> method, e.g., port=8037.</p> <p>Note</p> <p>These kwargs take precedence over the ones that are passed to the constructor via the <code>show_dash_kwargs</code> argument.</p> <code>{}</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def show_dash(\n    self,\n    mode=None,\n    config: dict | None = None,\n    init_dash_kwargs: dict | None = None,\n    graph_properties: dict | None = None,\n    **kwargs,\n):\n    \"\"\"Registers the `update_graph` callback &amp; show the figure in a dash app.\n\n    Parameters\n    ----------\n    mode: str, optional\n        Display mode. One of:\\n\n          * ``\"external\"``: The URL of the app will be displayed in the notebook\n            output cell. Clicking this URL will open the app in the default\n            web browser.\n          * ``\"inline\"``: The app will be displayed inline in the notebook output\n            cell in an iframe.\n          * ``\"inline_persistent\"``: The app will be displayed inline in the\n            notebook output cell in an iframe, if the app is not reachable a static\n            image of the figure is shown. Hence this is a persistent version of the\n            ``\"inline\"`` mode, allowing users to see a static figure in other\n            environments, browsers, etc.\n\n            !!! note\n\n                This mode requires the ``kaleido`` and ``flask_cors`` package.\n                Install them : ``pip install plotly_resampler[inline_persistent]``\n                or ``pip install kaleido flask_cors``.\n\n          * ``\"jupyterlab\"``: The app will be displayed in a dedicated tab in the\n            JupyterLab interface. Requires JupyterLab and the ``jupyterlab-dash``\n            extension.\n        By default None, which will result in the same behavior as ``\"external\"``.\n    config: dict, optional\n        The configuration options for displaying this figure, by default None.\n        This ``config`` parameter is the same as the dict that you would pass as\n        ``config`` argument to the `show` method.\n        See more [https://plotly.com/python/configuration-options/](https://plotly.com/python/configuration-options/)\n    init_dash_kwargs: dict, optional\n        Keyword arguments for the Dash app constructor.\n        !!! note\n            This variable is of special interest when working in a jupyterhub +\n            kubernetes environment. In this case, user notebook servers are spawned\n            as separate pods and user access to those servers are proxied via\n            jupyterhub. Dash requires the `requests_pathname_prefix` to be set on\n            __init__ - which can be done via this `init_dash_kwargs` argument.\n            Note that you should also pass the `jupyter_server_url` to the\n            `show_dash` method.\n            More details: https://github.com/predict-idlab/plotly-resampler/issues/265\n    graph_properties: dict, optional\n        Dictionary of (keyword, value) for the properties that should be passed to\n        the dcc.Graph, by default None.\n        e.g.: `{\"style\": {\"width\": \"50%\"}}`\n        Note: \"config\" is not allowed as key in this dict, as there is a distinct\n        ``config`` parameter for this property in this method.\n        See more [https://dash.plotly.com/dash-core-components/graph](https://dash.plotly.com/dash-core-components/graph)\n    **kwargs: dict\n        kwargs for the ``app.run_server()`` method, e.g., port=8037.\n        !!! note\n            These kwargs take precedence over the ones that are passed to the\n            constructor via the ``show_dash_kwargs`` argument.\n\n    \"\"\"\n    available_modes = list(dash._jupyter.JupyterDisplayMode.__args__) + [\n        \"inline_persistent\"\n    ]\n    assert (\n        mode is None or mode in available_modes\n    ), f\"mode must be one of {available_modes}\"\n    graph_properties = {} if graph_properties is None else graph_properties\n    assert \"config\" not in graph_properties  # There is a param for config\n    if self[\"layout\"][\"autosize\"] is True and self[\"layout\"][\"height\"] is None:\n        graph_properties.setdefault(\"style\", {}).update({\"height\": \"100%\"})\n\n    # 0. Check if the traces need to be updated when there is a xrange set\n    # This will be the case when the users has set a xrange (via the `update_layout`\n    # or `update_xaxes` methods`)\n    relayout_dict = {}\n    for xaxis_str in self._xaxis_list:\n        x_range = self.layout[xaxis_str].range\n        if x_range:  # when not None\n            relayout_dict[f\"{xaxis_str}.range[0]\"] = x_range[0]\n            relayout_dict[f\"{xaxis_str}.range[1]\"] = x_range[1]\n    if relayout_dict:  # when not empty\n        update_data = self._construct_update_data(relayout_dict)\n\n        if not self._is_no_update(update_data):  # when there is an update\n            with self.batch_update():\n                # First update the layout (first item of update_data)\n                self.layout.update(self._parse_relayout(update_data[0]))\n\n                # Then update the data\n                for updated_trace in update_data[1:]:\n                    trace_idx = updated_trace.pop(\"index\")\n                    self.data[trace_idx].update(updated_trace)\n\n    # 1. Construct the Dash app layout\n    init_dash_kwargs = {} if init_dash_kwargs is None else init_dash_kwargs\n    if self._create_overview:\n        # fmt: off\n        # Add the assets folder to the init_dash_kwargs\n        init_dash_kwargs[\"assets_folder\"] = os.path.relpath(ASSETS_FOLDER, os.getcwd())\n        # Also include the lodash script, as the client-side callbacks uses this\n        init_dash_kwargs[\"external_scripts\"] = [\"https://cdn.jsdelivr.net/npm/lodash/lodash.min.js\" ]\n        # fmt: on\n\n    # fmt: off\n    div = dash.html.Div(\n        children=[\n            dash.dcc.Graph(\n                id=\"resample-figure\", figure=self, config=config, **graph_properties\n            )\n        ],\n        style={\n            \"display\": \"flex\", \"flex-flow\": \"column\",\n            \"height\": \"95vh\", \"width\": \"100%\",\n        },\n    )\n    # fmt: on\n    if self._create_overview:\n        overview_config = config.copy() if config is not None else {}\n        overview_config[\"displayModeBar\"] = False\n        coarse_fig = self._create_overview_figure()\n        div.children += [\n            dash.dcc.Graph(\n                id=\"overview-figure\",\n                figure=coarse_fig,\n                config=overview_config,\n                **graph_properties,\n            ),\n        ]\n\n    # Create the app, populate the layout and register the resample callback\n    app = dash.Dash(\"local_app\", **init_dash_kwargs)\n    app.layout = div\n    self.register_update_graph_callback(\n        app,\n        \"resample-figure\",\n        \"overview-figure\" if self._create_overview else None,\n    )\n\n    # 2. Run the app\n    height_param = \"height\" if mode == \"inline_persistent\" else \"jupyter_height\"\n    if \"inline\" in mode and height_param not in kwargs:\n        # If app height is not specified -&gt; re-use figure height for inline dash app\n        #  Note: default layout height is 450 (whereas default app height is 650)\n        #  See: https://plotly.com/python/reference/layout/#layout-height\n        fig_height = self.layout.height if self.layout.height is not None else 450\n        kwargs[height_param] = fig_height + 18\n\n    # kwargs take precedence over the show_dash_kwargs\n    kwargs = {**self._show_dash_kwargs, **kwargs}\n\n    # Store the app information, so it can be killed\n    self._app = app\n    self._host = kwargs.get(\"host\", \"127.0.0.1\")\n    self._port = kwargs.get(\"port\", \"8050\")\n\n    # function signatures are slightly different for the (Jupyter)Dash and the\n    # JupyterDashInlinePersistent implementations\n    if mode == \"inline_persistent\":\n        from .jupyter_dash_persistent_inline_output import (\n            JupyterDashPersistentInlineOutput,\n        )\n\n        jpi = JupyterDashPersistentInlineOutput(self)\n        jpi.run_app(app=app, **kwargs)\n    else:\n        app.run(jupyter_mode=mode, **kwargs)\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureResampler.stop_server","title":"<code>stop_server(warn=True)</code>","text":"<p>Stop the running dash-app.</p> <p>Parameters:</p> Name Type Description Default <code>warn</code> <code>bool</code> <p>Whether a warning message will be shown or  not, by default True.</p> <code>True</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def stop_server(self, warn: bool = True):\n    \"\"\"Stop the running dash-app.\n\n    Parameters\n    ----------\n    warn: bool\n        Whether a warning message will be shown or  not, by default True.\n\n    !!! warning\n\n        This only works if the dash-app was started with [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash].\n    \"\"\"\n    if self._app is not None:\n        servers_dict = dash.jupyter_dash._servers\n        old_server = servers_dict.get((self._host, self._port))\n        if old_server:\n            old_server.shutdown()\n        del servers_dict[(self._host, self._port)]\n    elif warn:\n        warnings.warn(\n            \"Could not stop the server, either the \\n\"\n            + \"\\t- 'show-dash' method was not called, or \\n\"\n            + \"\\t- the dash-server wasn't started with 'show_dash'\"\n        )\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureWidgetResampler","title":"<code>FigureWidgetResampler</code>","text":"<p>               Bases: <code>AbstractFigureAggregator</code>, <code>FigureWidget</code></p> <p>Data aggregation functionality wrapper for <code>go.FigureWidgets</code>.</p> <p>Warning</p> <ul> <li>This wrapper only works within <code>jupyter</code>-based environments.</li> <li>The <code>.show()</code> method returns a static figure on which the   dynamic resampling cannot be performed. To allow dynamic resampling,   you should just output the <code>FigureWidgetResampler</code> object in a cell.</li> </ul> Source code in <code>plotly_resampler/figure_resampler/figurewidget_resampler.py</code> <pre><code>class FigureWidgetResampler(\n    AbstractFigureAggregator, go.FigureWidget, metaclass=_FigureWidgetResamplerM\n):\n    \"\"\"Data aggregation functionality wrapper for ``go.FigureWidgets``.\n\n    !!! warning\n\n        * This wrapper only works within ``jupyter``-based environments.\n        * The ``.show()`` method returns a **static figure** on which the\n          **dynamic resampling cannot be performed**. To allow dynamic resampling,\n          you should just output the ``FigureWidgetResampler`` object in a cell.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        figure: BaseFigure | dict = None,\n        convert_existing_traces: bool = True,\n        default_n_shown_samples: int = 1000,\n        default_downsampler: AbstractAggregator = MinMaxLTTB(),\n        default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n        resampled_trace_prefix_suffix: Tuple[str, str] = (\n            '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n            \"\",\n        ),\n        show_mean_aggregation_size: bool = True,\n        convert_traces_kwargs: dict | None = None,\n        verbose: bool = False,\n    ):\n        # Parse the figure input before calling `super`\n        f = self._get_figure_class(go.FigureWidget)()\n        f._data_validator.set_uid = False\n\n        if isinstance(figure, BaseFigure):\n            # A base figure object, can be;\n            # - a base plotly figure: go.Figure or go.FigureWidget\n            # - a plotly-resampler figure: subclass of AbstractFigureAggregator\n            # =&gt; we first copy the layout, grid_str and grid ref\n            f.layout = figure.layout\n            f._grid_str = figure._grid_str\n            f._grid_ref = figure._grid_ref\n            f.add_traces(figure.data)\n        elif isinstance(figure, dict) and (\n            \"data\" in figure or \"layout\" in figure  # or \"frames\" in figure  # TODO\n        ):\n            # A figure as a dict, can be;\n            # - a plotly figure as a dict (after calling `fig.to_dict()`)\n            # - a pickled (plotly-resampler) figure (after loading a pickled figure)\n            f.layout = figure.get(\"layout\")\n            f._grid_str = figure.get(\"_grid_str\")\n            f._grid_ref = figure.get(\"_grid_ref\")\n            f.add_traces(figure.get(\"data\"))\n            # `pr_props` is not None when loading a pickled plotly-resampler figure\n            f._pr_props = figure.get(\"pr_props\")\n            # `f._pr_props`` is an attribute to store properties of a plotly-resampler\n            # figure. This attribute is only used to pass information to the super()\n            # constructor. Once the super constructor is called, the attribute is\n            # removed.\n\n            # f.add_frames(figure.get(\"frames\")) TODO\n        elif isinstance(figure, (dict, list)):\n            # A single trace dict or a list of traces\n            f.add_traces(figure)\n\n        super().__init__(\n            f,\n            convert_existing_traces,\n            default_n_shown_samples,\n            default_downsampler,\n            default_gap_handler,\n            resampled_trace_prefix_suffix,\n            show_mean_aggregation_size,\n            convert_traces_kwargs,\n            verbose,\n        )\n\n        if isinstance(figure, AbstractFigureAggregator):\n            # Copy the `_hf_data` if the previous figure was an AbstractFigureAggregator\n            # And adjust the default max_n_samples and\n            self._hf_data.update(\n                self._copy_hf_data(figure._hf_data, adjust_default_values=True)\n            )\n\n            # Note: This hack ensures that the this figure object initially uses\n            # data of the whole view. More concretely; we create a dict\n            # serialization figure and adjust the hf-traces to the whole view\n            # with the check-update method (by passing no range / filter args)\n            with self.batch_update():\n                graph_dict: dict = self._get_current_graph()\n                update_indices = self._check_update_figure_dict(graph_dict)\n                for idx in update_indices:\n                    self.data[idx].update(graph_dict[\"data\"][idx])\n\n        self._prev_layout = None  # Contains the previous xaxis layout configuration\n\n        # used for logging purposes to save a history of layout changes\n        self._relayout_hist = []\n\n        # Assign the the update-methods to the corresponding classes\n        showspike_keys = [f\"{xaxis}.showspikes\" for xaxis in self._xaxis_list]\n        self.layout.on_change(self._update_spike_ranges, *showspike_keys)\n\n        x_relayout_keys = [f\"{xaxis}.range\" for xaxis in self._xaxis_list]\n        self.layout.on_change(self._update_x_ranges, *x_relayout_keys)\n\n    def _update_x_ranges(self, layout, *x_ranges, force_update: bool = False):\n        \"\"\"Update the the go.Figure data based on changed x-ranges.\n\n        Parameters\n        ----------\n        layout : go.Layout\n            The figure's (i.e, self) layout object. Remark that this is a reference,\n            so if we change self.layout (same object reference), this object will\n            change.\n        *x_ranges: iterable\n            A iterable list of current x-ranges, where each x-range is a tuple of two\n            items, indicating the current/new (if changed) left-right x-range,\n            respectively.\n        fore_update: bool\n            Whether an update of all traces will be forced, by default False.\n        \"\"\"\n        relayout_dict = {}  # variable in which we aim to reconstruct the relayout\n        # serialize the layout in a new dict object\n        layout = {\n            xaxis_str: layout[xaxis_str].to_plotly_json()\n            for xaxis_str in self._xaxis_list\n        }\n        if self._prev_layout is None:\n            self._prev_layout = layout\n\n        for xaxis_str, x_range in zip(self._xaxis_list, x_ranges):\n            # We also check whether \"range\" is within the xaxis its layout otherwise\n            # It is most-likely an autorange check\n            if (\n                \"range\" in layout[xaxis_str]\n                and self._prev_layout[xaxis_str].get(\"range\", []) != x_range\n                or (force_update and x_range is not None)\n            ):\n                # a change took place -&gt; add to the relayout dict\n                relayout_dict[f\"{xaxis_str}.range[0]\"] = x_range[0]\n                relayout_dict[f\"{xaxis_str}.range[1]\"] = x_range[1]\n\n                # An update will take place for that trace\n                # -&gt; save current xaxis range to _prev_layout\n                self._prev_layout[xaxis_str][\"range\"] = x_range\n\n        if relayout_dict:  # when not empty\n            # Construct the update data\n            update_data = self._construct_update_data(relayout_dict)\n\n            if self._is_no_update(update_data):\n                # Return when no data update\n                return\n\n            if self._print_verbose:\n                self._relayout_hist.append(dict(zip(self._xaxis_list, x_ranges)))\n                self._relayout_hist.append(layout)\n                self._relayout_hist.append([\"xaxis-range-update\", len(update_data) - 1])\n                self._relayout_hist.append(\"-\" * 30)\n\n            with self.batch_update():\n                # First update the layout (first item of update_data)\n                self.layout.update(self._parse_relayout(update_data[0]))\n\n                for xaxis_str in self._xaxis_list:\n                    if \"showspikes\" in layout[xaxis_str]:\n                        self.layout[xaxis_str].pop(\"showspikes\")\n\n                # Then update the data\n                for updated_trace in update_data[1:]:\n                    trace_idx = updated_trace.pop(\"index\")\n                    self.data[trace_idx].update(updated_trace)\n\n    def _update_spike_ranges(self, layout, *showspikes, force_update=False):\n        \"\"\"Update the go.Figure based on the changed spike-ranges.\n\n        Parameters\n        ----------\n        layout : go.Layout\n            The figure's (i.e, self) layout object. Remark that this is a reference,\n            so if we change self.layout (same object reference), this object will\n            change.\n        *showspikes: iterable\n            A iterable where each item is a bool, indicating  whether showspikes is set\n            to true/false for the corresponding xaxis in ``self._xaxis_list``.\n        force_update: bool\n            Bool indicating whether the range updates need to take place. This is\n            especially useful when you have recently updated the figure its data (with\n            the hf_data property) and want to perform an autoscale, independent from\n            the current figure-layout.\n        \"\"\"\n        relayout_dict = {}  # variable in which we aim to reconstruct the relayout\n        # serialize the layout in a new dict object\n        layout = {\n            xaxis_str: layout[xaxis_str].to_plotly_json()\n            for xaxis_str in self._xaxis_list\n        }\n\n        if self._prev_layout is None:\n            self._prev_layout = layout\n\n        for xaxis_str, showspike in zip(self._xaxis_list, showspikes):\n            if (\n                force_update\n                or\n                # autorange key must be set to True\n                (\n                    layout[xaxis_str].get(\"autorange\", False)\n                    # we only perform updates for traces which have 'range' property,\n                    # as we do need to reconstruct the update-data for these traces\n                    and self._prev_layout[xaxis_str].get(\"range\", None) is not None\n                )\n            ):\n                relayout_dict[f\"{xaxis_str}.autorange\"] = True\n                relayout_dict[f\"{xaxis_str}.showspikes\"] = showspike\n                # autorange -&gt; we pop the xaxis range\n                if \"range\" in layout[xaxis_str]:\n                    del layout[xaxis_str][\"range\"]\n\n        if len(relayout_dict):\n            # An update will take place, save current layout to _prev_layout\n            self._prev_layout = layout\n\n            # Construct the update data\n            update_data = self._construct_update_data(relayout_dict)\n            if not self._is_no_update(update_data):\n                if self._print_verbose:\n                    self._relayout_hist.append(layout)\n                    self._relayout_hist.append(\n                        [\"showspikes-update\", len(update_data) - 1]\n                    )\n                    self._relayout_hist.append(\"-\" * 30)\n\n                with self.batch_update():\n                    # First update the layout (first item of update_data)\n                    if not force_update:\n                        self.layout.update(self._parse_relayout(update_data[0]))\n\n                    # Also: Remove the showspikes from the layout, otherwise the autorange\n                    # will not work as intended (it will not be triggered again)\n                    # Note: this removal causes a second trigger of this method\n                    # which will go in the \"else\" part below.\n                    for xaxis_str in self._xaxis_list:\n                        self.layout[xaxis_str].pop(\"showspikes\")\n\n                    # Then, update the data\n                    for updated_trace in update_data[1:]:\n                        trace_idx = updated_trace.pop(\"index\")\n                        self.data[trace_idx].update(updated_trace)\n        elif self._print_verbose:\n            self._relayout_hist.append([\"showspikes\", \"initial call or showspikes\"])\n            self._relayout_hist.append(\"-\" * 40)\n\n    def reset_axes(self):\n        \"\"\"Reset the axes of the FigureWidgetResampler.\n\n        This is useful when adjusting the `hf_data` properties of the\n        ``FigureWidgetResampler``.\n        \"\"\"\n        self._update_spike_ranges(\n            self.layout, *[False] * len(self._xaxis_list), force_update=True\n        )\n        # Reset the layout\n        self.update_layout(\n            {\n                axis: {\"autorange\": None, \"range\": None}\n                for axis in self._xaxis_list + self._yaxis_list\n            }\n        )\n\n    def reload_data(self):\n        \"\"\"Reload all the data of FigureWidgetResampler for the current range-view.\n\n        This is useful when adjusting the `hf_data` properties of the\n        ``FigureWidgetResampler``.\n        \"\"\"\n        if all(\n            self.layout[xaxis].autorange\n            or (\n                self.layout[xaxis].autorange is None\n                and self.layout[xaxis].range is None\n            )\n            for xaxis in self._xaxis_list\n        ):\n            self._update_spike_ranges(\n                self.layout, *[False] * len(self._xaxis_list), force_update=True\n            )\n        else:\n            # Resample the data for the current range-view\n            self._update_x_ranges(\n                self.layout,\n                # Pass the current view to trigger a resample operation\n                *[self.layout[xaxis_str][\"range\"] for xaxis_str in self._xaxis_list],\n                force_update=True,\n            )\n            # TODO: when we know which traces have changed we can use\n            # a new -&gt; `update_xaxis_str` argument.\n\n    def __reduce__(self):\n        # Needed for pickling\n        # Specifically set the class name, as the metaclass is not easily picklable\n        return FigureWidgetResampler, *list(super().__reduce__())[1:]\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureWidgetResampler.reload_data","title":"<code>reload_data()</code>","text":"<p>Reload all the data of FigureWidgetResampler for the current range-view.</p> <p>This is useful when adjusting the <code>hf_data</code> properties of the <code>FigureWidgetResampler</code>.</p> Source code in <code>plotly_resampler/figure_resampler/figurewidget_resampler.py</code> <pre><code>def reload_data(self):\n    \"\"\"Reload all the data of FigureWidgetResampler for the current range-view.\n\n    This is useful when adjusting the `hf_data` properties of the\n    ``FigureWidgetResampler``.\n    \"\"\"\n    if all(\n        self.layout[xaxis].autorange\n        or (\n            self.layout[xaxis].autorange is None\n            and self.layout[xaxis].range is None\n        )\n        for xaxis in self._xaxis_list\n    ):\n        self._update_spike_ranges(\n            self.layout, *[False] * len(self._xaxis_list), force_update=True\n        )\n    else:\n        # Resample the data for the current range-view\n        self._update_x_ranges(\n            self.layout,\n            # Pass the current view to trigger a resample operation\n            *[self.layout[xaxis_str][\"range\"] for xaxis_str in self._xaxis_list],\n            force_update=True,\n        )\n</code></pre>"},{"location":"api/figure_resampler/#figure_resampler.FigureWidgetResampler.reset_axes","title":"<code>reset_axes()</code>","text":"<p>Reset the axes of the FigureWidgetResampler.</p> <p>This is useful when adjusting the <code>hf_data</code> properties of the <code>FigureWidgetResampler</code>.</p> Source code in <code>plotly_resampler/figure_resampler/figurewidget_resampler.py</code> <pre><code>def reset_axes(self):\n    \"\"\"Reset the axes of the FigureWidgetResampler.\n\n    This is useful when adjusting the `hf_data` properties of the\n    ``FigureWidgetResampler``.\n    \"\"\"\n    self._update_spike_ranges(\n        self.layout, *[False] * len(self._xaxis_list), force_update=True\n    )\n    # Reset the layout\n    self.update_layout(\n        {\n            axis: {\"autorange\": None, \"range\": None}\n            for axis in self._xaxis_list + self._yaxis_list\n        }\n    )\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler/","title":"figure_resampler","text":"<p><code>FigureResampler</code> wrapper around the plotly <code>go.Figure</code> class.</p> <p>Creates a web-application and uses <code>dash</code> callbacks to enable dynamic resampling.</p>"},{"location":"api/figure_resampler/figure_resampler/#figure_resampler.figure_resampler.FigureResampler","title":"<code>FigureResampler</code>","text":"<p>               Bases: <code>AbstractFigureAggregator</code>, <code>Figure</code></p> <p>Data aggregation functionality for <code>go.Figures</code>.</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>class FigureResampler(AbstractFigureAggregator, go.Figure):\n    \"\"\"Data aggregation functionality for ``go.Figures``.\"\"\"\n\n    def __init__(\n        self,\n        figure: BaseFigure | dict = None,\n        convert_existing_traces: bool = True,\n        default_n_shown_samples: int = 1000,\n        default_downsampler: AbstractAggregator = MinMaxLTTB(),\n        default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n        resampled_trace_prefix_suffix: Tuple[str, str] = (\n            '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n            \"\",\n        ),\n        show_mean_aggregation_size: bool = True,\n        convert_traces_kwargs: dict | None = None,\n        create_overview: bool = False,\n        overview_row_idxs: list = None,\n        overview_kwargs: dict = {},\n        verbose: bool = False,\n        show_dash_kwargs: dict | None = None,\n    ):\n        \"\"\"Initialize a dynamic aggregation data mirror using a dash web app.\n\n        Parameters\n        ----------\n        figure: BaseFigure\n            The figure that will be decorated. Can be either an empty figure\n            (e.g., ``go.Figure()``, ``make_subplots()``, ``go.FigureWidget``) or an\n            existing figure.\n        convert_existing_traces: bool\n            A bool indicating whether the high-frequency traces of the passed ``figure``\n            should be resampled, by default True. Hence, when set to False, the\n            high-frequency traces of the passed ``figure`` will not be resampled.\n        default_n_shown_samples: int, optional\n            The default number of samples that will be shown for each trace,\n            by default 1000.\\n\n            !!! note\n                - This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n                - If a trace withholds fewer datapoints than this parameter,\n                  the data will *not* be aggregated.\n        default_downsampler: AbstractAggregator, optional\n            An instance which implements the AbstractAggregator interface and\n            will be used as default downsampler, by default ``MinMaxLTTB`` with\n            ``MinMaxLTTB`` is a heuristic to the LTTB algorithm that uses pre-selection\n            of min-max values (default 4 per bin) to speed up LTTB (as now only 4 values\n            per bin are considered by LTTB). This min-max ratio of 4 can be changed by\n            initializing ``MinMaxLTTB`` with a different value for the ``minmax_ratio``\n            parameter. \\n\n            !!! note\n                This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n        default_gap_handler: AbstractGapHandler, optional\n            An instance which implements the AbstractGapHandler interface and\n            will be used as default gap handler, by default ``MedDiffGapHandler``.\n            ``MedDiffGapHandler`` will determine gaps by first calculating the median\n            aggregated x difference and then thresholding the aggregated x delta on a\n            multiple of this median difference.  \\n\n            !!! note\n                This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n        resampled_trace_prefix_suffix: str, optional\n            A tuple which contains the ``prefix`` and ``suffix``, respectively, which\n            will be added to the trace its legend-name when a resampled version of the\n            trace is shown. By default a bold, orange ``[R]`` is shown as prefix\n            (no suffix is shown).\n        show_mean_aggregation_size: bool, optional\n            Whether the mean aggregation bin size will be added as a suffix to the trace\n            its legend-name, by default True.\n        convert_traces_kwargs: dict, optional\n            A dict of kwargs that will be passed to the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method and\n            will be used to convert the existing traces. \\n\n            !!! note\n                This argument is only used when the passed ``figure`` contains data and\n                ``convert_existing_traces`` is set to True.\n        create_overview: bool, optional\n            Whether an overview will be added to the figure (also known as rangeslider),\n            by default False. An overview is a bidirectionally linked figure that is\n            placed below the FigureResampler figure and shows a coarse version on which\n            the current view of the FigureResampler figure is highlighted. The overview\n            can be used to quickly navigate through the data by dragging the selection\n            box.\n            !!! note\n                - In the case of subplots, the overview will be created for each subplot\n                  column. Only a single subplot row can be captured in the overview,\n                  this is by default the first row. If you want to customize this\n                  behavior, you can use the `overview_row_idxs` argument.\n                - This functionality is not yet extensively validated. Please report any\n                  issues you encounter on GitHub.\n        overview_row_idxs: list, optional\n            A list of integers corresponding to the row indices (START AT 0) of the\n            subplots columns that should be linked with the column its corresponding\n            overview. By default None, which will result in the first row being utilized\n            for each column.\n        overview_kwargs: dict, optional\n            A dict of kwargs that will be passed to the `update_layout` method of the\n            overview figure, by default {}, which will result in utilizing the\n            [`default`][_DEFAULT_OVERVIEW_LAYOUT_KWARGS] overview layout kwargs.\n        verbose: bool, optional\n            Whether some verbose messages will be printed or not, by default False.\n        show_dash_kwargs: dict, optional\n            A dict that will be used as default kwargs for the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method.\n            !!! note\n                The passed kwargs to the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method will take precedence over these defaults.\n\n        \"\"\"\n        # Parse the figure input before calling `super`\n        if is_figure(figure) and not is_fr(figure):\n            # A go.Figure\n            # =&gt; base case: the figure does not need to be adjusted\n            f = figure\n        else:\n            # Create a new figure object and make sure that the trace uid will not get\n            # adjusted when they are added.\n            f = self._get_figure_class(go.Figure)()\n            f._data_validator.set_uid = False\n\n            if isinstance(figure, BaseFigure):\n                # A base figure object, can be;\n                # - a go.FigureWidget\n                # - a plotly-resampler figure: subclass of AbstractFigureAggregator\n                # =&gt; we first copy the layout, grid_str and grid ref\n                f.layout = figure.layout\n                f._grid_str = figure._grid_str\n                f._grid_ref = figure._grid_ref\n                f.add_traces(figure.data)\n            elif isinstance(figure, dict) and (\n                \"data\" in figure or \"layout\" in figure  # or \"frames\" in figure  # TODO\n            ):\n                # A figure as a dict, can be;\n                # - a plotly figure as a dict (after calling `fig.to_dict()`)\n                # - a pickled (plotly-resampler) figure (after loading a pickled figure)\n                # =&gt; we first copy the layout, grid_str and grid ref\n                f.layout = figure.get(\"layout\")\n                f._grid_str = figure.get(\"_grid_str\")\n                f._grid_ref = figure.get(\"_grid_ref\")\n                f.add_traces(figure.get(\"data\"))\n                # `pr_props` is not None when loading a pickled plotly-resampler figure\n                f._pr_props = figure.get(\"pr_props\")\n                # `f._pr_props`` is an attribute to store properties of a\n                # plotly-resampler figure. This attribute is only used to pass\n                # information to the super() constructor. Once the super constructor is\n                # called, the attribute is removed.\n\n                # f.add_frames(figure.get(\"frames\")) TODO\n            elif isinstance(figure, (dict, list)):\n                # A single trace dict or a list of traces\n                f.add_traces(figure)\n\n        self._show_dash_kwargs = (\n            show_dash_kwargs if show_dash_kwargs is not None else {}\n        )\n\n        super().__init__(\n            f,\n            convert_existing_traces,\n            default_n_shown_samples,\n            default_downsampler,\n            default_gap_handler,\n            resampled_trace_prefix_suffix,\n            show_mean_aggregation_size,\n            convert_traces_kwargs,\n            verbose,\n        )\n\n        if isinstance(figure, AbstractFigureAggregator):\n            # Copy the `_hf_data` if the previous figure was an AbstractFigureAggregator\n            # and adjust the default `max_n_samples` and `downsampler`\n            self._hf_data.update(\n                self._copy_hf_data(figure._hf_data, adjust_default_values=True)\n            )\n\n            # Note: This hack ensures that the this figure object initially uses\n            # data of the whole view. More concretely; we create a dict\n            # serialization figure and adjust the hf-traces to the whole view\n            # with the check-update method (by passing no range / filter args)\n            with self.batch_update():\n                graph_dict: dict = self._get_current_graph()\n                update_indices = self._check_update_figure_dict(graph_dict)\n                for idx in update_indices:\n                    self.data[idx].update(graph_dict[\"data\"][idx])\n\n        self._create_overview = create_overview\n        # update the overview layout\n        overview_layout_kwargs = _DEFAULT_OVERVIEW_LAYOUT_KWARGS.copy()\n        overview_layout_kwargs.update(overview_kwargs)\n        self._overview_layout_kwargs = overview_layout_kwargs\n\n        # array representing the row indices per column (START AT 0) of the subplot\n        # that should be linked with the columns corresponding overview.\n        # By default, the first row (i.e. index 0) will be utilized for each column\n        self._overview_row_idxs = self._parse_subplot_row_indices(overview_row_idxs)\n\n        # The FigureResampler needs a dash app\n        self._app: dash.Dash | None = None\n        self._port: int | None = None\n        self._host: str | None = None\n        # Certain functions will be different when using persistent inline\n        # (namely `show_dash` and `stop_callback`)\n\n    def _get_subplot_rows_and_cols_from_grid(self) -&gt; Tuple[int, int]:\n        \"\"\"Get the number of rows and columns of the figure's grid.\n\n        Returns\n        -------\n        Tuple[int, int]\n            The number of rows and columns of the figure's grid, respectively.\n        \"\"\"\n        if self._grid_ref is None:  # case: go.Figure (no subplots)\n            return (1, 1)\n        # TODO: not 100% sure whether this is correct\n        return (len(self._grid_ref), len(self._grid_ref[0]))\n\n    def _parse_subplot_row_indices(self, row_indices: list = None) -&gt; List[int]:\n        \"\"\"Verify whether the passed row indices are valid.\n\n        Parameters\n        ----------\n        row_indices: list, optional\n            A list of integers representing the row indices for which the overview\n            should be created. The length of the list should be equal to the number of\n            columns of the figure. Each element of the list should be smaller than the\n            number of rows of the figure (thus note that the row indices start at 0). By\n            default None, which will result in the first row being utilized for each\n            column.\n            !!! note\n                When you do not want to use an overview of a certain column (because\n                a certain subplot spans more than 1 column), you can specify this by\n                setting that respecive row_index value to `None`.\n\n                For instance, the sbuplot on row 2, col 1 spans two coloms. So when you\n                intend to utilize that subplot within the overview, you want to specify\n                the row_indices as: `[1, None, ...]`\n\n        Returns\n        -------\n        List[int]\n            A list of integers representing the row indices per subplot column.\n\n        \"\"\"\n        n_rows, n_cols = self._get_subplot_rows_and_cols_from_grid()\n\n        # By default, the first row is utilized to set the row indices\n        if row_indices is None:\n            return [0] * n_cols\n\n        # perform some checks on the row indices\n        assert isinstance(row_indices, list), \"row indices must be a list\"\n        assert (\n            len(row_indices) == n_cols\n        ), \"the number of row indices must be equal to the number of columns\"\n        assert all(\n            [(li is None) or (0 &lt;= li &lt; n_rows) for li in row_indices]\n        ), \"row indices must be smaller than the number of rows\"\n\n        return row_indices\n\n    # determines which subplot data to take from main and put into coarse\n    def _remove_other_axes_for_coarse(self) -&gt; go.Figure:\n        # base case: no rows and cols to filter\n        if self._grid_ref is None:  # case: go.Figure (no subplots)\n            return self\n\n        # Create the grid specification for the overview figure (in `reduced_grid_ref`)\n        # The trace_list and the 2 axis lists are 1D arrays holding track of the traces\n        # and axes to track.\n        reduced_grid_ref = [[]]\n\n        # Store the xaxis keys (e.g., x2) of the traces to keep\n        trace_list = []\n        # Store the xaxis and yaxis layout keys of the traces to keep (e.g., xaxis2)\n        layout_xaxis_list, layout_yaxis_list = [], []\n        for col_idx, row_idx in enumerate(self._overview_row_idxs):\n            if row_idx is None:  # skip None value\n                continue\n\n            overview_grid_ref = self._grid_ref[row_idx][col_idx]\n            reduced_grid_ref[0].append(overview_grid_ref)  # [0] bc 1 row in overview\n            for subplot in overview_grid_ref:\n                trace_list.append(subplot.trace_kwargs[\"xaxis\"])\n\n                # store the layout keys so that we can retain the exact layout\n                xaxis_key, yaxis_key = subplot.layout_keys\n                layout_yaxis_list.append(yaxis_key)\n                layout_xaxis_list.append(xaxis_key)\n        # print(\"layout_list\", l_xaxis_list, l_yaxis_list)\n        # print(\"trace_list\", trace_list)\n\n        fig_dict = self._get_current_graph()  # a copy of the current graph\n\n        # copy the data from the relevant overview subplots\n        reduced_fig_dict = {\n            \"data\": [],\n            \"layout\": {\"template\": fig_dict[\"layout\"][\"template\"]},\n        }\n        # NOTE: we enumerate over the data of the full figure so that we can utilize the\n        # trace index to mimic the colorway.\n        for i, trace in enumerate(fig_dict[\"data\"]):\n            # NOTE: the interplay between line_color and marker_color seems to work in\n            # this implementation - a more thorough investigation might be needed\n            if trace.get(\"xaxis\", \"x\") in trace_list:\n                if \"line\" not in trace:\n                    trace[\"line\"] = {}\n                # Ensure that the same color is utilized\n                trace[\"line\"][\"color\"] = (\n                    self._layout_obj.template.layout.colorway[i]\n                    if self.data[i].line.color is None\n                    else self.data[i].line.color\n                )\n                # add the trace to the reduced figure\n                reduced_fig_dict[\"data\"].append(trace)\n\n        # Add the relevant layout keys to the reduced figure\n        for k, v in fig_dict[\"layout\"].items():\n            if k in layout_xaxis_list:\n                reduced_fig_dict[\"layout\"][k] = v\n            elif k in layout_yaxis_list:\n                v = v.copy()\n                # set the domain to [0, 1] to ensure that the overview figure has the\n                # global y-axis range\n                v.update({\"domain\": [0, 1]})\n                reduced_fig_dict[\"layout\"][k] = v\n\n        # Create a figure object using the reduced figure dict\n        reduced_fig = go.Figure(layout=reduced_fig_dict[\"layout\"])\n        reduced_fig._grid_ref = reduced_grid_ref\n        # Ensure that the trace uid is not adjusted, this must be set prior to adding\n        # the trace data. Otherwise, data aggregation will not work.\n        reduced_fig._data_validator.set_uid = False\n        reduced_fig.add_traces(reduced_fig_dict[\"data\"])\n        return reduced_fig\n\n    def _create_overview_figure(self) -&gt; go.Figure:\n        # create a new coarse fig\n        reduced_fig = self._remove_other_axes_for_coarse()\n\n        # Resample the coarse figure using 3x the default aggregation size to ensure\n        # that it contains sufficient details\n        coarse_fig_hf = FigureResampler(\n            reduced_fig,\n            default_n_shown_samples=3 * self._global_n_shown_samples,\n        )\n\n        # NOTE: this way we can alter props without altering the original hf data\n        # NOTE: this also copies the default aggregation functionality to the coarse figure\n        coarse_fig_hf._hf_data = {uid: trc.copy() for uid, trc in self._hf_data.items()}\n        for trace in coarse_fig_hf.hf_data:\n            trace[\"max_n_samples\"] *= 3\n\n        coarse_fig_dict = coarse_fig_hf._get_current_graph()\n        # add the 3x max_n_samples coarse figure data to the coarse_fig_dict\n        coarse_fig_hf._check_update_figure_dict(coarse_fig_dict)\n        del coarse_fig_hf\n\n        coarse_fig = go.Figure(layout=coarse_fig_dict[\"layout\"])\n        coarse_fig._grid_ref = reduced_fig._grid_ref\n        coarse_fig._data_validator.set_uid = False\n        coarse_fig.add_traces(coarse_fig_dict[\"data\"])\n        # remove any update menus for the coarse figure\n        coarse_fig.layout.pop(\"updatemenus\", None)\n        # remove the `rangeselector` options for all 'axis' keys in the layout of the\n        # coarse figure\n        for k, v in coarse_fig.layout._props.items():\n            if \"axis\" in k:\n                v.pop(\"rangeselector\", None)\n\n        # height of the overview scales with the height of the dynamic view\n        coarse_fig.update_layout(\n            **self._overview_layout_kwargs,\n            hovermode=False,\n            clickmode=\"event+select\",\n            dragmode=\"select\",\n        )\n        # Hide the grid\n        hide_kwrgs = dict(\n            showgrid=False,\n            showticklabels=False,\n            zeroline=False,\n            title_text=None,\n            mirror=True,\n            ticks=\"\",\n            showline=False,\n            linecolor=\"black\",\n        )\n        coarse_fig.update_yaxes(**hide_kwrgs)\n        coarse_fig.update_xaxes(**hide_kwrgs)\n\n        vrect_props = dict(\n            **dict(line_width=0, x0=0, x1=1),\n            **dict(fillcolor=\"lightblue\", opacity=0.25, layer=\"above\"),\n        )\n\n        if self._grid_ref is None:  # case: go.Figure (no subplots)\n            # set the fixed range to True\n            coarse_fig[\"layout\"][\"xaxis\"][\"fixedrange\"] = True\n            coarse_fig[\"layout\"][\"yaxis\"][\"fixedrange\"] = True\n\n            # add a shading to the overview\n            coarse_fig.add_vrect(xref=\"x domain\", **vrect_props)\n            return coarse_fig\n\n        col_idx_overview = 0\n        for col_idx, row_idx in enumerate(self._overview_row_idxs):\n            if row_idx is None:  # skip the None value\n                continue\n\n            # we will only use the first grid-ref (as we will otherwise have multiple\n            # overlapping selection boxes)\n            for subplot in self._grid_ref[row_idx][col_idx][:1]:\n                xaxis_key, yaxis_key = subplot.layout_keys\n\n                # set the fixed range to True\n                coarse_fig[\"layout\"][xaxis_key][\"fixedrange\"] = True\n                coarse_fig[\"layout\"][yaxis_key][\"fixedrange\"] = True\n\n                # add a shading to the overview\n                coarse_fig.add_vrect(\n                    col=col_idx_overview + 1,\n                    xref=f\"{subplot.trace_kwargs['xaxis']} domain\",\n                    **vrect_props,\n                )\n\n            col_idx_overview += 1  # only increase the index when not None\n\n        return coarse_fig\n\n    def show_dash(\n        self,\n        mode=None,\n        config: dict | None = None,\n        init_dash_kwargs: dict | None = None,\n        graph_properties: dict | None = None,\n        **kwargs,\n    ):\n        \"\"\"Registers the `update_graph` callback &amp; show the figure in a dash app.\n\n        Parameters\n        ----------\n        mode: str, optional\n            Display mode. One of:\\n\n              * ``\"external\"``: The URL of the app will be displayed in the notebook\n                output cell. Clicking this URL will open the app in the default\n                web browser.\n              * ``\"inline\"``: The app will be displayed inline in the notebook output\n                cell in an iframe.\n              * ``\"inline_persistent\"``: The app will be displayed inline in the\n                notebook output cell in an iframe, if the app is not reachable a static\n                image of the figure is shown. Hence this is a persistent version of the\n                ``\"inline\"`` mode, allowing users to see a static figure in other\n                environments, browsers, etc.\n\n                !!! note\n\n                    This mode requires the ``kaleido`` and ``flask_cors`` package.\n                    Install them : ``pip install plotly_resampler[inline_persistent]``\n                    or ``pip install kaleido flask_cors``.\n\n              * ``\"jupyterlab\"``: The app will be displayed in a dedicated tab in the\n                JupyterLab interface. Requires JupyterLab and the ``jupyterlab-dash``\n                extension.\n            By default None, which will result in the same behavior as ``\"external\"``.\n        config: dict, optional\n            The configuration options for displaying this figure, by default None.\n            This ``config`` parameter is the same as the dict that you would pass as\n            ``config`` argument to the `show` method.\n            See more [https://plotly.com/python/configuration-options/](https://plotly.com/python/configuration-options/)\n        init_dash_kwargs: dict, optional\n            Keyword arguments for the Dash app constructor.\n            !!! note\n                This variable is of special interest when working in a jupyterhub +\n                kubernetes environment. In this case, user notebook servers are spawned\n                as separate pods and user access to those servers are proxied via\n                jupyterhub. Dash requires the `requests_pathname_prefix` to be set on\n                __init__ - which can be done via this `init_dash_kwargs` argument.\n                Note that you should also pass the `jupyter_server_url` to the\n                `show_dash` method.\n                More details: https://github.com/predict-idlab/plotly-resampler/issues/265\n        graph_properties: dict, optional\n            Dictionary of (keyword, value) for the properties that should be passed to\n            the dcc.Graph, by default None.\n            e.g.: `{\"style\": {\"width\": \"50%\"}}`\n            Note: \"config\" is not allowed as key in this dict, as there is a distinct\n            ``config`` parameter for this property in this method.\n            See more [https://dash.plotly.com/dash-core-components/graph](https://dash.plotly.com/dash-core-components/graph)\n        **kwargs: dict\n            kwargs for the ``app.run_server()`` method, e.g., port=8037.\n            !!! note\n                These kwargs take precedence over the ones that are passed to the\n                constructor via the ``show_dash_kwargs`` argument.\n\n        \"\"\"\n        available_modes = list(dash._jupyter.JupyterDisplayMode.__args__) + [\n            \"inline_persistent\"\n        ]\n        assert (\n            mode is None or mode in available_modes\n        ), f\"mode must be one of {available_modes}\"\n        graph_properties = {} if graph_properties is None else graph_properties\n        assert \"config\" not in graph_properties  # There is a param for config\n        if self[\"layout\"][\"autosize\"] is True and self[\"layout\"][\"height\"] is None:\n            graph_properties.setdefault(\"style\", {}).update({\"height\": \"100%\"})\n\n        # 0. Check if the traces need to be updated when there is a xrange set\n        # This will be the case when the users has set a xrange (via the `update_layout`\n        # or `update_xaxes` methods`)\n        relayout_dict = {}\n        for xaxis_str in self._xaxis_list:\n            x_range = self.layout[xaxis_str].range\n            if x_range:  # when not None\n                relayout_dict[f\"{xaxis_str}.range[0]\"] = x_range[0]\n                relayout_dict[f\"{xaxis_str}.range[1]\"] = x_range[1]\n        if relayout_dict:  # when not empty\n            update_data = self._construct_update_data(relayout_dict)\n\n            if not self._is_no_update(update_data):  # when there is an update\n                with self.batch_update():\n                    # First update the layout (first item of update_data)\n                    self.layout.update(self._parse_relayout(update_data[0]))\n\n                    # Then update the data\n                    for updated_trace in update_data[1:]:\n                        trace_idx = updated_trace.pop(\"index\")\n                        self.data[trace_idx].update(updated_trace)\n\n        # 1. Construct the Dash app layout\n        init_dash_kwargs = {} if init_dash_kwargs is None else init_dash_kwargs\n        if self._create_overview:\n            # fmt: off\n            # Add the assets folder to the init_dash_kwargs\n            init_dash_kwargs[\"assets_folder\"] = os.path.relpath(ASSETS_FOLDER, os.getcwd())\n            # Also include the lodash script, as the client-side callbacks uses this\n            init_dash_kwargs[\"external_scripts\"] = [\"https://cdn.jsdelivr.net/npm/lodash/lodash.min.js\" ]\n            # fmt: on\n\n        # fmt: off\n        div = dash.html.Div(\n            children=[\n                dash.dcc.Graph(\n                    id=\"resample-figure\", figure=self, config=config, **graph_properties\n                )\n            ],\n            style={\n                \"display\": \"flex\", \"flex-flow\": \"column\",\n                \"height\": \"95vh\", \"width\": \"100%\",\n            },\n        )\n        # fmt: on\n        if self._create_overview:\n            overview_config = config.copy() if config is not None else {}\n            overview_config[\"displayModeBar\"] = False\n            coarse_fig = self._create_overview_figure()\n            div.children += [\n                dash.dcc.Graph(\n                    id=\"overview-figure\",\n                    figure=coarse_fig,\n                    config=overview_config,\n                    **graph_properties,\n                ),\n            ]\n\n        # Create the app, populate the layout and register the resample callback\n        app = dash.Dash(\"local_app\", **init_dash_kwargs)\n        app.layout = div\n        self.register_update_graph_callback(\n            app,\n            \"resample-figure\",\n            \"overview-figure\" if self._create_overview else None,\n        )\n\n        # 2. Run the app\n        height_param = \"height\" if mode == \"inline_persistent\" else \"jupyter_height\"\n        if \"inline\" in mode and height_param not in kwargs:\n            # If app height is not specified -&gt; re-use figure height for inline dash app\n            #  Note: default layout height is 450 (whereas default app height is 650)\n            #  See: https://plotly.com/python/reference/layout/#layout-height\n            fig_height = self.layout.height if self.layout.height is not None else 450\n            kwargs[height_param] = fig_height + 18\n\n        # kwargs take precedence over the show_dash_kwargs\n        kwargs = {**self._show_dash_kwargs, **kwargs}\n\n        # Store the app information, so it can be killed\n        self._app = app\n        self._host = kwargs.get(\"host\", \"127.0.0.1\")\n        self._port = kwargs.get(\"port\", \"8050\")\n\n        # function signatures are slightly different for the (Jupyter)Dash and the\n        # JupyterDashInlinePersistent implementations\n        if mode == \"inline_persistent\":\n            from .jupyter_dash_persistent_inline_output import (\n                JupyterDashPersistentInlineOutput,\n            )\n\n            jpi = JupyterDashPersistentInlineOutput(self)\n            jpi.run_app(app=app, **kwargs)\n        else:\n            app.run(jupyter_mode=mode, **kwargs)\n\n    def stop_server(self, warn: bool = True):\n        \"\"\"Stop the running dash-app.\n\n        Parameters\n        ----------\n        warn: bool\n            Whether a warning message will be shown or  not, by default True.\n\n        !!! warning\n\n            This only works if the dash-app was started with [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash].\n        \"\"\"\n        if self._app is not None:\n            servers_dict = dash.jupyter_dash._servers\n            old_server = servers_dict.get((self._host, self._port))\n            if old_server:\n                old_server.shutdown()\n            del servers_dict[(self._host, self._port)]\n        elif warn:\n            warnings.warn(\n                \"Could not stop the server, either the \\n\"\n                + \"\\t- 'show-dash' method was not called, or \\n\"\n                + \"\\t- the dash-server wasn't started with 'show_dash'\"\n            )\n\n    def construct_update_data_patch(\n        self, relayout_data: dict\n    ) -&gt; Union[dash.Patch, dash.no_update]:\n        \"\"\"Construct the Patch of the to-be-updated front-end data, based on the layout\n        change.\n\n        Attention\n        ---------\n        This method is tightly coupled with Dash app callbacks. It takes the front-end\n        figure its ``relayoutData`` as input and returns the ``dash.Patch`` which needs\n        to be sent to the ``figure`` property for the corresponding ``dcc.Graph``.\n\n        Parameters\n        ----------\n        relayout_data: dict\n            A dict containing the ``relayoutData`` (i.e., the changed layout data) of\n            the corresponding front-end graph.\n\n        Returns\n        -------\n        dash.Patch:\n            The Patch object containing the figure updates which needs to be sent to\n            the front-end.\n\n        \"\"\"\n        update_data = self._construct_update_data(relayout_data)\n        if not isinstance(update_data, list) or len(update_data) &lt;= 1:\n            return dash.no_update\n\n        patched_figure = dash.Patch()  # create patch\n        for trace in update_data[1:]:  # skip first item as it contains the relayout\n            trace_index = trace.pop(\"index\")  # the index of the corresponding trace\n            # All the other items are the trace properties which needs to be updated\n            for k, v in trace.items():\n                # NOTE: we need to use the `patched_figure` as a dict, and not\n                # `patched_figure.data` as the latter will replace **all** the\n                # data for the corresponding trace, and we just want to update the\n                # specific trace its properties.\n                patched_figure[\"data\"][trace_index][k] = v\n        return patched_figure\n\n    def register_update_graph_callback(\n        self,\n        app: dash.Dash,\n        graph_id: str,\n        coarse_graph_id: Optional[str] = None,\n    ):\n        \"\"\"Register the [`construct_update_data_patch`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.construct_update_data_patch]\n        method as callback function to the passed dash-app.\n\n        Parameters\n        ----------\n        app: Union[dash.Dash, JupyterDash]\n            The app in which the callback will be registered.\n        graph_id:\n            The id of the ``dcc.Graph``-component which withholds the to-be resampled\n            Figure.\n        coarse_graph_id: str, optional\n            The id of the ``dcc.Graph``-component which withholds the coarse overview\n            Figure, by default None.\n\n        \"\"\"\n        # As we use the figure again as output, we need to set: allow_duplicate=True\n\n        if coarse_graph_id is not None:\n            # update pr graph range with overview selection\n            app.clientside_callback(\n                dash.ClientsideFunction(\n                    namespace=\"clientside\", function_name=\"coarse_to_main\"\n                ),\n                dash.Output(graph_id, \"id\", allow_duplicate=True),\n                dash.Input(coarse_graph_id, \"selectedData\"),\n                dash.State(graph_id, \"id\"),\n                dash.State(coarse_graph_id, \"id\"),\n                prevent_initial_call=True,\n            )\n\n            # update selectbox with clientside callback\n            app.clientside_callback(\n                dash.ClientsideFunction(\n                    namespace=\"clientside\", function_name=\"main_to_coarse\"\n                ),\n                dash.Output(coarse_graph_id, \"id\", allow_duplicate=True),\n                dash.Input(graph_id, \"relayoutData\"),\n                dash.State(coarse_graph_id, \"id\"),\n                dash.State(graph_id, \"id\"),\n                prevent_initial_call=True,\n            )\n\n        app.callback(\n            dash.Output(graph_id, \"figure\", allow_duplicate=True),\n            dash.Input(graph_id, \"relayoutData\"),\n            prevent_initial_call=True,\n        )(self.construct_update_data_patch)\n\n    def _get_pr_props_keys(self) -&gt; List[str]:\n        # Add the additional plotly-resampler properties of this class\n        return super()._get_pr_props_keys() + [\"_show_dash_kwargs\"]\n\n    def _ipython_display_(self):\n        # To display the figure inline as a dash app\n        self.show_dash(mode=\"inline\")\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler/#figure_resampler.figure_resampler.FigureResampler.__init__","title":"<code>__init__(figure=None, convert_existing_traces=True, default_n_shown_samples=1000, default_downsampler=MinMaxLTTB(), default_gap_handler=MedDiffGapHandler(), resampled_trace_prefix_suffix=('&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ', ''), show_mean_aggregation_size=True, convert_traces_kwargs=None, create_overview=False, overview_row_idxs=None, overview_kwargs={}, verbose=False, show_dash_kwargs=None)</code>","text":"<p>Initialize a dynamic aggregation data mirror using a dash web app.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>BaseFigure | dict</code> <p>The figure that will be decorated. Can be either an empty figure (e.g., <code>go.Figure()</code>, <code>make_subplots()</code>, <code>go.FigureWidget</code>) or an existing figure.</p> <code>None</code> <code>convert_existing_traces</code> <code>bool</code> <p>A bool indicating whether the high-frequency traces of the passed <code>figure</code> should be resampled, by default True. Hence, when set to False, the high-frequency traces of the passed <code>figure</code> will not be resampled.</p> <code>True</code> <code>default_n_shown_samples</code> <code>int</code> <p>The default number of samples that will be shown for each trace, by default 1000.</p> <p>Note</p> <ul> <li>This can be overridden within the <code>add_trace</code> method.</li> <li>If a trace withholds fewer datapoints than this parameter,   the data will not be aggregated.</li> </ul> <code>1000</code> <code>default_downsampler</code> <code>AbstractAggregator</code> <p>An instance which implements the AbstractAggregator interface and will be used as default downsampler, by default <code>MinMaxLTTB</code> with <code>MinMaxLTTB</code> is a heuristic to the LTTB algorithm that uses pre-selection of min-max values (default 4 per bin) to speed up LTTB (as now only 4 values per bin are considered by LTTB). This min-max ratio of 4 can be changed by initializing <code>MinMaxLTTB</code> with a different value for the <code>minmax_ratio</code> parameter. </p> <p>Note</p> <p>This can be overridden within the <code>add_trace</code> method.</p> <code>MinMaxLTTB()</code> <code>default_gap_handler</code> <code>AbstractGapHandler</code> <p>An instance which implements the AbstractGapHandler interface and will be used as default gap handler, by default <code>MedDiffGapHandler</code>. <code>MedDiffGapHandler</code> will determine gaps by first calculating the median aggregated x difference and then thresholding the aggregated x delta on a multiple of this median difference.  </p> <p>Note</p> <p>This can be overridden within the <code>add_trace</code> method.</p> <code>MedDiffGapHandler()</code> <code>resampled_trace_prefix_suffix</code> <code>Tuple[str, str]</code> <p>A tuple which contains the <code>prefix</code> and <code>suffix</code>, respectively, which will be added to the trace its legend-name when a resampled version of the trace is shown. By default a bold, orange <code>[R]</code> is shown as prefix (no suffix is shown).</p> <code>('&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ', '')</code> <code>show_mean_aggregation_size</code> <code>bool</code> <p>Whether the mean aggregation bin size will be added as a suffix to the trace its legend-name, by default True.</p> <code>True</code> <code>convert_traces_kwargs</code> <code>dict | None</code> <p>A dict of kwargs that will be passed to the <code>add_trace</code> method and will be used to convert the existing traces. </p> <p>Note</p> <p>This argument is only used when the passed <code>figure</code> contains data and <code>convert_existing_traces</code> is set to True.</p> <code>None</code> <code>create_overview</code> <code>bool</code> <p>Whether an overview will be added to the figure (also known as rangeslider), by default False. An overview is a bidirectionally linked figure that is placed below the FigureResampler figure and shows a coarse version on which the current view of the FigureResampler figure is highlighted. The overview can be used to quickly navigate through the data by dragging the selection box.</p> <p>Note</p> <ul> <li>In the case of subplots, the overview will be created for each subplot   column. Only a single subplot row can be captured in the overview,   this is by default the first row. If you want to customize this   behavior, you can use the <code>overview_row_idxs</code> argument.</li> <li>This functionality is not yet extensively validated. Please report any   issues you encounter on GitHub.</li> </ul> <code>False</code> <code>overview_row_idxs</code> <code>list</code> <p>A list of integers corresponding to the row indices (START AT 0) of the subplots columns that should be linked with the column its corresponding overview. By default None, which will result in the first row being utilized for each column.</p> <code>None</code> <code>overview_kwargs</code> <code>dict</code> <p>A dict of kwargs that will be passed to the <code>update_layout</code> method of the overview figure, by default {}, which will result in utilizing the <code>default</code> overview layout kwargs.</p> <code>{}</code> <code>verbose</code> <code>bool</code> <p>Whether some verbose messages will be printed or not, by default False.</p> <code>False</code> <code>show_dash_kwargs</code> <code>dict | None</code> <p>A dict that will be used as default kwargs for the <code>show_dash</code> method.</p> <p>Note</p> <p>The passed kwargs to the <code>show_dash</code> method will take precedence over these defaults.</p> <code>None</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def __init__(\n    self,\n    figure: BaseFigure | dict = None,\n    convert_existing_traces: bool = True,\n    default_n_shown_samples: int = 1000,\n    default_downsampler: AbstractAggregator = MinMaxLTTB(),\n    default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n    resampled_trace_prefix_suffix: Tuple[str, str] = (\n        '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n        \"\",\n    ),\n    show_mean_aggregation_size: bool = True,\n    convert_traces_kwargs: dict | None = None,\n    create_overview: bool = False,\n    overview_row_idxs: list = None,\n    overview_kwargs: dict = {},\n    verbose: bool = False,\n    show_dash_kwargs: dict | None = None,\n):\n    \"\"\"Initialize a dynamic aggregation data mirror using a dash web app.\n\n    Parameters\n    ----------\n    figure: BaseFigure\n        The figure that will be decorated. Can be either an empty figure\n        (e.g., ``go.Figure()``, ``make_subplots()``, ``go.FigureWidget``) or an\n        existing figure.\n    convert_existing_traces: bool\n        A bool indicating whether the high-frequency traces of the passed ``figure``\n        should be resampled, by default True. Hence, when set to False, the\n        high-frequency traces of the passed ``figure`` will not be resampled.\n    default_n_shown_samples: int, optional\n        The default number of samples that will be shown for each trace,\n        by default 1000.\\n\n        !!! note\n            - This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n            - If a trace withholds fewer datapoints than this parameter,\n              the data will *not* be aggregated.\n    default_downsampler: AbstractAggregator, optional\n        An instance which implements the AbstractAggregator interface and\n        will be used as default downsampler, by default ``MinMaxLTTB`` with\n        ``MinMaxLTTB`` is a heuristic to the LTTB algorithm that uses pre-selection\n        of min-max values (default 4 per bin) to speed up LTTB (as now only 4 values\n        per bin are considered by LTTB). This min-max ratio of 4 can be changed by\n        initializing ``MinMaxLTTB`` with a different value for the ``minmax_ratio``\n        parameter. \\n\n        !!! note\n            This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n    default_gap_handler: AbstractGapHandler, optional\n        An instance which implements the AbstractGapHandler interface and\n        will be used as default gap handler, by default ``MedDiffGapHandler``.\n        ``MedDiffGapHandler`` will determine gaps by first calculating the median\n        aggregated x difference and then thresholding the aggregated x delta on a\n        multiple of this median difference.  \\n\n        !!! note\n            This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n    resampled_trace_prefix_suffix: str, optional\n        A tuple which contains the ``prefix`` and ``suffix``, respectively, which\n        will be added to the trace its legend-name when a resampled version of the\n        trace is shown. By default a bold, orange ``[R]`` is shown as prefix\n        (no suffix is shown).\n    show_mean_aggregation_size: bool, optional\n        Whether the mean aggregation bin size will be added as a suffix to the trace\n        its legend-name, by default True.\n    convert_traces_kwargs: dict, optional\n        A dict of kwargs that will be passed to the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method and\n        will be used to convert the existing traces. \\n\n        !!! note\n            This argument is only used when the passed ``figure`` contains data and\n            ``convert_existing_traces`` is set to True.\n    create_overview: bool, optional\n        Whether an overview will be added to the figure (also known as rangeslider),\n        by default False. An overview is a bidirectionally linked figure that is\n        placed below the FigureResampler figure and shows a coarse version on which\n        the current view of the FigureResampler figure is highlighted. The overview\n        can be used to quickly navigate through the data by dragging the selection\n        box.\n        !!! note\n            - In the case of subplots, the overview will be created for each subplot\n              column. Only a single subplot row can be captured in the overview,\n              this is by default the first row. If you want to customize this\n              behavior, you can use the `overview_row_idxs` argument.\n            - This functionality is not yet extensively validated. Please report any\n              issues you encounter on GitHub.\n    overview_row_idxs: list, optional\n        A list of integers corresponding to the row indices (START AT 0) of the\n        subplots columns that should be linked with the column its corresponding\n        overview. By default None, which will result in the first row being utilized\n        for each column.\n    overview_kwargs: dict, optional\n        A dict of kwargs that will be passed to the `update_layout` method of the\n        overview figure, by default {}, which will result in utilizing the\n        [`default`][_DEFAULT_OVERVIEW_LAYOUT_KWARGS] overview layout kwargs.\n    verbose: bool, optional\n        Whether some verbose messages will be printed or not, by default False.\n    show_dash_kwargs: dict, optional\n        A dict that will be used as default kwargs for the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method.\n        !!! note\n            The passed kwargs to the [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash] method will take precedence over these defaults.\n\n    \"\"\"\n    # Parse the figure input before calling `super`\n    if is_figure(figure) and not is_fr(figure):\n        # A go.Figure\n        # =&gt; base case: the figure does not need to be adjusted\n        f = figure\n    else:\n        # Create a new figure object and make sure that the trace uid will not get\n        # adjusted when they are added.\n        f = self._get_figure_class(go.Figure)()\n        f._data_validator.set_uid = False\n\n        if isinstance(figure, BaseFigure):\n            # A base figure object, can be;\n            # - a go.FigureWidget\n            # - a plotly-resampler figure: subclass of AbstractFigureAggregator\n            # =&gt; we first copy the layout, grid_str and grid ref\n            f.layout = figure.layout\n            f._grid_str = figure._grid_str\n            f._grid_ref = figure._grid_ref\n            f.add_traces(figure.data)\n        elif isinstance(figure, dict) and (\n            \"data\" in figure or \"layout\" in figure  # or \"frames\" in figure  # TODO\n        ):\n            # A figure as a dict, can be;\n            # - a plotly figure as a dict (after calling `fig.to_dict()`)\n            # - a pickled (plotly-resampler) figure (after loading a pickled figure)\n            # =&gt; we first copy the layout, grid_str and grid ref\n            f.layout = figure.get(\"layout\")\n            f._grid_str = figure.get(\"_grid_str\")\n            f._grid_ref = figure.get(\"_grid_ref\")\n            f.add_traces(figure.get(\"data\"))\n            # `pr_props` is not None when loading a pickled plotly-resampler figure\n            f._pr_props = figure.get(\"pr_props\")\n            # `f._pr_props`` is an attribute to store properties of a\n            # plotly-resampler figure. This attribute is only used to pass\n            # information to the super() constructor. Once the super constructor is\n            # called, the attribute is removed.\n\n            # f.add_frames(figure.get(\"frames\")) TODO\n        elif isinstance(figure, (dict, list)):\n            # A single trace dict or a list of traces\n            f.add_traces(figure)\n\n    self._show_dash_kwargs = (\n        show_dash_kwargs if show_dash_kwargs is not None else {}\n    )\n\n    super().__init__(\n        f,\n        convert_existing_traces,\n        default_n_shown_samples,\n        default_downsampler,\n        default_gap_handler,\n        resampled_trace_prefix_suffix,\n        show_mean_aggregation_size,\n        convert_traces_kwargs,\n        verbose,\n    )\n\n    if isinstance(figure, AbstractFigureAggregator):\n        # Copy the `_hf_data` if the previous figure was an AbstractFigureAggregator\n        # and adjust the default `max_n_samples` and `downsampler`\n        self._hf_data.update(\n            self._copy_hf_data(figure._hf_data, adjust_default_values=True)\n        )\n\n        # Note: This hack ensures that the this figure object initially uses\n        # data of the whole view. More concretely; we create a dict\n        # serialization figure and adjust the hf-traces to the whole view\n        # with the check-update method (by passing no range / filter args)\n        with self.batch_update():\n            graph_dict: dict = self._get_current_graph()\n            update_indices = self._check_update_figure_dict(graph_dict)\n            for idx in update_indices:\n                self.data[idx].update(graph_dict[\"data\"][idx])\n\n    self._create_overview = create_overview\n    # update the overview layout\n    overview_layout_kwargs = _DEFAULT_OVERVIEW_LAYOUT_KWARGS.copy()\n    overview_layout_kwargs.update(overview_kwargs)\n    self._overview_layout_kwargs = overview_layout_kwargs\n\n    # array representing the row indices per column (START AT 0) of the subplot\n    # that should be linked with the columns corresponding overview.\n    # By default, the first row (i.e. index 0) will be utilized for each column\n    self._overview_row_idxs = self._parse_subplot_row_indices(overview_row_idxs)\n\n    # The FigureResampler needs a dash app\n    self._app: dash.Dash | None = None\n    self._port: int | None = None\n    self._host: str | None = None\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler/#figure_resampler.figure_resampler.FigureResampler.construct_update_data_patch","title":"<code>construct_update_data_patch(relayout_data)</code>","text":"<p>Construct the Patch of the to-be-updated front-end data, based on the layout change.</p> Attention <p>This method is tightly coupled with Dash app callbacks. It takes the front-end figure its <code>relayoutData</code> as input and returns the <code>dash.Patch</code> which needs to be sent to the <code>figure</code> property for the corresponding <code>dcc.Graph</code>.</p> <p>Parameters:</p> Name Type Description Default <code>relayout_data</code> <code>dict</code> <p>A dict containing the <code>relayoutData</code> (i.e., the changed layout data) of the corresponding front-end graph.</p> required <p>Returns:</p> Type Description <code>dash.Patch:</code> <p>The Patch object containing the figure updates which needs to be sent to the front-end.</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def construct_update_data_patch(\n    self, relayout_data: dict\n) -&gt; Union[dash.Patch, dash.no_update]:\n    \"\"\"Construct the Patch of the to-be-updated front-end data, based on the layout\n    change.\n\n    Attention\n    ---------\n    This method is tightly coupled with Dash app callbacks. It takes the front-end\n    figure its ``relayoutData`` as input and returns the ``dash.Patch`` which needs\n    to be sent to the ``figure`` property for the corresponding ``dcc.Graph``.\n\n    Parameters\n    ----------\n    relayout_data: dict\n        A dict containing the ``relayoutData`` (i.e., the changed layout data) of\n        the corresponding front-end graph.\n\n    Returns\n    -------\n    dash.Patch:\n        The Patch object containing the figure updates which needs to be sent to\n        the front-end.\n\n    \"\"\"\n    update_data = self._construct_update_data(relayout_data)\n    if not isinstance(update_data, list) or len(update_data) &lt;= 1:\n        return dash.no_update\n\n    patched_figure = dash.Patch()  # create patch\n    for trace in update_data[1:]:  # skip first item as it contains the relayout\n        trace_index = trace.pop(\"index\")  # the index of the corresponding trace\n        # All the other items are the trace properties which needs to be updated\n        for k, v in trace.items():\n            # NOTE: we need to use the `patched_figure` as a dict, and not\n            # `patched_figure.data` as the latter will replace **all** the\n            # data for the corresponding trace, and we just want to update the\n            # specific trace its properties.\n            patched_figure[\"data\"][trace_index][k] = v\n    return patched_figure\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler/#figure_resampler.figure_resampler.FigureResampler.register_update_graph_callback","title":"<code>register_update_graph_callback(app, graph_id, coarse_graph_id=None)</code>","text":"<p>Register the <code>construct_update_data_patch</code> method as callback function to the passed dash-app.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Dash</code> <p>The app in which the callback will be registered.</p> required <code>graph_id</code> <code>str</code> <p>The id of the <code>dcc.Graph</code>-component which withholds the to-be resampled Figure.</p> required <code>coarse_graph_id</code> <code>Optional[str]</code> <p>The id of the <code>dcc.Graph</code>-component which withholds the coarse overview Figure, by default None.</p> <code>None</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def register_update_graph_callback(\n    self,\n    app: dash.Dash,\n    graph_id: str,\n    coarse_graph_id: Optional[str] = None,\n):\n    \"\"\"Register the [`construct_update_data_patch`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.construct_update_data_patch]\n    method as callback function to the passed dash-app.\n\n    Parameters\n    ----------\n    app: Union[dash.Dash, JupyterDash]\n        The app in which the callback will be registered.\n    graph_id:\n        The id of the ``dcc.Graph``-component which withholds the to-be resampled\n        Figure.\n    coarse_graph_id: str, optional\n        The id of the ``dcc.Graph``-component which withholds the coarse overview\n        Figure, by default None.\n\n    \"\"\"\n    # As we use the figure again as output, we need to set: allow_duplicate=True\n\n    if coarse_graph_id is not None:\n        # update pr graph range with overview selection\n        app.clientside_callback(\n            dash.ClientsideFunction(\n                namespace=\"clientside\", function_name=\"coarse_to_main\"\n            ),\n            dash.Output(graph_id, \"id\", allow_duplicate=True),\n            dash.Input(coarse_graph_id, \"selectedData\"),\n            dash.State(graph_id, \"id\"),\n            dash.State(coarse_graph_id, \"id\"),\n            prevent_initial_call=True,\n        )\n\n        # update selectbox with clientside callback\n        app.clientside_callback(\n            dash.ClientsideFunction(\n                namespace=\"clientside\", function_name=\"main_to_coarse\"\n            ),\n            dash.Output(coarse_graph_id, \"id\", allow_duplicate=True),\n            dash.Input(graph_id, \"relayoutData\"),\n            dash.State(coarse_graph_id, \"id\"),\n            dash.State(graph_id, \"id\"),\n            prevent_initial_call=True,\n        )\n\n    app.callback(\n        dash.Output(graph_id, \"figure\", allow_duplicate=True),\n        dash.Input(graph_id, \"relayoutData\"),\n        prevent_initial_call=True,\n    )(self.construct_update_data_patch)\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler/#figure_resampler.figure_resampler.FigureResampler.show_dash","title":"<code>show_dash(mode=None, config=None, init_dash_kwargs=None, graph_properties=None, **kwargs)</code>","text":"<p>Registers the <code>update_graph</code> callback &amp; show the figure in a dash app.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <p>Display mode. One of:</p> <ul> <li><code>\"external\"</code>: The URL of the app will be displayed in the notebook     output cell. Clicking this URL will open the app in the default     web browser.</li> <li><code>\"inline\"</code>: The app will be displayed inline in the notebook output     cell in an iframe.</li> <li> <p><code>\"inline_persistent\"</code>: The app will be displayed inline in the     notebook output cell in an iframe, if the app is not reachable a static     image of the figure is shown. Hence this is a persistent version of the     <code>\"inline\"</code> mode, allowing users to see a static figure in other     environments, browsers, etc.</p> <p>Note</p> <p>This mode requires the <code>kaleido</code> and <code>flask_cors</code> package. Install them : <code>pip install plotly_resampler[inline_persistent]</code> or <code>pip install kaleido flask_cors</code>.</p> </li> <li> <p><code>\"jupyterlab\"</code>: The app will be displayed in a dedicated tab in the     JupyterLab interface. Requires JupyterLab and the <code>jupyterlab-dash</code>     extension. By default None, which will result in the same behavior as <code>\"external\"</code>.</p> </li> </ul> <code>None</code> <code>config</code> <code>dict | None</code> <p>The configuration options for displaying this figure, by default None. This <code>config</code> parameter is the same as the dict that you would pass as <code>config</code> argument to the <code>show</code> method. See more https://plotly.com/python/configuration-options/</p> <code>None</code> <code>init_dash_kwargs</code> <code>dict | None</code> <p>Keyword arguments for the Dash app constructor.</p> <p>Note</p> <p>This variable is of special interest when working in a jupyterhub + kubernetes environment. In this case, user notebook servers are spawned as separate pods and user access to those servers are proxied via jupyterhub. Dash requires the <code>requests_pathname_prefix</code> to be set on init - which can be done via this <code>init_dash_kwargs</code> argument. Note that you should also pass the <code>jupyter_server_url</code> to the <code>show_dash</code> method. More details: https://github.com/predict-idlab/plotly-resampler/issues/265</p> <code>None</code> <code>graph_properties</code> <code>dict | None</code> <p>Dictionary of (keyword, value) for the properties that should be passed to the dcc.Graph, by default None. e.g.: <code>{\"style\": {\"width\": \"50%\"}}</code> Note: \u201cconfig\u201d is not allowed as key in this dict, as there is a distinct <code>config</code> parameter for this property in this method. See more https://dash.plotly.com/dash-core-components/graph</p> <code>None</code> <code>**kwargs</code> <p>kwargs for the <code>app.run_server()</code> method, e.g., port=8037.</p> <p>Note</p> <p>These kwargs take precedence over the ones that are passed to the constructor via the <code>show_dash_kwargs</code> argument.</p> <code>{}</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def show_dash(\n    self,\n    mode=None,\n    config: dict | None = None,\n    init_dash_kwargs: dict | None = None,\n    graph_properties: dict | None = None,\n    **kwargs,\n):\n    \"\"\"Registers the `update_graph` callback &amp; show the figure in a dash app.\n\n    Parameters\n    ----------\n    mode: str, optional\n        Display mode. One of:\\n\n          * ``\"external\"``: The URL of the app will be displayed in the notebook\n            output cell. Clicking this URL will open the app in the default\n            web browser.\n          * ``\"inline\"``: The app will be displayed inline in the notebook output\n            cell in an iframe.\n          * ``\"inline_persistent\"``: The app will be displayed inline in the\n            notebook output cell in an iframe, if the app is not reachable a static\n            image of the figure is shown. Hence this is a persistent version of the\n            ``\"inline\"`` mode, allowing users to see a static figure in other\n            environments, browsers, etc.\n\n            !!! note\n\n                This mode requires the ``kaleido`` and ``flask_cors`` package.\n                Install them : ``pip install plotly_resampler[inline_persistent]``\n                or ``pip install kaleido flask_cors``.\n\n          * ``\"jupyterlab\"``: The app will be displayed in a dedicated tab in the\n            JupyterLab interface. Requires JupyterLab and the ``jupyterlab-dash``\n            extension.\n        By default None, which will result in the same behavior as ``\"external\"``.\n    config: dict, optional\n        The configuration options for displaying this figure, by default None.\n        This ``config`` parameter is the same as the dict that you would pass as\n        ``config`` argument to the `show` method.\n        See more [https://plotly.com/python/configuration-options/](https://plotly.com/python/configuration-options/)\n    init_dash_kwargs: dict, optional\n        Keyword arguments for the Dash app constructor.\n        !!! note\n            This variable is of special interest when working in a jupyterhub +\n            kubernetes environment. In this case, user notebook servers are spawned\n            as separate pods and user access to those servers are proxied via\n            jupyterhub. Dash requires the `requests_pathname_prefix` to be set on\n            __init__ - which can be done via this `init_dash_kwargs` argument.\n            Note that you should also pass the `jupyter_server_url` to the\n            `show_dash` method.\n            More details: https://github.com/predict-idlab/plotly-resampler/issues/265\n    graph_properties: dict, optional\n        Dictionary of (keyword, value) for the properties that should be passed to\n        the dcc.Graph, by default None.\n        e.g.: `{\"style\": {\"width\": \"50%\"}}`\n        Note: \"config\" is not allowed as key in this dict, as there is a distinct\n        ``config`` parameter for this property in this method.\n        See more [https://dash.plotly.com/dash-core-components/graph](https://dash.plotly.com/dash-core-components/graph)\n    **kwargs: dict\n        kwargs for the ``app.run_server()`` method, e.g., port=8037.\n        !!! note\n            These kwargs take precedence over the ones that are passed to the\n            constructor via the ``show_dash_kwargs`` argument.\n\n    \"\"\"\n    available_modes = list(dash._jupyter.JupyterDisplayMode.__args__) + [\n        \"inline_persistent\"\n    ]\n    assert (\n        mode is None or mode in available_modes\n    ), f\"mode must be one of {available_modes}\"\n    graph_properties = {} if graph_properties is None else graph_properties\n    assert \"config\" not in graph_properties  # There is a param for config\n    if self[\"layout\"][\"autosize\"] is True and self[\"layout\"][\"height\"] is None:\n        graph_properties.setdefault(\"style\", {}).update({\"height\": \"100%\"})\n\n    # 0. Check if the traces need to be updated when there is a xrange set\n    # This will be the case when the users has set a xrange (via the `update_layout`\n    # or `update_xaxes` methods`)\n    relayout_dict = {}\n    for xaxis_str in self._xaxis_list:\n        x_range = self.layout[xaxis_str].range\n        if x_range:  # when not None\n            relayout_dict[f\"{xaxis_str}.range[0]\"] = x_range[0]\n            relayout_dict[f\"{xaxis_str}.range[1]\"] = x_range[1]\n    if relayout_dict:  # when not empty\n        update_data = self._construct_update_data(relayout_dict)\n\n        if not self._is_no_update(update_data):  # when there is an update\n            with self.batch_update():\n                # First update the layout (first item of update_data)\n                self.layout.update(self._parse_relayout(update_data[0]))\n\n                # Then update the data\n                for updated_trace in update_data[1:]:\n                    trace_idx = updated_trace.pop(\"index\")\n                    self.data[trace_idx].update(updated_trace)\n\n    # 1. Construct the Dash app layout\n    init_dash_kwargs = {} if init_dash_kwargs is None else init_dash_kwargs\n    if self._create_overview:\n        # fmt: off\n        # Add the assets folder to the init_dash_kwargs\n        init_dash_kwargs[\"assets_folder\"] = os.path.relpath(ASSETS_FOLDER, os.getcwd())\n        # Also include the lodash script, as the client-side callbacks uses this\n        init_dash_kwargs[\"external_scripts\"] = [\"https://cdn.jsdelivr.net/npm/lodash/lodash.min.js\" ]\n        # fmt: on\n\n    # fmt: off\n    div = dash.html.Div(\n        children=[\n            dash.dcc.Graph(\n                id=\"resample-figure\", figure=self, config=config, **graph_properties\n            )\n        ],\n        style={\n            \"display\": \"flex\", \"flex-flow\": \"column\",\n            \"height\": \"95vh\", \"width\": \"100%\",\n        },\n    )\n    # fmt: on\n    if self._create_overview:\n        overview_config = config.copy() if config is not None else {}\n        overview_config[\"displayModeBar\"] = False\n        coarse_fig = self._create_overview_figure()\n        div.children += [\n            dash.dcc.Graph(\n                id=\"overview-figure\",\n                figure=coarse_fig,\n                config=overview_config,\n                **graph_properties,\n            ),\n        ]\n\n    # Create the app, populate the layout and register the resample callback\n    app = dash.Dash(\"local_app\", **init_dash_kwargs)\n    app.layout = div\n    self.register_update_graph_callback(\n        app,\n        \"resample-figure\",\n        \"overview-figure\" if self._create_overview else None,\n    )\n\n    # 2. Run the app\n    height_param = \"height\" if mode == \"inline_persistent\" else \"jupyter_height\"\n    if \"inline\" in mode and height_param not in kwargs:\n        # If app height is not specified -&gt; re-use figure height for inline dash app\n        #  Note: default layout height is 450 (whereas default app height is 650)\n        #  See: https://plotly.com/python/reference/layout/#layout-height\n        fig_height = self.layout.height if self.layout.height is not None else 450\n        kwargs[height_param] = fig_height + 18\n\n    # kwargs take precedence over the show_dash_kwargs\n    kwargs = {**self._show_dash_kwargs, **kwargs}\n\n    # Store the app information, so it can be killed\n    self._app = app\n    self._host = kwargs.get(\"host\", \"127.0.0.1\")\n    self._port = kwargs.get(\"port\", \"8050\")\n\n    # function signatures are slightly different for the (Jupyter)Dash and the\n    # JupyterDashInlinePersistent implementations\n    if mode == \"inline_persistent\":\n        from .jupyter_dash_persistent_inline_output import (\n            JupyterDashPersistentInlineOutput,\n        )\n\n        jpi = JupyterDashPersistentInlineOutput(self)\n        jpi.run_app(app=app, **kwargs)\n    else:\n        app.run(jupyter_mode=mode, **kwargs)\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler/#figure_resampler.figure_resampler.FigureResampler.stop_server","title":"<code>stop_server(warn=True)</code>","text":"<p>Stop the running dash-app.</p> <p>Parameters:</p> Name Type Description Default <code>warn</code> <code>bool</code> <p>Whether a warning message will be shown or  not, by default True.</p> <code>True</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler.py</code> <pre><code>def stop_server(self, warn: bool = True):\n    \"\"\"Stop the running dash-app.\n\n    Parameters\n    ----------\n    warn: bool\n        Whether a warning message will be shown or  not, by default True.\n\n    !!! warning\n\n        This only works if the dash-app was started with [`show_dash`][figure_resampler.figure_resampler.FigureResampler.show_dash].\n    \"\"\"\n    if self._app is not None:\n        servers_dict = dash.jupyter_dash._servers\n        old_server = servers_dict.get((self._host, self._port))\n        if old_server:\n            old_server.shutdown()\n        del servers_dict[(self._host, self._port)]\n    elif warn:\n        warnings.warn(\n            \"Could not stop the server, either the \\n\"\n            + \"\\t- 'show-dash' method was not called, or \\n\"\n            + \"\\t- the dash-server wasn't started with 'show_dash'\"\n        )\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/","title":"figure_resampler_interface","text":"<p>Abstract <code>AbstractFigureAggregator</code> interface for the concrete Resampler classes.</p>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator","title":"<code>AbstractFigureAggregator</code>","text":"<p>               Bases: <code>BaseFigure</code>, <code>ABC</code></p> <p>Abstract interface for data aggregation functionality for plotly figures.</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler_interface.py</code> <pre><code>class AbstractFigureAggregator(BaseFigure, ABC):\n    \"\"\"Abstract interface for data aggregation functionality for plotly figures.\"\"\"\n\n    _high_frequency_traces = [\"scatter\", \"scattergl\"]\n\n    def __init__(\n        self,\n        figure: BaseFigure,\n        convert_existing_traces: bool = True,\n        default_n_shown_samples: int = 1000,\n        default_downsampler: AbstractAggregator = MinMaxLTTB(),\n        default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n        resampled_trace_prefix_suffix: Tuple[str, str] = (\n            '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n            \"\",\n        ),\n        show_mean_aggregation_size: bool = True,\n        convert_traces_kwargs: dict | None = None,\n        verbose: bool = False,\n    ):\n        \"\"\"Instantiate a resampling data mirror.\n\n        Parameters\n        ----------\n        figure: BaseFigure\n            The figure that will be decorated. Can be either an empty figure\n            (e.g., ``go.Figure()``, ``make_subplots()``, ``go.FigureWidget``) or an\n            existing figure.\n        convert_existing_traces: bool\n            A bool indicating whether the high-frequency traces of the passed ``figure``\n            should be resampled, by default True. Hence, when set to False, the\n            high-frequency traces of the passed ``figure`` will not be resampled.\n        default_n_shown_samples: int, optional\n            The default number of samples that will be shown for each trace,\n            by default 1000.\\n\n            !!! note\n                * This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n                * If a trace withholds fewer datapoints than this parameter,\n                  the data will *not* be aggregated.\n        default_downsampler: AbstractAggregator\n            An instance which implements the AbstractSeriesDownsampler interface and\n            will be used as default downsampler, by default ``MinMaxLTTB``. \\n\n            !!! note\n                This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n        default_gap_handler: GapHandler\n            An instance which implements the AbstractGapHandler interface and will be\n            used as default gap handler, by default ``MedDiffGapHandler``. \\n\n            !!! note\n                This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n        resampled_trace_prefix_suffix: str, optional\n            A tuple which contains the ``prefix`` and ``suffix``, respectively, which\n            will be added to the trace its legend-name when a resampled version of the\n            trace is shown. By default, a bold, orange ``[R]`` is shown as prefix\n            (no suffix is shown).\n        show_mean_aggregation_size: bool, optional\n            Whether the mean aggregation bin size will be added as a suffix to the trace\n            its legend-name, by default True.\n        convert_traces_kwargs: dict, optional\n            A dict of kwargs that will be passed to the [`add_traces`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_traces] method and\n            will be used to convert the existing traces. \\n\n            !!! note\n                This argument is only used when the passed ``figure`` contains data and\n                ``convert_existing_traces`` is set to True.\n        verbose: bool, optional\n            Whether some verbose messages will be printed or not, by default False.\n\n        \"\"\"\n        self._hf_data: Dict[str, dict] = {}\n        self._global_n_shown_samples = default_n_shown_samples\n        self._print_verbose = verbose\n        self._show_mean_aggregation_size = show_mean_aggregation_size\n\n        assert len(resampled_trace_prefix_suffix) == 2\n        self._prefix, self._suffix = resampled_trace_prefix_suffix\n\n        self._global_downsampler = default_downsampler\n        self._global_gap_handler = default_gap_handler\n\n        # Given figure should always be a BaseFigure that is not wrapped by\n        # a plotly-resampler class\n        assert isinstance(figure, BaseFigure)\n        assert not issubclass(type(figure), AbstractFigureAggregator)\n        self._figure_class = figure.__class__\n\n        # Overwrite the passed arguments with the property dict values\n        # (this is the case when the PR figure is created from a pickled object)\n        if hasattr(figure, \"_pr_props\"):\n            pr_props = figure._pr_props  # a dict of PR properties\n            if pr_props is not None:\n                # Overwrite the default arguments with the serialized properties\n                for k, v in pr_props.items():\n                    setattr(self, k, v)\n            delattr(figure, \"_pr_props\")  # should not be stored anymore\n\n        if convert_existing_traces:\n            # call __init__ with the correct layout and set the `_grid_ref` of the\n            # to-be-converted figure\n            f_ = self._figure_class(layout=figure.layout)\n            f_._grid_str = figure._grid_str\n            f_._grid_ref = figure._grid_ref\n            super().__init__(f_)\n\n            if convert_traces_kwargs is None:\n                convert_traces_kwargs = {}\n\n            # make sure that the UIDs of these traces do not get adjusted\n            self._data_validator.set_uid = False\n            self.add_traces(figure.data, **convert_traces_kwargs)\n        else:\n            super().__init__(figure)\n            self._data_validator.set_uid = False\n\n        # A list of al xaxis and yaxis string names\n        # e.g., \"xaxis\", \"xaxis2\", \"xaxis3\", .... for _xaxis_list\n        self._xaxis_list = self._re_matches(\n            re.compile(r\"xaxis\\d*\"), self._layout.keys()\n        )\n        self._yaxis_list = self._re_matches(\n            re.compile(r\"yaxis\\d*\"), self._layout.keys()\n        )\n        # edge case: an empty `go.Figure()` does not yet contain axes keys\n        if not len(self._xaxis_list):\n            self._xaxis_list = [\"xaxis\"]\n            self._yaxis_list = [\"yaxis\"]\n\n        # Make sure to reset the layout its range\n        # self.update_layout(\n        #     {\n        #         axis: {\"autorange\": None, \"range\": None}\n        #         for axis in self._xaxis_list + self._yaxis_list\n        #     }\n        # )\n\n    def _print(self, *values):\n        \"\"\"Helper method for printing if ``verbose`` is set to True.\"\"\"\n        if self._print_verbose:\n            print(*values)\n\n    def _query_hf_data(self, trace: dict) -&gt; Optional[dict]:\n        \"\"\"Query the internal ``_hf_data`` attribute and returns a match based on\n        ``uid``.\n\n        Parameters\n        ----------\n        trace : dict\n            The trace where we want to find a match for.\n\n        Returns\n        -------\n        Optional[dict]\n            The ``hf_data``-trace dict if a match is found, else ``None``.\n\n        \"\"\"\n        uid = trace[\"uid\"]\n        hf_trace_data = self._hf_data.get(uid)\n        if hf_trace_data is None:\n            trace_props = {\n                k: trace[k] for k in set(trace.keys()).difference({\"x\", \"y\"})\n            }\n            self._print(f\"[W] trace with {trace_props} not found\")\n        return hf_trace_data\n\n    def _get_current_graph(self) -&gt; dict:\n        \"\"\"Create an efficient copy of the current graph by omitting the \"hovertext\",\n        \"x\", and \"y\" properties of each trace.\n\n        Returns\n        -------\n        dict\n            The current graph dict\n\n        See Also\n        --------\n        https://github.com/plotly/plotly.py/blob/2e7f322c5ea4096ce6efe3b4b9a34d9647a8be9c/packages/python/plotly/plotly/basedatatypes.py#L3278\n        \"\"\"\n        return {\n            \"data\": [\n                {\n                    k: copy(trace[k])\n                    # TODO: why not \"text\" as well? -&gt; we can use _hf_data_container.fields then\n                    for k in set(trace.keys()).difference({\"x\", \"y\", \"hovertext\"})\n                }\n                for trace in self._data\n            ],\n            \"layout\": copy(self._layout),\n        }\n\n    def _parse_trace_name(\n        self, hf_trace_data: dict, slice_len: int, agg_x: np.ndarray\n    ) -&gt; str:\n        \"\"\"Parse the trace name.\n\n        Parameters\n        ----------\n        hf_trace_data : dict\n            The high-frequency trace data dict.\n        slice_len : int\n            The length of the slice.\n        agg_x : np.ndarray\n            The x-axis values of the aggregated trace.\n\n        Returns\n        -------\n        str\n            The parsed trace name.\n            When no downsampling is needed, the original trace name is returned.\n            When downsampling is needed, the average bin size (expressed in x-units) is\n            added in orange color with a `~` to the trace name.\n\n        \"\"\"\n        if slice_len &lt;= hf_trace_data[\"max_n_samples\"]:  # When no downsampling needed\n            return hf_trace_data[\"name\"]\n\n        # The data is downsampled, so we add the downsampling information to the name\n        agg_prefix, agg_suffix = ' &lt;i style=\"color:#fc9944\"&gt;~', \"&lt;/i&gt;\"\n        name = self._prefix + hf_trace_data[\"name\"] + self._suffix\n\n        # Add the mean aggregation bin size to the trace name\n        if self._show_mean_aggregation_size:\n            # Base case ...\n            if len(agg_x) &lt; 2:\n                return name\n\n            mean_bin_size = (agg_x[-1] - agg_x[0]) / agg_x.shape[0]  # mean bin size\n            if isinstance(mean_bin_size, (np.timedelta64, pd.Timedelta)):\n                mean_bin_size = round_td_str(pd.Timedelta(mean_bin_size))\n            else:\n                mean_bin_size = round_number_str(mean_bin_size)\n            name += f\"{agg_prefix}{mean_bin_size}{agg_suffix}\"\n        return name\n\n    def _check_update_trace_data(\n        self,\n        trace: dict,\n        start: Optional[Union[str, float]] = None,\n        end: Optional[Union[str, float]] = None,\n    ) -&gt; Optional[Union[dict, BaseTraceType]]:\n        \"\"\"Check and update the passed ``trace`` its data properties based on the\n        slice range.\n\n        Note\n        ----\n        This is a pass by reference. The passed trace object will be updated and\n        returned if found in ``hf_data``.\n\n        Parameters\n        ----------\n        trace : BaseTraceType or dict\n             - An instances of a trace class from the ``plotly.graph_objects`` (go)\n                package (e.g, ``go.Scatter``, ``go.Bar``)\n             - or a dict where:\n\n                  - The 'type' property specifies the trace type (e.g.\n                    'scatter', 'bar', 'area', etc.). If the dict has no 'type'\n                    property then 'scatter' is assumed.\n                  - All remaining properties are passed to the constructor\n                    of the specified trace type.\n\n        start : Union[float, str], optional\n            The start index for which we want resampled data to be updated to,\n            by default None,\n        end : Union[float, str], optional\n            The end index for which we want the resampled data to be updated to,\n            by default None\n\n        Returns\n        -------\n        Optional[Union[dict, BaseTraceType]]\n            If the matching ``hf_series`` is found in ``hf_dict``, an (updated) trace\n            will be returned, otherwise None.\n\n        Note\n        ----\n        * If ``start`` and ``stop`` are strings, they most likely represent time-strings\n        * ``start`` and ``stop`` will always be of the same type (float / time-string)\n           because their underlying axis is the same.\n\n        \"\"\"\n        hf_trace_data = self._query_hf_data(trace)\n\n        if hf_trace_data is None:\n            self._print(\"hf_data not found\")\n            return None\n\n        # Parse trace data (necessary when updating the trace data)\n        for k in _hf_data_container._fields:\n            if isinstance(\n                hf_trace_data[k], (np.ndarray, pd.RangeIndex, pd.DatetimeIndex)\n            ):\n                # is faster to escape the loop here than check inside the hasattr if\n                continue\n            elif pd.DatetimeTZDtype.is_dtype(hf_trace_data[k]):\n                # When we use the .values method, timezone information is lost\n                # so convert it to pd.DatetimeIndex, which preserves the tz-info\n                hf_trace_data[k] = pd.Index(hf_trace_data[k])\n            elif hasattr(hf_trace_data[k], \"values\"):\n                # when not a range index or datetime index\n                hf_trace_data[k] = hf_trace_data[k].values\n\n        # Also check if the y-data is empty, if so, return an empty trace\n        if len(hf_trace_data[\"y\"]) == 0:\n            trace[\"x\"] = []\n            trace[\"y\"] = []\n            trace[\"name\"] = hf_trace_data[\"name\"]\n            return trace\n\n        # Leverage the axis type to get the start and end indices\n        # Note: the axis type specified in the figure layout takes precedence over the\n        # the axis type which is inferred from the data (and stored in hf_trace_data)\n        # TODO: verify if we need to use `axis`of anchor as key to determing axis type\n        axis = trace.get(\"xaxis\", \"x\")\n        axis_type = self.layout._props.get(axis[:1] + \"axis\" + axis[1:], {}).get(\n            \"type\", hf_trace_data[\"axis_type\"]\n        )\n        start_idx, end_idx = PlotlyAggregatorParser.get_start_end_indices(\n            hf_trace_data, axis_type, start, end\n        )\n\n        # Return an invisible, single-point, trace when the sliced hf_series doesn't\n        # contain any data in the current view\n        if end_idx == start_idx:\n            trace[\"x\"] = [hf_trace_data[\"x\"][0]]\n            trace[\"y\"] = [None]\n            trace[\"name\"] = hf_trace_data[\"name\"]\n            return trace\n\n        agg_x, agg_y, indices = PlotlyAggregatorParser.aggregate(\n            hf_trace_data, start_idx, end_idx\n        )\n\n        # -------------------- Set the hf_trace_data_props -------------------\n        trace[\"x\"] = agg_x\n        trace[\"y\"] = agg_y\n        trace[\"name\"] = self._parse_trace_name(\n            hf_trace_data, end_idx - start_idx, agg_x\n        )\n\n        def _nest_dict_rec(k: str, v: any, out: dict) -&gt; None:\n            \"\"\"Recursively nest a dict based on the key whose '_' indicates level.\"\"\"\n            k, *rest = k.split(\"_\", 1)\n            if rest:\n                _nest_dict_rec(rest[0], v, out.setdefault(k, {}))\n            else:\n                out[k] = v\n\n        # Check if (hover)text also needs to be downsampled\n        for k in [\"text\", \"hovertext\", \"marker_size\", \"marker_color\", \"customdata\"]:\n            k_val = hf_trace_data.get(k)\n            if isinstance(k_val, (np.ndarray, pd.Series)):\n                assert isinstance(\n                    hf_trace_data[\"downsampler\"], DataPointSelector\n                ), \"Only DataPointSelector can downsample non-data trace array props.\"\n                _nest_dict_rec(k, k_val[start_idx + indices], trace)\n            elif k_val is not None:\n                trace[k] = k_val\n\n        return trace\n\n    def _layout_xaxis_to_trace_xaxis_mapping(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Construct a dict which maps the layout xaxis keys to the trace xaxis keys.\n\n        Returns\n        -------\n        Dict[str, List[str]]\n            A dict with the layout xaxis values as keys and the trace its corresponding\n            xaxis anchor value.\n\n        \"\"\"\n        # edge case: an empty `go.Figure()` does not yet contain axes keys\n        if self._grid_ref is None:\n            return {\"xaxis\": [\"x\"]}\n\n        mapping_dict = {}\n        for sub_plot in itertools.chain.from_iterable(self._grid_ref):  # flattten\n            sub_plot = [] if sub_plot is None else sub_plot\n            for axes in sub_plot:  # NOTE: you can have multiple axes in a subplot\n                layout_xaxes = axes.layout_keys[0]\n                trace_xaxes = axes.trace_kwargs[\"xaxis\"]\n\n                # append the trace xaxis to the layout xaxis key its value list\n                mapping_dict.setdefault(layout_xaxes, []).append(trace_xaxes)\n        return mapping_dict\n\n    def _check_update_figure_dict(\n        self,\n        figure: dict,\n        start: Optional[Union[float, str]] = None,\n        stop: Optional[Union[float, str]] = None,\n        layout_xaxis_filter: Optional[str] = None,\n        updated_trace_indices: Optional[List[int]] = None,\n    ) -&gt; List[int]:\n        \"\"\"Check and update the traces within the figure dict.\n\n        hint\n        ----\n        This method will most likely be used within a ``Dash`` callback to resample the\n        view, based on the configured number of parameters.\n\n        Note\n        ----\n        This is a pass by reference. The passed figure object will be updated.\n        No new view of this figure will be created, hence no return!\n\n        Parameters\n        ----------\n        figure : dict\n            The figure dict which will be updated.\n        start : Union[float, str], optional\n            The start time for the new resampled data view, by default None.\n        stop : Union[float, str], optional\n            The end time for the new resampled data view, by default None.\n        layout_xaxis_filter: str, optional\n            Additional layout xaxis filter, e.g. the affected x-axis values by the\n            triggered relayout event (e.g. xaxis), by default None.\n        updated_trace_indices: List[int], optional\n            List of trace indices that already have been updated, by default None.\n\n        Returns\n        -------\n        List[int]\n            A list of indices withholding the trace-data-array-index from the of data\n            modalities which are updated.\n\n        \"\"\"\n        if updated_trace_indices is None:\n            updated_trace_indices = []\n\n        if layout_xaxis_filter is not None:\n            layout_trace_mapping = self._layout_xaxis_to_trace_xaxis_mapping()\n            # Retrieve the trace xaxis values that are affected by the relayout event\n            trace_xaxis_filter: List[str] = layout_trace_mapping[layout_xaxis_filter]\n\n        for idx, trace in enumerate(figure[\"data\"]):\n            # We skip when (i) the trace-idx already has been updated or (ii) when\n            # there is a layout_xaxis_filter and the trace xaxis is not in the filter\n            if idx in updated_trace_indices or (\n                layout_xaxis_filter is not None\n                and trace.get(\"xaxis\", \"x\") not in trace_xaxis_filter\n            ):\n                continue\n\n            # If we managed to find and update the trace, it will return the trace\n            # and thus not None.\n            updated_trace = self._check_update_trace_data(trace, start=start, end=stop)\n            if updated_trace is not None:\n                updated_trace_indices.append(idx)\n        return updated_trace_indices\n\n    @staticmethod\n    def _get_figure_class(constr: type) -&gt; type:\n        \"\"\"Get the plotly figure class (constructor) for the given class (constructor).\n\n        !!! note\n            This method will always return a plotly constructor, even when the given\n            `constr` is decorated (after executing the ``register_plotly_resampler``\n            function).\n\n        Parameters\n        ----------\n        constr: type\n            The constructor class for which we want to retrieve the plotly constructor.\n\n        Returns\n        -------\n        type:\n            The plotly figure class (constructor) of the given `constr`.\n\n        \"\"\"\n        from ..registering import _get_plotly_constr  # To avoid ImportError\n\n        return _get_plotly_constr(constr)\n\n    @property\n    def hf_data(self):\n        \"\"\"Property to adjust the `data` component of the current graph\n\n        !!! note\n            The user has full responsibility to adjust ``hf_data`` properly.\n\n\n        ??? example\n\n            ```python\n                &gt;&gt;&gt; from plotly_resampler import FigureResampler\n                &gt;&gt;&gt; fig = FigureResampler(go.Figure())\n                &gt;&gt;&gt; fig.add_trace(...)\n                &gt;&gt;&gt; # Adjust the y property of the above added trace\n                &gt;&gt;&gt; fig.hf_data[-1][\"y\"] = - s ** 2\n                &gt;&gt;&gt; fig.hf_data\n                [\n                    {\n                        'max_n_samples': 1000,\n                        'x': RangeIndex(start=0, stop=11000000, step=1),\n                        'y': array([-0.01339909,  0.01390696,, ...,  0.25051913, 0.55876513]),\n                        'axis_type': 'linear',\n                        'downsampler': &lt;plotly_resampler.aggregation.aggregators.LTTB at 0x7f786d5a9ca0&gt;,\n                        'text': None,\n                        'hovertext': None\n                    },\n                ]\n            ```\n        \"\"\"\n        return list(self._hf_data.values())\n\n    def _parse_get_trace_props(\n        self,\n        trace: BaseTraceType,\n        hf_x: Iterable = None,\n        hf_y: Iterable = None,\n        hf_text: Iterable = None,\n        hf_hovertext: Iterable = None,\n        hf_marker_size: Iterable = None,\n        hf_marker_color: Iterable = None,\n    ) -&gt; _hf_data_container:\n        \"\"\"Parse and capture the possibly high-frequency trace-props in a datacontainer.\n\n        Parameters\n        ----------\n        trace : BaseTraceType\n            The trace which will be parsed.\n        hf_x : Iterable, optional\n            High-frequency trace \"x\" data, overrides the current trace its x-data.\n        hf_y : Iterable, optional\n            High-frequency trace \"y\" data, overrides the current trace its y-data.\n        hf_text : Iterable, optional\n            High-frequency trace \"text\" data, overrides the current trace its text-data.\n        hf_hovertext : Iterable, optional\n            High-frequency trace \"hovertext\" data, overrides the current trace its\n            hovertext data.\n\n        Returns\n        -------\n        _hf_data_container\n            A namedtuple which serves as a datacontainer.\n\n        \"\"\"\n        hf_x: np.ndarray | pd.Index = (\n            # fmt: off\n            (np.asarray(trace[\"x\"]) if trace[\"x\"] is not None else None)\n            if hasattr(trace, \"x\") and hf_x is None\n            # If we cast a tz-aware datetime64 array to `.values` we lose the tz-info \n            # and the UTC time will be displayed instead of the tz-localized time, \n            # hence we cast to a pd.DatetimeIndex, which preserves the tz-info\n            # As a matter of fact, to resolve #231, we also convert non-tz-aware \n            # datetime64 arrays to an pd.Index\n            else pd.Index(hf_x) if pd.core.dtypes.common.is_datetime64_any_dtype(hf_x)\n            else hf_x.values if isinstance(hf_x, pd.Series)\n            else hf_x if isinstance(hf_x, pd.Index)\n            else np.asarray(hf_x)\n            # fmt: on\n        )\n        if pd.core.dtypes.common.is_datetime64_any_dtype(hf_x):\n            hf_x = pd.Index(hf_x)\n\n        hf_y = (\n            trace[\"y\"]\n            if hasattr(trace, \"y\") and hf_y is None\n            else hf_y.values if isinstance(hf_y, (pd.Series, pd.Index)) else hf_y\n        )\n        # NOTE: the if will not be triggered for a categorical series its values\n        if not hasattr(hf_y, \"dtype\"):\n            hf_y: np.ndarray = np.asarray(hf_y)\n\n        hf_text = (\n            hf_text\n            if hf_text is not None\n            else (\n                trace[\"text\"]\n                if hasattr(trace, \"text\") and trace[\"text\"] is not None\n                else None\n            )\n        )\n\n        hf_hovertext = (\n            hf_hovertext\n            if hf_hovertext is not None\n            else (\n                trace[\"hovertext\"]\n                if hasattr(trace, \"hovertext\") and trace[\"hovertext\"] is not None\n                else None\n            )\n        )\n\n        hf_marker_size = (\n            trace[\"marker\"][\"size\"]\n            if (\n                hf_marker_size is None\n                and hasattr(trace, \"marker\")\n                and \"size\" in trace[\"marker\"]\n            )\n            else hf_marker_size\n        )\n\n        hf_marker_color = (\n            trace[\"marker\"][\"color\"]\n            if (\n                hf_marker_color is None\n                and hasattr(trace, \"marker\")\n                and \"color\" in trace[\"marker\"]\n            )\n            else hf_marker_color\n        )\n\n        hf_customdata = trace[\"customdata\"] if hasattr(trace, \"customdata\") else None\n\n        if trace[\"type\"].lower() in self._high_frequency_traces:\n            if hf_x is None:  # if no data as x or hf_x is passed\n                if hf_y.ndim != 0:  # if hf_y is an array\n                    hf_x = pd.RangeIndex(0, len(hf_y))  # np.arange(len(hf_y))\n                else:  # if no data as y or hf_y is passed\n                    hf_x = np.asarray([])\n                    hf_y = np.asarray([])\n\n            assert hf_y.ndim == np.ndim(hf_x), (\n                \"plotly-resampler requires scatter data \"\n                \"(i.e., x and y, or hf_x and hf_y) to have the same dimensionality!\"\n            )\n            # When the x or y of a trace has more than 1 dimension, it is not at all\n            # straightforward how it should be resampled.\n            assert hf_y.ndim &lt;= 1 and np.ndim(hf_x) &lt;= 1, (\n                \"plotly-resampler requires scatter data \"\n                \"(i.e., x and y, or hf_x and hf_y) to be &lt;= 1 dimensional!\"\n            )\n\n            # Note: this converts the hf property to a np.ndarray\n            if isinstance(hf_text, (tuple, list, np.ndarray, pd.Series)):\n                hf_text = np.asarray(hf_text)\n            if isinstance(hf_hovertext, (tuple, list, np.ndarray, pd.Series)):\n                hf_hovertext = np.asarray(hf_hovertext)\n            if isinstance(hf_marker_size, (tuple, list, np.ndarray, pd.Series)):\n                hf_marker_size = np.asarray(hf_marker_size)\n            if isinstance(hf_marker_color, (tuple, list, np.ndarray, pd.Series)):\n                hf_marker_color = np.asarray(hf_marker_color)\n\n            # Try to parse the hf_x data if it is of object type or\n            if len(hf_x) and (hf_x.dtype.type is np.str_ or hf_x.dtype == \"object\"):\n                try:\n                    # Try to parse to numeric\n                    hf_x = pd.to_numeric(hf_x, errors=\"raise\")\n                except (ValueError, TypeError):\n                    try:\n                        # Try to parse to datetime\n                        hf_x = pd.to_datetime(hf_x, utc=False, errors=\"raise\")\n                        # Will be cast to object array if it contains multiple timezones.\n                        if hf_x.dtype == \"object\":\n                            raise ValueError(\n                                \"The x-data contains multiple timezones, which is not \"\n                                \"supported by plotly-resampler!\"\n                            )\n                    except (ValueError, TypeError):\n                        raise ValueError(\n                            \"plotly-resampler requires the x-data to be numeric or \"\n                            \"datetime-like \\nMore details in the stacktrace above.\"\n                        )\n\n            # If the categorical or string-like hf_y data is of type object (happens\n            # when y argument is used for the trace constructor instead of hf_y), we\n            # transform it to type string as such it will be sent as categorical data\n            # to the downsampling algorithm\n            if hf_y.dtype == \"object\" or hf_y.dtype.type == np.str_:\n                # But first, we try to parse to a numeric dtype (as this is the\n                # behavior that plotly supports)\n                # Note that a bool array of type object will remain a bool array (and\n                # not will be transformed to an array of ints (0, 1))\n                try:\n                    hf_y = pd.to_numeric(hf_y, errors=\"raise\")\n                except ValueError:\n                    hf_y = pd.Categorical(hf_y)  # TODO: ordered=True?\n            assert len(hf_x) == len(hf_y), \"x and y have different length!\"\n        else:\n            self._print(f\"trace {trace['type']} is not a high-frequency trace\")\n\n            # hf_x and hf_y have priority over the traces' data\n            if hasattr(trace, \"x\"):\n                trace[\"x\"] = hf_x\n\n            if hasattr(trace, \"y\"):\n                trace[\"y\"] = hf_y\n\n            if hasattr(trace, \"text\"):\n                trace[\"text\"] = hf_text\n\n            if hasattr(trace, \"hovertext\"):\n                trace[\"hovertext\"] = hf_hovertext\n            if hasattr(trace, \"marker\"):\n                if hasattr(trace.marker, \"size\"):\n                    trace.marker.size = hf_marker_size\n                if hasattr(trace.marker, \"color\"):\n                    trace.marker.color = hf_marker_color\n\n        return _hf_data_container(\n            hf_x,\n            hf_y,\n            hf_text,\n            hf_hovertext,\n            hf_marker_size,\n            hf_marker_color,\n            hf_customdata,\n        )\n\n    def _construct_hf_data_dict(\n        self,\n        dc: _hf_data_container,\n        trace: BaseTraceType,\n        downsampler: AbstractAggregator | None,\n        gap_handler: AbstractGapHandler | None,\n        max_n_samples: int | None,\n        offset=0,\n    ) -&gt; dict:\n        \"\"\"Create the `hf_data` dict which will be put in the `_hf_data` property.\n\n        Parameters\n        ----------\n        dc : _hf_data_container\n            The hf_data container, withholding the parsed hf-data.\n        trace : BaseTraceType\n            The trace.\n        downsampler : AbstractAggregator | None\n            The downsampler which will be used.\n        gap_handler : AbstractGapHandler | None\n            The gap handler which will be used.\n        max_n_samples : int | None\n            The max number of output samples.\n\n        Returns\n        -------\n        dict\n            The hf_data dict.\n        \"\"\"\n        # Checking this now avoids less interpretable `KeyError` when resampling\n        assert_text = (\n            \"In order to perform time series aggregation, the data must be \"\n            \"sorted in time; i.e., the x-data must be (non-strictly) \"\n            \"monotonically increasing.\"\n        )\n        if isinstance(dc.x, pd.Index):\n            assert dc.x.is_monotonic_increasing, assert_text\n        else:\n            assert pd.Series(dc.x).is_monotonic_increasing, assert_text\n\n        # As we support prefix-suffixing of downsampled data, we assure that\n        # each trace has a name\n        # https://github.com/plotly/plotly.py/blob/ce0ed07d872c487698bde9d52e1f1aadf17aa65f/packages/python/plotly/plotly/basedatatypes.py#L539\n        # The link above indicates that the trace index is derived from `data`\n        if trace.name is None:\n            trace.name = f\"trace {len(self.data) + offset}\"\n\n        # Determine (1) the axis type and (2) the downsampler instance\n        # &amp; (3) store a hf_data entry for the corresponding trace,\n        # identified by its UUID\n        axis_type = (\n            \"date\"\n            if isinstance(dc.x, pd.DatetimeIndex)\n            or pd.core.dtypes.common.is_datetime64_any_dtype(dc.x)\n            else \"linear\"\n        )\n\n        default_n_samples = False\n        if max_n_samples is None:\n            default_n_samples = True\n            max_n_samples = self._global_n_shown_samples\n\n        default_downsampler = False\n        if downsampler is None:\n            default_downsampler = True\n            downsampler = self._global_downsampler\n\n        default_gap_handler = False\n        if gap_handler is None:\n            default_gap_handler = True\n            gap_handler = self._global_gap_handler\n\n        # TODO -&gt; can't we just store the DC here (might be less duplication of\n        #  code knowledge, because now, you need to know all the eligible hf_keys in\n        #  dc\n        return {\n            \"max_n_samples\": max_n_samples,\n            \"default_n_samples\": default_n_samples,\n            \"name\": trace.name,\n            \"axis_type\": axis_type,\n            \"downsampler\": downsampler,\n            \"default_downsampler\": default_downsampler,\n            \"gap_handler\": gap_handler,\n            \"default_gap_handler\": default_gap_handler,\n            **dc._asdict(),\n        }\n\n    @staticmethod\n    def _add_trace_to_add_traces_kwargs(kwargs: dict) -&gt; dict:\n        \"\"\"Convert the `add_trace` kwargs to the `add_traces` kwargs.\"\"\"\n        # The keywords that need to be converted to a list\n        convert_keywords = [\"row\", \"col\", \"secondary_y\"]\n\n        updated_kwargs = {}  # The updated kwargs (from `add_trace` to `add_traces`)\n        for keyword in convert_keywords:\n            value = kwargs.pop(keyword, None)\n            if value is not None:\n                updated_kwargs[f\"{keyword}s\"] = [value]\n            else:\n                updated_kwargs[f\"{keyword}s\"] = None\n\n        return {**kwargs, **updated_kwargs}\n\n    def add_trace(\n        self,\n        trace: Union[BaseTraceType, dict],\n        max_n_samples: int = None,\n        downsampler: AbstractAggregator = None,\n        gap_handler: AbstractGapHandler = None,\n        limit_to_view: bool = False,\n        # Use these if you want some speedups (and are working with really large data)\n        hf_x: Iterable = None,\n        hf_y: Iterable = None,\n        hf_text: Union[str, Iterable] = None,\n        hf_hovertext: Union[str, Iterable] = None,\n        hf_marker_size: Union[str, Iterable] = None,\n        hf_marker_color: Union[str, Iterable] = None,\n        **trace_kwargs,\n    ):\n        \"\"\"Add a trace to the figure.\n\n        Parameters\n        ----------\n        trace : BaseTraceType or dict\n            Either:\n\n              - An instances of a trace class from the ``plotly.graph_objects`` (go)\n                package (e.g., ``go.Scatter``, ``go.Bar``)\n              - or a dict where:\n\n                - The type property specifies the trace type (e.g. scatter, bar,\n                  area, etc.). If the dict has no 'type' property then scatter is\n                  assumed.\n                - All remaining properties are passed to the constructor\n                  of the specified trace type.\n        max_n_samples : int, optional\n            The maximum number of samples that will be shown by the trace.\\n\n            !!! note\n                If this variable is not set; ``_global_n_shown_samples`` will be used.\n        downsampler: AbstractAggregator, optional\n            The abstract series downsampler method.\\n\n            !!! note\n                If this variable is not set, ``_global_downsampler`` will be used.\n        gap_handler: AbstractGapHandler, optional\n            The abstract series gap handler method.\\n\n            !!! note\n                If this variable is not set, ``_global_gap_handler`` will be used.\n        limit_to_view: boolean, optional\n            If set to True, the trace's datapoints will be cut to the corresponding\n            front-end view, even if the total number of samples is lower than\n            ``max_n_samples``, By default False.\\n\n            Remark that setting this parameter to True ensures that low frequency traces\n            are added to the ``hf_data`` property.\n        hf_x: Iterable, optional\n            The original high frequency series positions, can be either a time-series or\n            an increasing, numerical index. If set, this has priority over the trace its\n            data.\n        hf_y: Iterable, optional\n            The original high frequency values. If set, this has priority over the\n            trace its data.\n        hf_text: Iterable, optional\n            The original high frequency text. If set, this has priority over the trace\n            its ``text`` argument.\n        hf_hovertext: Iterable, optional\n            The original high frequency hovertext. If set, this has priority over the\n            trace its ```hovertext`` argument.\n        hf_marker_size: Iterable, optional\n            The original high frequency marker size. If set, this has priority over the\n            trace its ``marker.size`` argument.\n        hf_marker_color: Iterable, optional\n            The original high frequency marker color. If set, this has priority over the\n            trace its ``marker.color`` argument.\n        **trace_kwargs: dict\n            Additional trace related keyword arguments.\n            e.g.: row=.., col=..., secondary_y=...\n\n            !!! info \"See Also\"\n                [`Figure.add_trace`](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html#plotly.graph_objects.Figure.add_trace&gt;) docs.\n\n        Returns\n        -------\n        BaseFigure\n            The Figure on which ``add_trace`` was called on; i.e. self.\n\n        !!! note\n\n            Constructing traces with **very large data amounts** really takes some time.\n            To speed this up; use this [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method and\n\n            1. Create a trace with no data (empty lists)\n            2. pass the high frequency data to this method using the ``hf_x`` and ``hf_y``\n               parameters.\n\n            See the example below:\n                ```python\n                &gt;&gt;&gt; from plotly.subplots import make_subplots\n                &gt;&gt;&gt; s = pd.Series()  # a high-frequency series, with more than 1e7 samples\n                &gt;&gt;&gt; fig = FigureResampler(go.Figure())\n                &gt;&gt;&gt; fig.add_trace(go.Scattergl(x=[], y=[], ...), hf_x=s.index, hf_y=s)\n                ```\n\n            !!! todo\n                * explain why adding x and y to a trace is so slow\n                * check and simplify the example above\n\n        !!! tip\n\n            * If you **do not want to downsample** your data, set ``max_n_samples`` to the\n              the number of datapoints of your trace!\n\n        !!! warning\n\n            * The ``NaN`` values in either ``hf_y`` or ``trace.y`` will be omitted! We do\n              not allow ``NaN`` values in ``hf_x`` or ``trace.x``.\n            * ``hf_x``, ``hf_y``, ``hf_text``, and ``hf_hovertext`` are useful when you deal\n              with large amounts of data (as it can increase the speed of this add_trace()\n              method with ~30%). These arguments have priority over the trace's data and\n              (hover)text attributes.\n            * Low-frequency time-series data, i.e. traces that are not resampled, can hinder\n              the the automatic-zooming (y-scaling) as these will not be stored in the\n              back-end and thus not be scaled to the view.\n              To circumvent this, the ``limit_to_view`` argument can be set, resulting in\n              also storing the low-frequency series in the back-end.\n\n        \"\"\"\n        # to comply with the plotly data input acceptance behavior\n        if isinstance(trace, (list, tuple)):\n            raise ValueError(\"Trace must be either a dict or a BaseTraceType\")\n\n        max_out_s = (\n            self._global_n_shown_samples if max_n_samples is None else max_n_samples\n        )\n\n        # Validate the trace and convert to a trace object\n        if not isinstance(trace, BaseTraceType):\n            trace = self._data_validator.validate_coerce(trace)[0]\n\n        # First add a UUID, as each (even the non-hf_data traces), must contain this\n        # key for comparison. If the trace already has a UUID, we will keep it.\n        uuid_str = str(uuid4()) if trace.uid is None else trace.uid\n        trace.uid = uuid_str\n\n        # These traces will determine the autoscale its RANGE!\n        #   -&gt; so also store when `limit_to_view` is set.\n        if trace[\"type\"].lower() in self._high_frequency_traces:\n            # construct the hf_data_container\n            # TODO in future version -&gt; maybe regex on kwargs which start with `hf_`\n            dc = self._parse_get_trace_props(\n                trace,\n                hf_x,\n                hf_y,\n                hf_text,\n                hf_hovertext,\n                hf_marker_size,\n                hf_marker_color,\n            )\n\n            n_samples = len(dc.x)\n            if n_samples &gt; max_out_s or limit_to_view:\n                self._print(\n                    f\"\\t[i] DOWNSAMPLE {trace['name']}\\t{n_samples}-&gt;{max_out_s}\"\n                )\n\n                self._hf_data[uuid_str] = self._construct_hf_data_dict(\n                    dc,\n                    trace=trace,\n                    downsampler=downsampler,\n                    gap_handler=gap_handler,\n                    max_n_samples=max_n_samples,\n                )\n\n                # Before we update the trace, we create a new pointer to that trace in\n                # which the downsampled data will be stored. This way, the original\n                # data of the trace to this `add_trace` method will not be altered.\n                # We copy (by reference) all the non-data properties of the trace in\n                # the new trace.\n                trace = trace._props  # convert the trace into a dict\n                # NOTE: there is no need to store `marker` property here.\n                # If needed, it will be added  to `trace` via `check_update_trace_data`\n                trace = {\n                    k: trace[k] for k in set(trace.keys()).difference(set(dc._fields))\n                }\n\n                # NOTE:\n                # If all the raw data needs to be sent to the javascript, and the trace\n                # is high-frequency, this would take significant time!\n                # Hence, you first downsample the trace.\n                trace = self._check_update_trace_data(trace)\n                assert trace is not None\n                return super(AbstractFigureAggregator, self).add_traces(\n                    [trace], **self._add_trace_to_add_traces_kwargs(trace_kwargs)\n                )\n            else:\n                self._print(f\"[i] NOT resampling {trace['name']} - len={n_samples}\")\n                trace._process_kwargs(**{k: getattr(dc, k) for k in dc._fields})\n                return super(AbstractFigureAggregator, self).add_traces(\n                    [trace], **self._add_trace_to_add_traces_kwargs(trace_kwargs)\n                )\n\n        return super().add_traces(\n            [trace], **self._add_trace_to_add_traces_kwargs(trace_kwargs)\n        )\n\n    def add_traces(\n        self,\n        data: List[BaseTraceType | dict] | BaseTraceType | Dict,\n        max_n_samples: None | List[int] | int = None,\n        downsamplers: None | List[AbstractAggregator] | AbstractAggregator = None,\n        gap_handlers: None | List[AbstractGapHandler] | AbstractGapHandler = None,\n        limit_to_views: List[bool] | bool = False,\n        **traces_kwargs,\n    ):\n        \"\"\"Add traces to the figure.\n\n        !!! note\n\n            Make sure to look at the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] function for more info about\n            **speed optimization**, and dealing with not ``high-frequency`` data, but\n            still want to resample / limit the data to the front-end view.\n\n        Parameters\n        ----------\n        data : List[BaseTraceType  |  dict]\n            A list of trace specifications to be added.\n            Trace specifications may be either:\n\n              - Instances of trace classes from the plotly.graph_objs\n                package (e.g plotly.graph_objs.Scatter, plotly.graph_objs.Bar).\n              - Dicts where:\n\n                  - The 'type' property specifies the trace type (e.g.\n                    'scatter', 'bar', 'area', etc.). If the dict has no 'type'\n                    property then 'scatter' is assumed.\n                  - All remaining properties are passed to the constructor\n                    of the specified trace type.\n        max_n_samples : None | List[int] | int, optional\n              The maximum number of samples that will be shown for each trace.\n              If a single integer is passed, all traces will use this number. If this\n              variable is not set; ``_global_n_shown_samples`` will be used.\n        downsamplers : None | List[AbstractAggregator] | AbstractAggregator, optional\n            The downsampler that will be used to aggregate the traces. If a single\n            aggregator is passed, all traces will use this aggregator.\n            If this variable is not set, ``_global_downsampler`` will be used.\n        gap_handlers : None | List[AbstractGapHandler] | AbstractGapHandler, optional\n            The gap handler that will be used to aggregate the traces. If a single\n            gap handler is passed, all traces will use this gap handler.\n            If this variable is not set, ``_global_gap_handler`` will be used.\n        limit_to_views : None | List[bool] | bool, optional\n            List of limit_to_view booleans for the added traces. If set to True the\n            trace's datapoints will be cut to the corresponding front-end view, even if\n            the total number of samples is lower than ``max_n_samples``.\n            If a single boolean is passed, all to be added traces will use this value,\n            by default False.\\n\n            Remark that setting this parameter to True ensures that low frequency traces\n            are added to the ``hf_data`` property.\n        **traces_kwargs: dict\n            Additional trace related keyword arguments.\n            e.g.: rows=.., cols=..., secondary_ys=...\n\n            !!! info \"See Also\"\n\n                [`Figure.add_traces`](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html#plotly.graph_objects.Figure.add_traces&gt;) docs.\n\n        Returns\n        -------\n        BaseFigure\n            The Figure on which ``add_traces`` was called on; i.e. self.\n\n        \"\"\"\n        # note: Plotly its add_traces also a allows non list-like input e.g. a scatter\n        # object; the code below is an exact copy of their internally applied parsing\n        if not isinstance(data, (list, tuple)):\n            data = [data]\n\n        # Convert each trace into a BaseTraceType object\n        data = [\n            (\n                self._data_validator.validate_coerce(trace)[0]\n                if not isinstance(trace, BaseTraceType)\n                else trace\n            )\n            for trace in data\n        ]\n\n        # First add a UUID, as each (even the non-hf_data traces), must contain this\n        # key for comparison. If the trace already has a UUID, we will keep it.\n        for trace in data:\n            uuid_str = str(uuid4()) if trace.uid is None else trace.uid\n            trace.uid = uuid_str\n\n        # Convert the data properties\n        if isinstance(max_n_samples, (int, np.integer)) or max_n_samples is None:\n            max_n_samples = [max_n_samples] * len(data)\n        if isinstance(downsamplers, AbstractAggregator) or downsamplers is None:\n            downsamplers = [downsamplers] * len(data)\n        if isinstance(gap_handlers, AbstractGapHandler) or gap_handlers is None:\n            gap_handlers = [gap_handlers] * len(data)\n        if isinstance(limit_to_views, bool):\n            limit_to_views = [limit_to_views] * len(data)\n\n        zipped = zip(data, max_n_samples, downsamplers, gap_handlers, limit_to_views)\n        for i, (trace, max_out, downsampler, gap_handler, limit_to_view) in enumerate(\n            zipped\n        ):\n            if (\n                trace.type.lower() not in self._high_frequency_traces\n                or self._hf_data.get(trace.uid) is not None\n            ):\n                continue\n\n            max_out_s = self._global_n_shown_samples if max_out is None else max_out\n            if not limit_to_view and (trace.y is None or len(trace.y) &lt;= max_out_s):\n                continue\n\n            dc = self._parse_get_trace_props(trace)\n            self._hf_data[trace.uid] = self._construct_hf_data_dict(\n                dc,\n                trace=trace,\n                downsampler=downsampler,\n                gap_handler=gap_handler,\n                max_n_samples=max_out,\n                offset=i,\n            )\n\n            # convert the trace into a dict, and only withholds the non-hf props\n            trace = trace._props\n            trace = {k: trace[k] for k in set(trace.keys()).difference(set(dc._fields))}\n\n            # update the trace data with the HF props\n            trace = self._check_update_trace_data(trace)\n            assert trace is not None\n            data[i] = trace\n\n        return super().add_traces(data, **traces_kwargs)\n\n    def _clear_figure(self):\n        \"\"\"Clear the current figure object its data and layout.\"\"\"\n        self._hf_data = {}\n        self.data = []\n        self._data = []\n        self._layout = {}\n        self.layout = {}\n\n    def _copy_hf_data(self, hf_data: dict, adjust_default_values: bool = False) -&gt; dict:\n        \"\"\"Copy (i.e. create a new key reference, not a deep copy) of a hf_data dict.\n\n        Parameters\n        ----------\n        hf_data : dict\n            The hf_data dict, having the trace 'uid' as key and the\n            hf-data, together with its aggregation properties as dict-values\n        adjust_default_values: bool\n            Whether the default values (of the downsampler, max # shown samples) will\n            be adjusted according to the values of this object, by default False\n\n        Returns\n        -------\n        dict\n            The copied (&amp; default values adjusted) output dict.\n\n        \"\"\"\n        hf_data_cp = {\n            uid: {k: hf_dict[k] for k in set(hf_dict.keys())}\n            for uid, hf_dict in hf_data.items()\n        }\n\n        # Adjust the default arguments to the current argument values\n        if adjust_default_values:\n            for hf_props in hf_data_cp.values():\n                if hf_props.get(\"default_downsampler\", False):\n                    hf_props[\"downsampler\"] = self._global_downsampler\n                if hf_props.get(\"default_gap_handler\", False):\n                    hf_props[\"gap_handler\"] = self._global_gap_handler\n                if hf_props.get(\"default_n_samples\", False):\n                    hf_props[\"max_n_samples\"] = self._global_n_shown_samples\n\n        return hf_data_cp\n\n    def replace(self, figure: go.Figure, convert_existing_traces: bool = True):\n        \"\"\"Replace the current figure layout with the passed figure object.\n\n        Parameters\n        ----------\n        figure: go.Figure\n            The figure object which will replace the existing figure.\n        convert_existing_traces: bool, Optional\n            A bool indicating whether the traces of the passed ``figure`` should be\n            resampled, by default True.\n\n        \"\"\"\n        self._clear_figure()\n        self.__init__(\n            figure=figure,\n            convert_existing_traces=convert_existing_traces,\n            default_n_shown_samples=self._global_n_shown_samples,\n            default_downsampler=self._global_downsampler,\n            default_gap_handler=self._global_gap_handler,\n            resampled_trace_prefix_suffix=(self._prefix, self._suffix),\n            show_mean_aggregation_size=self._show_mean_aggregation_size,\n            verbose=self._print_verbose,\n        )\n\n    def _parse_relayout(self, relayout_dict: dict) -&gt; dict:\n        \"\"\"Update the relayout object so that the autorange will be set to None when\n        there are xy-matches.\n\n        Parameters\n        ----------\n        relayout_dict : dict\n            The relayout dictionary.\n        \"\"\"\n        # 1. Create a new dict with additional layout updates for the front-end\n        extra_layout_updates = {}\n\n        # 1.1. Set autorange to False for each layout item with a specified x-range\n        xy_matches = self._re_matches(\n            re.compile(r\"[xy]axis\\d*.range\\[\\d+]\"), relayout_dict.keys()\n        )\n        for range_change_axis in xy_matches:\n            axis = range_change_axis.split(\".\")[0]\n            extra_layout_updates[f\"{axis}.autorange\"] = None\n        return extra_layout_updates\n\n    def _construct_update_data(\n        self,\n        relayout_data: dict,\n    ) -&gt; Union[List[dict], None]:\n        \"\"\"Construct the to-be-updated front-end data, based on the layout change.\n\n        Parameters\n        ----------\n        relayout_data: dict\n            A dict containing the ``relayoutData`` (i.e., the changed layout data) of\n            the corresponding front-end graph.\n\n        Returns\n        -------\n        List[dict]:\n            A list of dicts, where each dict-item is a representation of a trace its\n            *data* properties which are affected by the front-end layout change. |br|\n            In other words, only traces which need to be updated will be sent to the\n            front-end. Additionally, each trace-dict withholds the *index* of its\n            corresponding position in the ``figure[data]`` array with the ``index``-key\n            in each dict.\n\n        \"\"\"\n        current_graph = self._get_current_graph()\n        updated_trace_indices, cl_k = [], []\n        if relayout_data:\n            # flatten the possibly nested dict using '.' as separator\n            relayout_data = nested_to_record(relayout_data, sep=\".\")\n            self._print(\"-\" * 100 + \"\\n\", \"changed layout\", relayout_data)\n\n            cl_k = list(relayout_data.keys())\n\n            # ------------------ HF DATA aggregation ---------------------\n            # 1. Base case - there is an x-range specified in the front-end\n            start_matches = self._re_matches(re.compile(r\"xaxis\\d*.range\\[0]\"), cl_k)\n            stop_matches = self._re_matches(re.compile(r\"xaxis\\d*.range\\[1]\"), cl_k)\n\n            # related issue: https://github.com/predict-idlab/plotly-resampler/pull/336\n            # When the user sets x range via update_xaxes and the number of points in\n            # data exceeds the default_n_shown_samples, then after resetting the axes\n            # the relayout may only have \"xaxis.range\", instead of \"xaxis.range[0]\" and\n            # \"xaxis.range[1]\". If this happens, we need to manually add \"xaxis.range[0]\"\n            # and \"xaxis.range[1]\", otherwise resetting axes wouldn't work.\n            if not (start_matches and stop_matches):\n                range_matches = self._re_matches(re.compile(r\"xaxis\\d*.range\"), cl_k)\n                for range_match in range_matches:\n                    x_range = relayout_data[range_match]\n                    start, stop = x_range\n                    start_match = range_match + \"[0]\"\n                    stop_match = range_match + \"[1]\"\n                    relayout_data[start_match] = start\n                    relayout_data[stop_match] = stop\n                    start_matches.append(start_match)\n                    stop_matches.append(stop_match)\n                    del x_range, start, stop, start_match, stop_match\n            if start_matches and stop_matches:  # when both are not empty\n                for t_start_key, t_stop_key in zip(start_matches, stop_matches):\n                    # Check if the xaxis&lt;NUMB&gt; part of xaxis&lt;NUMB&gt;.[0-1] matches\n                    xaxis = t_start_key.split(\".\")[0]\n                    assert xaxis == t_stop_key.split(\".\")[0]\n                    # -&gt; we want to copy the layout on the back-end\n                    updated_trace_indices = self._check_update_figure_dict(\n                        current_graph,\n                        start=relayout_data[t_start_key],\n                        stop=relayout_data[t_stop_key],\n                        layout_xaxis_filter=xaxis,\n                        updated_trace_indices=updated_trace_indices,\n                    )\n\n            # 2. The user clicked on either autorange | reset axes\n            autorange_matches = self._re_matches(\n                re.compile(r\"xaxis\\d*.autorange\"), cl_k\n            )\n            spike_matches = self._re_matches(re.compile(r\"xaxis\\d*.showspikes\"), cl_k)\n            # 2.1 Reset-axes -&gt; autorange &amp; reset to the global data view\n            if autorange_matches and spike_matches:  # when both are not empty\n                for autorange_key in autorange_matches:\n                    if relayout_data[autorange_key]:\n                        xaxis = autorange_key.split(\".\")[0]\n                        updated_trace_indices = self._check_update_figure_dict(\n                            current_graph,\n                            layout_xaxis_filter=xaxis,\n                            updated_trace_indices=updated_trace_indices,\n                        )\n            # 2.1. Autorange -&gt; do nothing, the autorange will be applied on the\n            #      current front-end view\n            elif (\n                autorange_matches and not spike_matches\n            ):  # when only autorange is not empty\n                # PreventUpdate returns a 204 status code response on the\n                # relayout post request\n                return dash.no_update\n\n        # If we do not have any traces to be updated, we will return an empty\n        # request response\n        if not updated_trace_indices:  # when updated_trace_indices is empty\n            # PreventUpdate returns a 204 status-code response on the relayout post\n            # request\n            return dash.no_update\n\n        # -------------------- construct callback data --------------------------\n        # 1. Create the layout data for the front-end\n        layout_traces_list: List[dict] = [relayout_data]\n\n        # 2. Create the additional trace data for the frond-end\n        relevant_keys = list(_hf_data_container._fields) + [\"name\", \"marker\"]\n        # Note that only updated trace-data will be sent to the client\n        for idx in updated_trace_indices:\n            trace = current_graph[\"data\"][idx]\n            # TODO: check if we can reduce even more\n            trace_reduced = {k: trace[k] for k in relevant_keys if k in trace}\n\n            # Store the index into the corresponding to-be-sent trace-data so\n            # the client front-end can know which trace needs to be updated\n            trace_reduced.update({\"index\": idx})\n\n            layout_traces_list.append(trace_reduced)\n        return layout_traces_list\n\n    @staticmethod\n    def _re_matches(regex: re.Pattern, strings: Iterable[str]) -&gt; List[str]:\n        \"\"\"Returns all the items in ``strings`` which regex.match(es) ``regex``.\"\"\"\n        matches = []\n        for item in strings:\n            m = regex.match(item)\n            if m is not None:\n                matches.append(m.string)\n        return sorted(matches)\n\n    @staticmethod\n    def _is_no_update(update_data: Union[List[dict], dash.no_update]) -&gt; bool:\n        return update_data is dash.no_update\n\n    # ------------------------------- Magic methods ---------------------------------\n\n    def _get_pr_props_keys(self) -&gt; List[str]:\n        \"\"\"Returns the keys (i.e., the names) of the plotly-resampler properties.\n\n        Note\n        ----\n        This method is used to serialize the object in the `__reduce__` method.\n\n        \"\"\"\n        return [\n            \"_hf_data\",\n            \"_global_n_shown_samples\",\n            \"_print_verbose\",\n            \"_show_mean_aggregation_size\",\n            \"_prefix\",\n            \"_suffix\",\n            \"_global_downsampler\",\n            \"_global_gap_handler\",\n        ]\n\n    def __reduce__(self):\n        \"\"\"Overwrite the reduce method (which is used to support deep copying and\n        pickling).\n\n        Note\n        ----\n        We do not overwrite the `to_dict` method, as this is used to send the figure\n        to the frontend (and thus should not capture the plotly-resampler properties).\n        \"\"\"\n        _, props = super().__reduce__()\n        assert len(props) == 1  # I don't know why this would be &gt; 1\n        props = props[0]\n\n        # Add the plotly-resampler properties\n        props[\"pr_props\"] = {}\n        for k in self._get_pr_props_keys():\n            props[\"pr_props\"][k] = getattr(self, k)\n        return self.__class__, (props,)  # (props,) to comply with plotly magic\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator.hf_data","title":"<code>hf_data</code>  <code>property</code>","text":"<p>Property to adjust the <code>data</code> component of the current graph</p> <p>Note</p> <p>The user has full responsibility to adjust <code>hf_data</code> properly.</p> Example <pre><code>    &gt;&gt;&gt; from plotly_resampler import FigureResampler\n    &gt;&gt;&gt; fig = FigureResampler(go.Figure())\n    &gt;&gt;&gt; fig.add_trace(...)\n    &gt;&gt;&gt; # Adjust the y property of the above added trace\n    &gt;&gt;&gt; fig.hf_data[-1][\"y\"] = - s ** 2\n    &gt;&gt;&gt; fig.hf_data\n    [\n        {\n            'max_n_samples': 1000,\n            'x': RangeIndex(start=0, stop=11000000, step=1),\n            'y': array([-0.01339909,  0.01390696,, ...,  0.25051913, 0.55876513]),\n            'axis_type': 'linear',\n            'downsampler': &lt;plotly_resampler.aggregation.aggregators.LTTB at 0x7f786d5a9ca0&gt;,\n            'text': None,\n            'hovertext': None\n        },\n    ]\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator.__init__","title":"<code>__init__(figure, convert_existing_traces=True, default_n_shown_samples=1000, default_downsampler=MinMaxLTTB(), default_gap_handler=MedDiffGapHandler(), resampled_trace_prefix_suffix=('&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ', ''), show_mean_aggregation_size=True, convert_traces_kwargs=None, verbose=False)</code>","text":"<p>Instantiate a resampling data mirror.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>BaseFigure</code> <p>The figure that will be decorated. Can be either an empty figure (e.g., <code>go.Figure()</code>, <code>make_subplots()</code>, <code>go.FigureWidget</code>) or an existing figure.</p> required <code>convert_existing_traces</code> <code>bool</code> <p>A bool indicating whether the high-frequency traces of the passed <code>figure</code> should be resampled, by default True. Hence, when set to False, the high-frequency traces of the passed <code>figure</code> will not be resampled.</p> <code>True</code> <code>default_n_shown_samples</code> <code>int</code> <p>The default number of samples that will be shown for each trace, by default 1000.</p> <p>Note</p> <ul> <li>This can be overridden within the <code>add_trace</code> method.</li> <li>If a trace withholds fewer datapoints than this parameter,   the data will not be aggregated.</li> </ul> <code>1000</code> <code>default_downsampler</code> <code>AbstractAggregator</code> <p>An instance which implements the AbstractSeriesDownsampler interface and will be used as default downsampler, by default <code>MinMaxLTTB</code>. </p> <p>Note</p> <p>This can be overridden within the <code>add_trace</code> method.</p> <code>MinMaxLTTB()</code> <code>default_gap_handler</code> <code>AbstractGapHandler</code> <p>An instance which implements the AbstractGapHandler interface and will be used as default gap handler, by default <code>MedDiffGapHandler</code>. </p> <p>Note</p> <p>This can be overridden within the <code>add_trace</code> method.</p> <code>MedDiffGapHandler()</code> <code>resampled_trace_prefix_suffix</code> <code>Tuple[str, str]</code> <p>A tuple which contains the <code>prefix</code> and <code>suffix</code>, respectively, which will be added to the trace its legend-name when a resampled version of the trace is shown. By default, a bold, orange <code>[R]</code> is shown as prefix (no suffix is shown).</p> <code>('&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ', '')</code> <code>show_mean_aggregation_size</code> <code>bool</code> <p>Whether the mean aggregation bin size will be added as a suffix to the trace its legend-name, by default True.</p> <code>True</code> <code>convert_traces_kwargs</code> <code>dict | None</code> <p>A dict of kwargs that will be passed to the <code>add_traces</code> method and will be used to convert the existing traces. </p> <p>Note</p> <p>This argument is only used when the passed <code>figure</code> contains data and <code>convert_existing_traces</code> is set to True.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether some verbose messages will be printed or not, by default False.</p> <code>False</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler_interface.py</code> <pre><code>def __init__(\n    self,\n    figure: BaseFigure,\n    convert_existing_traces: bool = True,\n    default_n_shown_samples: int = 1000,\n    default_downsampler: AbstractAggregator = MinMaxLTTB(),\n    default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n    resampled_trace_prefix_suffix: Tuple[str, str] = (\n        '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n        \"\",\n    ),\n    show_mean_aggregation_size: bool = True,\n    convert_traces_kwargs: dict | None = None,\n    verbose: bool = False,\n):\n    \"\"\"Instantiate a resampling data mirror.\n\n    Parameters\n    ----------\n    figure: BaseFigure\n        The figure that will be decorated. Can be either an empty figure\n        (e.g., ``go.Figure()``, ``make_subplots()``, ``go.FigureWidget``) or an\n        existing figure.\n    convert_existing_traces: bool\n        A bool indicating whether the high-frequency traces of the passed ``figure``\n        should be resampled, by default True. Hence, when set to False, the\n        high-frequency traces of the passed ``figure`` will not be resampled.\n    default_n_shown_samples: int, optional\n        The default number of samples that will be shown for each trace,\n        by default 1000.\\n\n        !!! note\n            * This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n            * If a trace withholds fewer datapoints than this parameter,\n              the data will *not* be aggregated.\n    default_downsampler: AbstractAggregator\n        An instance which implements the AbstractSeriesDownsampler interface and\n        will be used as default downsampler, by default ``MinMaxLTTB``. \\n\n        !!! note\n            This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n    default_gap_handler: GapHandler\n        An instance which implements the AbstractGapHandler interface and will be\n        used as default gap handler, by default ``MedDiffGapHandler``. \\n\n        !!! note\n            This can be overridden within the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method.\n    resampled_trace_prefix_suffix: str, optional\n        A tuple which contains the ``prefix`` and ``suffix``, respectively, which\n        will be added to the trace its legend-name when a resampled version of the\n        trace is shown. By default, a bold, orange ``[R]`` is shown as prefix\n        (no suffix is shown).\n    show_mean_aggregation_size: bool, optional\n        Whether the mean aggregation bin size will be added as a suffix to the trace\n        its legend-name, by default True.\n    convert_traces_kwargs: dict, optional\n        A dict of kwargs that will be passed to the [`add_traces`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_traces] method and\n        will be used to convert the existing traces. \\n\n        !!! note\n            This argument is only used when the passed ``figure`` contains data and\n            ``convert_existing_traces`` is set to True.\n    verbose: bool, optional\n        Whether some verbose messages will be printed or not, by default False.\n\n    \"\"\"\n    self._hf_data: Dict[str, dict] = {}\n    self._global_n_shown_samples = default_n_shown_samples\n    self._print_verbose = verbose\n    self._show_mean_aggregation_size = show_mean_aggregation_size\n\n    assert len(resampled_trace_prefix_suffix) == 2\n    self._prefix, self._suffix = resampled_trace_prefix_suffix\n\n    self._global_downsampler = default_downsampler\n    self._global_gap_handler = default_gap_handler\n\n    # Given figure should always be a BaseFigure that is not wrapped by\n    # a plotly-resampler class\n    assert isinstance(figure, BaseFigure)\n    assert not issubclass(type(figure), AbstractFigureAggregator)\n    self._figure_class = figure.__class__\n\n    # Overwrite the passed arguments with the property dict values\n    # (this is the case when the PR figure is created from a pickled object)\n    if hasattr(figure, \"_pr_props\"):\n        pr_props = figure._pr_props  # a dict of PR properties\n        if pr_props is not None:\n            # Overwrite the default arguments with the serialized properties\n            for k, v in pr_props.items():\n                setattr(self, k, v)\n        delattr(figure, \"_pr_props\")  # should not be stored anymore\n\n    if convert_existing_traces:\n        # call __init__ with the correct layout and set the `_grid_ref` of the\n        # to-be-converted figure\n        f_ = self._figure_class(layout=figure.layout)\n        f_._grid_str = figure._grid_str\n        f_._grid_ref = figure._grid_ref\n        super().__init__(f_)\n\n        if convert_traces_kwargs is None:\n            convert_traces_kwargs = {}\n\n        # make sure that the UIDs of these traces do not get adjusted\n        self._data_validator.set_uid = False\n        self.add_traces(figure.data, **convert_traces_kwargs)\n    else:\n        super().__init__(figure)\n        self._data_validator.set_uid = False\n\n    # A list of al xaxis and yaxis string names\n    # e.g., \"xaxis\", \"xaxis2\", \"xaxis3\", .... for _xaxis_list\n    self._xaxis_list = self._re_matches(\n        re.compile(r\"xaxis\\d*\"), self._layout.keys()\n    )\n    self._yaxis_list = self._re_matches(\n        re.compile(r\"yaxis\\d*\"), self._layout.keys()\n    )\n    # edge case: an empty `go.Figure()` does not yet contain axes keys\n    if not len(self._xaxis_list):\n        self._xaxis_list = [\"xaxis\"]\n        self._yaxis_list = [\"yaxis\"]\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator.__reduce__","title":"<code>__reduce__()</code>","text":"<p>Overwrite the reduce method (which is used to support deep copying and pickling).</p> Note <p>We do not overwrite the <code>to_dict</code> method, as this is used to send the figure to the frontend (and thus should not capture the plotly-resampler properties).</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler_interface.py</code> <pre><code>def __reduce__(self):\n    \"\"\"Overwrite the reduce method (which is used to support deep copying and\n    pickling).\n\n    Note\n    ----\n    We do not overwrite the `to_dict` method, as this is used to send the figure\n    to the frontend (and thus should not capture the plotly-resampler properties).\n    \"\"\"\n    _, props = super().__reduce__()\n    assert len(props) == 1  # I don't know why this would be &gt; 1\n    props = props[0]\n\n    # Add the plotly-resampler properties\n    props[\"pr_props\"] = {}\n    for k in self._get_pr_props_keys():\n        props[\"pr_props\"][k] = getattr(self, k)\n    return self.__class__, (props,)  # (props,) to comply with plotly magic\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace","title":"<code>add_trace(trace, max_n_samples=None, downsampler=None, gap_handler=None, limit_to_view=False, hf_x=None, hf_y=None, hf_text=None, hf_hovertext=None, hf_marker_size=None, hf_marker_color=None, **trace_kwargs)</code>","text":"<p>Add a trace to the figure.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>BaseTraceType or dict</code> <p>Either:</p> <ul> <li>An instances of a trace class from the <code>plotly.graph_objects</code> (go)     package (e.g., <code>go.Scatter</code>, <code>go.Bar</code>)</li> <li> <p>or a dict where:</p> <ul> <li>The type property specifies the trace type (e.g. scatter, bar,   area, etc.). If the dict has no \u2018type\u2019 property then scatter is   assumed.</li> <li>All remaining properties are passed to the constructor   of the specified trace type.</li> </ul> </li> </ul> required <code>max_n_samples</code> <code>int</code> <p>The maximum number of samples that will be shown by the trace.</p> <p>Note</p> <p>If this variable is not set; <code>_global_n_shown_samples</code> will be used.</p> <code>None</code> <code>downsampler</code> <code>AbstractAggregator</code> <p>The abstract series downsampler method.</p> <p>Note</p> <p>If this variable is not set, <code>_global_downsampler</code> will be used.</p> <code>None</code> <code>gap_handler</code> <code>AbstractGapHandler</code> <p>The abstract series gap handler method.</p> <p>Note</p> <p>If this variable is not set, <code>_global_gap_handler</code> will be used.</p> <code>None</code> <code>limit_to_view</code> <code>bool</code> <p>If set to True, the trace\u2019s datapoints will be cut to the corresponding front-end view, even if the total number of samples is lower than <code>max_n_samples</code>, By default False.</p> <p>Remark that setting this parameter to True ensures that low frequency traces are added to the <code>hf_data</code> property.</p> <code>False</code> <code>hf_x</code> <code>Iterable</code> <p>The original high frequency series positions, can be either a time-series or an increasing, numerical index. If set, this has priority over the trace its data.</p> <code>None</code> <code>hf_y</code> <code>Iterable</code> <p>The original high frequency values. If set, this has priority over the trace its data.</p> <code>None</code> <code>hf_text</code> <code>Union[str, Iterable]</code> <p>The original high frequency text. If set, this has priority over the trace its <code>text</code> argument.</p> <code>None</code> <code>hf_hovertext</code> <code>Union[str, Iterable]</code> <p>The original high frequency hovertext. If set, this has priority over the trace its <code>`hovertext</code> argument.</p> <code>None</code> <code>hf_marker_size</code> <code>Union[str, Iterable]</code> <p>The original high frequency marker size. If set, this has priority over the trace its <code>marker.size</code> argument.</p> <code>None</code> <code>hf_marker_color</code> <code>Union[str, Iterable]</code> <p>The original high frequency marker color. If set, this has priority over the trace its <code>marker.color</code> argument.</p> <code>None</code> <code>**trace_kwargs</code> <p>Additional trace related keyword arguments. e.g.: row=.., col=\u2026, secondary_y=\u2026</p> <p>See Also</p> <p><code>Figure.add_trace</code> docs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseFigure</code> <p>The Figure on which <code>add_trace</code> was called on; i.e. self.</p> <code>!!! note</code> <p>Constructing traces with very large data amounts really takes some time. To speed this up; use this <code>add_trace</code> method and</p> <ol> <li>Create a trace with no data (empty lists)</li> <li>pass the high frequency data to this method using the <code>hf_x</code> and <code>hf_y</code>    parameters.</li> </ol> <p>See the example below:     <pre><code>&gt;&gt;&gt; from plotly.subplots import make_subplots\n&gt;&gt;&gt; s = pd.Series()  # a high-frequency series, with more than 1e7 samples\n&gt;&gt;&gt; fig = FigureResampler(go.Figure())\n&gt;&gt;&gt; fig.add_trace(go.Scattergl(x=[], y=[], ...), hf_x=s.index, hf_y=s)\n</code></pre></p> <p>Todo</p> <ul> <li>explain why adding x and y to a trace is so slow</li> <li>check and simplify the example above</li> </ul> <code>!!! tip</code> <ul> <li>If you do not want to downsample your data, set <code>max_n_samples</code> to the   the number of datapoints of your trace!</li> </ul> <code>!!! warning</code> <ul> <li>The <code>NaN</code> values in either <code>hf_y</code> or <code>trace.y</code> will be omitted! We do   not allow <code>NaN</code> values in <code>hf_x</code> or <code>trace.x</code>.</li> <li><code>hf_x</code>, <code>hf_y</code>, <code>hf_text</code>, and <code>hf_hovertext</code> are useful when you deal   with large amounts of data (as it can increase the speed of this add_trace()   method with ~30%). These arguments have priority over the trace\u2019s data and   (hover)text attributes.</li> <li>Low-frequency time-series data, i.e. traces that are not resampled, can hinder   the the automatic-zooming (y-scaling) as these will not be stored in the   back-end and thus not be scaled to the view.   To circumvent this, the <code>limit_to_view</code> argument can be set, resulting in   also storing the low-frequency series in the back-end.</li> </ul> Source code in <code>plotly_resampler/figure_resampler/figure_resampler_interface.py</code> <pre><code>def add_trace(\n    self,\n    trace: Union[BaseTraceType, dict],\n    max_n_samples: int = None,\n    downsampler: AbstractAggregator = None,\n    gap_handler: AbstractGapHandler = None,\n    limit_to_view: bool = False,\n    # Use these if you want some speedups (and are working with really large data)\n    hf_x: Iterable = None,\n    hf_y: Iterable = None,\n    hf_text: Union[str, Iterable] = None,\n    hf_hovertext: Union[str, Iterable] = None,\n    hf_marker_size: Union[str, Iterable] = None,\n    hf_marker_color: Union[str, Iterable] = None,\n    **trace_kwargs,\n):\n    \"\"\"Add a trace to the figure.\n\n    Parameters\n    ----------\n    trace : BaseTraceType or dict\n        Either:\n\n          - An instances of a trace class from the ``plotly.graph_objects`` (go)\n            package (e.g., ``go.Scatter``, ``go.Bar``)\n          - or a dict where:\n\n            - The type property specifies the trace type (e.g. scatter, bar,\n              area, etc.). If the dict has no 'type' property then scatter is\n              assumed.\n            - All remaining properties are passed to the constructor\n              of the specified trace type.\n    max_n_samples : int, optional\n        The maximum number of samples that will be shown by the trace.\\n\n        !!! note\n            If this variable is not set; ``_global_n_shown_samples`` will be used.\n    downsampler: AbstractAggregator, optional\n        The abstract series downsampler method.\\n\n        !!! note\n            If this variable is not set, ``_global_downsampler`` will be used.\n    gap_handler: AbstractGapHandler, optional\n        The abstract series gap handler method.\\n\n        !!! note\n            If this variable is not set, ``_global_gap_handler`` will be used.\n    limit_to_view: boolean, optional\n        If set to True, the trace's datapoints will be cut to the corresponding\n        front-end view, even if the total number of samples is lower than\n        ``max_n_samples``, By default False.\\n\n        Remark that setting this parameter to True ensures that low frequency traces\n        are added to the ``hf_data`` property.\n    hf_x: Iterable, optional\n        The original high frequency series positions, can be either a time-series or\n        an increasing, numerical index. If set, this has priority over the trace its\n        data.\n    hf_y: Iterable, optional\n        The original high frequency values. If set, this has priority over the\n        trace its data.\n    hf_text: Iterable, optional\n        The original high frequency text. If set, this has priority over the trace\n        its ``text`` argument.\n    hf_hovertext: Iterable, optional\n        The original high frequency hovertext. If set, this has priority over the\n        trace its ```hovertext`` argument.\n    hf_marker_size: Iterable, optional\n        The original high frequency marker size. If set, this has priority over the\n        trace its ``marker.size`` argument.\n    hf_marker_color: Iterable, optional\n        The original high frequency marker color. If set, this has priority over the\n        trace its ``marker.color`` argument.\n    **trace_kwargs: dict\n        Additional trace related keyword arguments.\n        e.g.: row=.., col=..., secondary_y=...\n\n        !!! info \"See Also\"\n            [`Figure.add_trace`](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html#plotly.graph_objects.Figure.add_trace&gt;) docs.\n\n    Returns\n    -------\n    BaseFigure\n        The Figure on which ``add_trace`` was called on; i.e. self.\n\n    !!! note\n\n        Constructing traces with **very large data amounts** really takes some time.\n        To speed this up; use this [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] method and\n\n        1. Create a trace with no data (empty lists)\n        2. pass the high frequency data to this method using the ``hf_x`` and ``hf_y``\n           parameters.\n\n        See the example below:\n            ```python\n            &gt;&gt;&gt; from plotly.subplots import make_subplots\n            &gt;&gt;&gt; s = pd.Series()  # a high-frequency series, with more than 1e7 samples\n            &gt;&gt;&gt; fig = FigureResampler(go.Figure())\n            &gt;&gt;&gt; fig.add_trace(go.Scattergl(x=[], y=[], ...), hf_x=s.index, hf_y=s)\n            ```\n\n        !!! todo\n            * explain why adding x and y to a trace is so slow\n            * check and simplify the example above\n\n    !!! tip\n\n        * If you **do not want to downsample** your data, set ``max_n_samples`` to the\n          the number of datapoints of your trace!\n\n    !!! warning\n\n        * The ``NaN`` values in either ``hf_y`` or ``trace.y`` will be omitted! We do\n          not allow ``NaN`` values in ``hf_x`` or ``trace.x``.\n        * ``hf_x``, ``hf_y``, ``hf_text``, and ``hf_hovertext`` are useful when you deal\n          with large amounts of data (as it can increase the speed of this add_trace()\n          method with ~30%). These arguments have priority over the trace's data and\n          (hover)text attributes.\n        * Low-frequency time-series data, i.e. traces that are not resampled, can hinder\n          the the automatic-zooming (y-scaling) as these will not be stored in the\n          back-end and thus not be scaled to the view.\n          To circumvent this, the ``limit_to_view`` argument can be set, resulting in\n          also storing the low-frequency series in the back-end.\n\n    \"\"\"\n    # to comply with the plotly data input acceptance behavior\n    if isinstance(trace, (list, tuple)):\n        raise ValueError(\"Trace must be either a dict or a BaseTraceType\")\n\n    max_out_s = (\n        self._global_n_shown_samples if max_n_samples is None else max_n_samples\n    )\n\n    # Validate the trace and convert to a trace object\n    if not isinstance(trace, BaseTraceType):\n        trace = self._data_validator.validate_coerce(trace)[0]\n\n    # First add a UUID, as each (even the non-hf_data traces), must contain this\n    # key for comparison. If the trace already has a UUID, we will keep it.\n    uuid_str = str(uuid4()) if trace.uid is None else trace.uid\n    trace.uid = uuid_str\n\n    # These traces will determine the autoscale its RANGE!\n    #   -&gt; so also store when `limit_to_view` is set.\n    if trace[\"type\"].lower() in self._high_frequency_traces:\n        # construct the hf_data_container\n        # TODO in future version -&gt; maybe regex on kwargs which start with `hf_`\n        dc = self._parse_get_trace_props(\n            trace,\n            hf_x,\n            hf_y,\n            hf_text,\n            hf_hovertext,\n            hf_marker_size,\n            hf_marker_color,\n        )\n\n        n_samples = len(dc.x)\n        if n_samples &gt; max_out_s or limit_to_view:\n            self._print(\n                f\"\\t[i] DOWNSAMPLE {trace['name']}\\t{n_samples}-&gt;{max_out_s}\"\n            )\n\n            self._hf_data[uuid_str] = self._construct_hf_data_dict(\n                dc,\n                trace=trace,\n                downsampler=downsampler,\n                gap_handler=gap_handler,\n                max_n_samples=max_n_samples,\n            )\n\n            # Before we update the trace, we create a new pointer to that trace in\n            # which the downsampled data will be stored. This way, the original\n            # data of the trace to this `add_trace` method will not be altered.\n            # We copy (by reference) all the non-data properties of the trace in\n            # the new trace.\n            trace = trace._props  # convert the trace into a dict\n            # NOTE: there is no need to store `marker` property here.\n            # If needed, it will be added  to `trace` via `check_update_trace_data`\n            trace = {\n                k: trace[k] for k in set(trace.keys()).difference(set(dc._fields))\n            }\n\n            # NOTE:\n            # If all the raw data needs to be sent to the javascript, and the trace\n            # is high-frequency, this would take significant time!\n            # Hence, you first downsample the trace.\n            trace = self._check_update_trace_data(trace)\n            assert trace is not None\n            return super(AbstractFigureAggregator, self).add_traces(\n                [trace], **self._add_trace_to_add_traces_kwargs(trace_kwargs)\n            )\n        else:\n            self._print(f\"[i] NOT resampling {trace['name']} - len={n_samples}\")\n            trace._process_kwargs(**{k: getattr(dc, k) for k in dc._fields})\n            return super(AbstractFigureAggregator, self).add_traces(\n                [trace], **self._add_trace_to_add_traces_kwargs(trace_kwargs)\n            )\n\n    return super().add_traces(\n        [trace], **self._add_trace_to_add_traces_kwargs(trace_kwargs)\n    )\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_traces","title":"<code>add_traces(data, max_n_samples=None, downsamplers=None, gap_handlers=None, limit_to_views=False, **traces_kwargs)</code>","text":"<p>Add traces to the figure.</p> <p>Note</p> <p>Make sure to look at the <code>add_trace</code> function for more info about speed optimization, and dealing with not <code>high-frequency</code> data, but still want to resample / limit the data to the front-end view.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[BaseTraceType | dict]</code> <p>A list of trace specifications to be added. Trace specifications may be either:</p> <ul> <li>Instances of trace classes from the plotly.graph_objs     package (e.g plotly.graph_objs.Scatter, plotly.graph_objs.Bar).</li> <li> <p>Dicts where:</p> <ul> <li>The \u2018type\u2019 property specifies the trace type (e.g.     \u2018scatter\u2019, \u2018bar\u2019, \u2018area\u2019, etc.). If the dict has no \u2018type\u2019     property then \u2018scatter\u2019 is assumed.</li> <li>All remaining properties are passed to the constructor     of the specified trace type.</li> </ul> </li> </ul> required <code>max_n_samples</code> <code>None | List[int] | int</code> <p>The maximum number of samples that will be shown for each trace.   If a single integer is passed, all traces will use this number. If this   variable is not set; <code>_global_n_shown_samples</code> will be used.</p> <code>None</code> <code>downsamplers</code> <code>None | List[AbstractAggregator] | AbstractAggregator</code> <p>The downsampler that will be used to aggregate the traces. If a single aggregator is passed, all traces will use this aggregator. If this variable is not set, <code>_global_downsampler</code> will be used.</p> <code>None</code> <code>gap_handlers</code> <code>None | List[AbstractGapHandler] | AbstractGapHandler</code> <p>The gap handler that will be used to aggregate the traces. If a single gap handler is passed, all traces will use this gap handler. If this variable is not set, <code>_global_gap_handler</code> will be used.</p> <code>None</code> <code>limit_to_views</code> <code>None | List[bool] | bool</code> <p>List of limit_to_view booleans for the added traces. If set to True the trace\u2019s datapoints will be cut to the corresponding front-end view, even if the total number of samples is lower than <code>max_n_samples</code>. If a single boolean is passed, all to be added traces will use this value, by default False.</p> <p>Remark that setting this parameter to True ensures that low frequency traces are added to the <code>hf_data</code> property.</p> <code>False</code> <code>**traces_kwargs</code> <p>Additional trace related keyword arguments. e.g.: rows=.., cols=\u2026, secondary_ys=\u2026</p> <p>See Also</p> <p><code>Figure.add_traces</code> docs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseFigure</code> <p>The Figure on which <code>add_traces</code> was called on; i.e. self.</p> Source code in <code>plotly_resampler/figure_resampler/figure_resampler_interface.py</code> <pre><code>def add_traces(\n    self,\n    data: List[BaseTraceType | dict] | BaseTraceType | Dict,\n    max_n_samples: None | List[int] | int = None,\n    downsamplers: None | List[AbstractAggregator] | AbstractAggregator = None,\n    gap_handlers: None | List[AbstractGapHandler] | AbstractGapHandler = None,\n    limit_to_views: List[bool] | bool = False,\n    **traces_kwargs,\n):\n    \"\"\"Add traces to the figure.\n\n    !!! note\n\n        Make sure to look at the [`add_trace`][figure_resampler.figure_resampler_interface.AbstractFigureAggregator.add_trace] function for more info about\n        **speed optimization**, and dealing with not ``high-frequency`` data, but\n        still want to resample / limit the data to the front-end view.\n\n    Parameters\n    ----------\n    data : List[BaseTraceType  |  dict]\n        A list of trace specifications to be added.\n        Trace specifications may be either:\n\n          - Instances of trace classes from the plotly.graph_objs\n            package (e.g plotly.graph_objs.Scatter, plotly.graph_objs.Bar).\n          - Dicts where:\n\n              - The 'type' property specifies the trace type (e.g.\n                'scatter', 'bar', 'area', etc.). If the dict has no 'type'\n                property then 'scatter' is assumed.\n              - All remaining properties are passed to the constructor\n                of the specified trace type.\n    max_n_samples : None | List[int] | int, optional\n          The maximum number of samples that will be shown for each trace.\n          If a single integer is passed, all traces will use this number. If this\n          variable is not set; ``_global_n_shown_samples`` will be used.\n    downsamplers : None | List[AbstractAggregator] | AbstractAggregator, optional\n        The downsampler that will be used to aggregate the traces. If a single\n        aggregator is passed, all traces will use this aggregator.\n        If this variable is not set, ``_global_downsampler`` will be used.\n    gap_handlers : None | List[AbstractGapHandler] | AbstractGapHandler, optional\n        The gap handler that will be used to aggregate the traces. If a single\n        gap handler is passed, all traces will use this gap handler.\n        If this variable is not set, ``_global_gap_handler`` will be used.\n    limit_to_views : None | List[bool] | bool, optional\n        List of limit_to_view booleans for the added traces. If set to True the\n        trace's datapoints will be cut to the corresponding front-end view, even if\n        the total number of samples is lower than ``max_n_samples``.\n        If a single boolean is passed, all to be added traces will use this value,\n        by default False.\\n\n        Remark that setting this parameter to True ensures that low frequency traces\n        are added to the ``hf_data`` property.\n    **traces_kwargs: dict\n        Additional trace related keyword arguments.\n        e.g.: rows=.., cols=..., secondary_ys=...\n\n        !!! info \"See Also\"\n\n            [`Figure.add_traces`](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html#plotly.graph_objects.Figure.add_traces&gt;) docs.\n\n    Returns\n    -------\n    BaseFigure\n        The Figure on which ``add_traces`` was called on; i.e. self.\n\n    \"\"\"\n    # note: Plotly its add_traces also a allows non list-like input e.g. a scatter\n    # object; the code below is an exact copy of their internally applied parsing\n    if not isinstance(data, (list, tuple)):\n        data = [data]\n\n    # Convert each trace into a BaseTraceType object\n    data = [\n        (\n            self._data_validator.validate_coerce(trace)[0]\n            if not isinstance(trace, BaseTraceType)\n            else trace\n        )\n        for trace in data\n    ]\n\n    # First add a UUID, as each (even the non-hf_data traces), must contain this\n    # key for comparison. If the trace already has a UUID, we will keep it.\n    for trace in data:\n        uuid_str = str(uuid4()) if trace.uid is None else trace.uid\n        trace.uid = uuid_str\n\n    # Convert the data properties\n    if isinstance(max_n_samples, (int, np.integer)) or max_n_samples is None:\n        max_n_samples = [max_n_samples] * len(data)\n    if isinstance(downsamplers, AbstractAggregator) or downsamplers is None:\n        downsamplers = [downsamplers] * len(data)\n    if isinstance(gap_handlers, AbstractGapHandler) or gap_handlers is None:\n        gap_handlers = [gap_handlers] * len(data)\n    if isinstance(limit_to_views, bool):\n        limit_to_views = [limit_to_views] * len(data)\n\n    zipped = zip(data, max_n_samples, downsamplers, gap_handlers, limit_to_views)\n    for i, (trace, max_out, downsampler, gap_handler, limit_to_view) in enumerate(\n        zipped\n    ):\n        if (\n            trace.type.lower() not in self._high_frequency_traces\n            or self._hf_data.get(trace.uid) is not None\n        ):\n            continue\n\n        max_out_s = self._global_n_shown_samples if max_out is None else max_out\n        if not limit_to_view and (trace.y is None or len(trace.y) &lt;= max_out_s):\n            continue\n\n        dc = self._parse_get_trace_props(trace)\n        self._hf_data[trace.uid] = self._construct_hf_data_dict(\n            dc,\n            trace=trace,\n            downsampler=downsampler,\n            gap_handler=gap_handler,\n            max_n_samples=max_out,\n            offset=i,\n        )\n\n        # convert the trace into a dict, and only withholds the non-hf props\n        trace = trace._props\n        trace = {k: trace[k] for k in set(trace.keys()).difference(set(dc._fields))}\n\n        # update the trace data with the HF props\n        trace = self._check_update_trace_data(trace)\n        assert trace is not None\n        data[i] = trace\n\n    return super().add_traces(data, **traces_kwargs)\n</code></pre>"},{"location":"api/figure_resampler/figure_resampler_interface/#figure_resampler.figure_resampler_interface.AbstractFigureAggregator.replace","title":"<code>replace(figure, convert_existing_traces=True)</code>","text":"<p>Replace the current figure layout with the passed figure object.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Figure</code> <p>The figure object which will replace the existing figure.</p> required <code>convert_existing_traces</code> <code>bool</code> <p>A bool indicating whether the traces of the passed <code>figure</code> should be resampled, by default True.</p> <code>True</code> Source code in <code>plotly_resampler/figure_resampler/figure_resampler_interface.py</code> <pre><code>def replace(self, figure: go.Figure, convert_existing_traces: bool = True):\n    \"\"\"Replace the current figure layout with the passed figure object.\n\n    Parameters\n    ----------\n    figure: go.Figure\n        The figure object which will replace the existing figure.\n    convert_existing_traces: bool, Optional\n        A bool indicating whether the traces of the passed ``figure`` should be\n        resampled, by default True.\n\n    \"\"\"\n    self._clear_figure()\n    self.__init__(\n        figure=figure,\n        convert_existing_traces=convert_existing_traces,\n        default_n_shown_samples=self._global_n_shown_samples,\n        default_downsampler=self._global_downsampler,\n        default_gap_handler=self._global_gap_handler,\n        resampled_trace_prefix_suffix=(self._prefix, self._suffix),\n        show_mean_aggregation_size=self._show_mean_aggregation_size,\n        verbose=self._print_verbose,\n    )\n</code></pre>"},{"location":"api/figure_resampler/figurewidget_resampler/","title":"figurewidget_resampler","text":"<p><code>FigureWidgetResampler</code> wrapper around the plotly <code>go.FigureWidget</code> class.</p> <p>Utilizes the <code>fig.layout.on_change</code> method to enable dynamic resampling.</p>"},{"location":"api/figure_resampler/figurewidget_resampler/#figure_resampler.figurewidget_resampler.FigureWidgetResampler","title":"<code>FigureWidgetResampler</code>","text":"<p>               Bases: <code>AbstractFigureAggregator</code>, <code>FigureWidget</code></p> <p>Data aggregation functionality wrapper for <code>go.FigureWidgets</code>.</p> <p>Warning</p> <ul> <li>This wrapper only works within <code>jupyter</code>-based environments.</li> <li>The <code>.show()</code> method returns a static figure on which the   dynamic resampling cannot be performed. To allow dynamic resampling,   you should just output the <code>FigureWidgetResampler</code> object in a cell.</li> </ul> Source code in <code>plotly_resampler/figure_resampler/figurewidget_resampler.py</code> <pre><code>class FigureWidgetResampler(\n    AbstractFigureAggregator, go.FigureWidget, metaclass=_FigureWidgetResamplerM\n):\n    \"\"\"Data aggregation functionality wrapper for ``go.FigureWidgets``.\n\n    !!! warning\n\n        * This wrapper only works within ``jupyter``-based environments.\n        * The ``.show()`` method returns a **static figure** on which the\n          **dynamic resampling cannot be performed**. To allow dynamic resampling,\n          you should just output the ``FigureWidgetResampler`` object in a cell.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        figure: BaseFigure | dict = None,\n        convert_existing_traces: bool = True,\n        default_n_shown_samples: int = 1000,\n        default_downsampler: AbstractAggregator = MinMaxLTTB(),\n        default_gap_handler: AbstractGapHandler = MedDiffGapHandler(),\n        resampled_trace_prefix_suffix: Tuple[str, str] = (\n            '&lt;b style=\"color:sandybrown\"&gt;[R]&lt;/b&gt; ',\n            \"\",\n        ),\n        show_mean_aggregation_size: bool = True,\n        convert_traces_kwargs: dict | None = None,\n        verbose: bool = False,\n    ):\n        # Parse the figure input before calling `super`\n        f = self._get_figure_class(go.FigureWidget)()\n        f._data_validator.set_uid = False\n\n        if isinstance(figure, BaseFigure):\n            # A base figure object, can be;\n            # - a base plotly figure: go.Figure or go.FigureWidget\n            # - a plotly-resampler figure: subclass of AbstractFigureAggregator\n            # =&gt; we first copy the layout, grid_str and grid ref\n            f.layout = figure.layout\n            f._grid_str = figure._grid_str\n            f._grid_ref = figure._grid_ref\n            f.add_traces(figure.data)\n        elif isinstance(figure, dict) and (\n            \"data\" in figure or \"layout\" in figure  # or \"frames\" in figure  # TODO\n        ):\n            # A figure as a dict, can be;\n            # - a plotly figure as a dict (after calling `fig.to_dict()`)\n            # - a pickled (plotly-resampler) figure (after loading a pickled figure)\n            f.layout = figure.get(\"layout\")\n            f._grid_str = figure.get(\"_grid_str\")\n            f._grid_ref = figure.get(\"_grid_ref\")\n            f.add_traces(figure.get(\"data\"))\n            # `pr_props` is not None when loading a pickled plotly-resampler figure\n            f._pr_props = figure.get(\"pr_props\")\n            # `f._pr_props`` is an attribute to store properties of a plotly-resampler\n            # figure. This attribute is only used to pass information to the super()\n            # constructor. Once the super constructor is called, the attribute is\n            # removed.\n\n            # f.add_frames(figure.get(\"frames\")) TODO\n        elif isinstance(figure, (dict, list)):\n            # A single trace dict or a list of traces\n            f.add_traces(figure)\n\n        super().__init__(\n            f,\n            convert_existing_traces,\n            default_n_shown_samples,\n            default_downsampler,\n            default_gap_handler,\n            resampled_trace_prefix_suffix,\n            show_mean_aggregation_size,\n            convert_traces_kwargs,\n            verbose,\n        )\n\n        if isinstance(figure, AbstractFigureAggregator):\n            # Copy the `_hf_data` if the previous figure was an AbstractFigureAggregator\n            # And adjust the default max_n_samples and\n            self._hf_data.update(\n                self._copy_hf_data(figure._hf_data, adjust_default_values=True)\n            )\n\n            # Note: This hack ensures that the this figure object initially uses\n            # data of the whole view. More concretely; we create a dict\n            # serialization figure and adjust the hf-traces to the whole view\n            # with the check-update method (by passing no range / filter args)\n            with self.batch_update():\n                graph_dict: dict = self._get_current_graph()\n                update_indices = self._check_update_figure_dict(graph_dict)\n                for idx in update_indices:\n                    self.data[idx].update(graph_dict[\"data\"][idx])\n\n        self._prev_layout = None  # Contains the previous xaxis layout configuration\n\n        # used for logging purposes to save a history of layout changes\n        self._relayout_hist = []\n\n        # Assign the the update-methods to the corresponding classes\n        showspike_keys = [f\"{xaxis}.showspikes\" for xaxis in self._xaxis_list]\n        self.layout.on_change(self._update_spike_ranges, *showspike_keys)\n\n        x_relayout_keys = [f\"{xaxis}.range\" for xaxis in self._xaxis_list]\n        self.layout.on_change(self._update_x_ranges, *x_relayout_keys)\n\n    def _update_x_ranges(self, layout, *x_ranges, force_update: bool = False):\n        \"\"\"Update the the go.Figure data based on changed x-ranges.\n\n        Parameters\n        ----------\n        layout : go.Layout\n            The figure's (i.e, self) layout object. Remark that this is a reference,\n            so if we change self.layout (same object reference), this object will\n            change.\n        *x_ranges: iterable\n            A iterable list of current x-ranges, where each x-range is a tuple of two\n            items, indicating the current/new (if changed) left-right x-range,\n            respectively.\n        fore_update: bool\n            Whether an update of all traces will be forced, by default False.\n        \"\"\"\n        relayout_dict = {}  # variable in which we aim to reconstruct the relayout\n        # serialize the layout in a new dict object\n        layout = {\n            xaxis_str: layout[xaxis_str].to_plotly_json()\n            for xaxis_str in self._xaxis_list\n        }\n        if self._prev_layout is None:\n            self._prev_layout = layout\n\n        for xaxis_str, x_range in zip(self._xaxis_list, x_ranges):\n            # We also check whether \"range\" is within the xaxis its layout otherwise\n            # It is most-likely an autorange check\n            if (\n                \"range\" in layout[xaxis_str]\n                and self._prev_layout[xaxis_str].get(\"range\", []) != x_range\n                or (force_update and x_range is not None)\n            ):\n                # a change took place -&gt; add to the relayout dict\n                relayout_dict[f\"{xaxis_str}.range[0]\"] = x_range[0]\n                relayout_dict[f\"{xaxis_str}.range[1]\"] = x_range[1]\n\n                # An update will take place for that trace\n                # -&gt; save current xaxis range to _prev_layout\n                self._prev_layout[xaxis_str][\"range\"] = x_range\n\n        if relayout_dict:  # when not empty\n            # Construct the update data\n            update_data = self._construct_update_data(relayout_dict)\n\n            if self._is_no_update(update_data):\n                # Return when no data update\n                return\n\n            if self._print_verbose:\n                self._relayout_hist.append(dict(zip(self._xaxis_list, x_ranges)))\n                self._relayout_hist.append(layout)\n                self._relayout_hist.append([\"xaxis-range-update\", len(update_data) - 1])\n                self._relayout_hist.append(\"-\" * 30)\n\n            with self.batch_update():\n                # First update the layout (first item of update_data)\n                self.layout.update(self._parse_relayout(update_data[0]))\n\n                for xaxis_str in self._xaxis_list:\n                    if \"showspikes\" in layout[xaxis_str]:\n                        self.layout[xaxis_str].pop(\"showspikes\")\n\n                # Then update the data\n                for updated_trace in update_data[1:]:\n                    trace_idx = updated_trace.pop(\"index\")\n                    self.data[trace_idx].update(updated_trace)\n\n    def _update_spike_ranges(self, layout, *showspikes, force_update=False):\n        \"\"\"Update the go.Figure based on the changed spike-ranges.\n\n        Parameters\n        ----------\n        layout : go.Layout\n            The figure's (i.e, self) layout object. Remark that this is a reference,\n            so if we change self.layout (same object reference), this object will\n            change.\n        *showspikes: iterable\n            A iterable where each item is a bool, indicating  whether showspikes is set\n            to true/false for the corresponding xaxis in ``self._xaxis_list``.\n        force_update: bool\n            Bool indicating whether the range updates need to take place. This is\n            especially useful when you have recently updated the figure its data (with\n            the hf_data property) and want to perform an autoscale, independent from\n            the current figure-layout.\n        \"\"\"\n        relayout_dict = {}  # variable in which we aim to reconstruct the relayout\n        # serialize the layout in a new dict object\n        layout = {\n            xaxis_str: layout[xaxis_str].to_plotly_json()\n            for xaxis_str in self._xaxis_list\n        }\n\n        if self._prev_layout is None:\n            self._prev_layout = layout\n\n        for xaxis_str, showspike in zip(self._xaxis_list, showspikes):\n            if (\n                force_update\n                or\n                # autorange key must be set to True\n                (\n                    layout[xaxis_str].get(\"autorange\", False)\n                    # we only perform updates for traces which have 'range' property,\n                    # as we do need to reconstruct the update-data for these traces\n                    and self._prev_layout[xaxis_str].get(\"range\", None) is not None\n                )\n            ):\n                relayout_dict[f\"{xaxis_str}.autorange\"] = True\n                relayout_dict[f\"{xaxis_str}.showspikes\"] = showspike\n                # autorange -&gt; we pop the xaxis range\n                if \"range\" in layout[xaxis_str]:\n                    del layout[xaxis_str][\"range\"]\n\n        if len(relayout_dict):\n            # An update will take place, save current layout to _prev_layout\n            self._prev_layout = layout\n\n            # Construct the update data\n            update_data = self._construct_update_data(relayout_dict)\n            if not self._is_no_update(update_data):\n                if self._print_verbose:\n                    self._relayout_hist.append(layout)\n                    self._relayout_hist.append(\n                        [\"showspikes-update\", len(update_data) - 1]\n                    )\n                    self._relayout_hist.append(\"-\" * 30)\n\n                with self.batch_update():\n                    # First update the layout (first item of update_data)\n                    if not force_update:\n                        self.layout.update(self._parse_relayout(update_data[0]))\n\n                    # Also: Remove the showspikes from the layout, otherwise the autorange\n                    # will not work as intended (it will not be triggered again)\n                    # Note: this removal causes a second trigger of this method\n                    # which will go in the \"else\" part below.\n                    for xaxis_str in self._xaxis_list:\n                        self.layout[xaxis_str].pop(\"showspikes\")\n\n                    # Then, update the data\n                    for updated_trace in update_data[1:]:\n                        trace_idx = updated_trace.pop(\"index\")\n                        self.data[trace_idx].update(updated_trace)\n        elif self._print_verbose:\n            self._relayout_hist.append([\"showspikes\", \"initial call or showspikes\"])\n            self._relayout_hist.append(\"-\" * 40)\n\n    def reset_axes(self):\n        \"\"\"Reset the axes of the FigureWidgetResampler.\n\n        This is useful when adjusting the `hf_data` properties of the\n        ``FigureWidgetResampler``.\n        \"\"\"\n        self._update_spike_ranges(\n            self.layout, *[False] * len(self._xaxis_list), force_update=True\n        )\n        # Reset the layout\n        self.update_layout(\n            {\n                axis: {\"autorange\": None, \"range\": None}\n                for axis in self._xaxis_list + self._yaxis_list\n            }\n        )\n\n    def reload_data(self):\n        \"\"\"Reload all the data of FigureWidgetResampler for the current range-view.\n\n        This is useful when adjusting the `hf_data` properties of the\n        ``FigureWidgetResampler``.\n        \"\"\"\n        if all(\n            self.layout[xaxis].autorange\n            or (\n                self.layout[xaxis].autorange is None\n                and self.layout[xaxis].range is None\n            )\n            for xaxis in self._xaxis_list\n        ):\n            self._update_spike_ranges(\n                self.layout, *[False] * len(self._xaxis_list), force_update=True\n            )\n        else:\n            # Resample the data for the current range-view\n            self._update_x_ranges(\n                self.layout,\n                # Pass the current view to trigger a resample operation\n                *[self.layout[xaxis_str][\"range\"] for xaxis_str in self._xaxis_list],\n                force_update=True,\n            )\n            # TODO: when we know which traces have changed we can use\n            # a new -&gt; `update_xaxis_str` argument.\n\n    def __reduce__(self):\n        # Needed for pickling\n        # Specifically set the class name, as the metaclass is not easily picklable\n        return FigureWidgetResampler, *list(super().__reduce__())[1:]\n</code></pre>"},{"location":"api/figure_resampler/figurewidget_resampler/#figure_resampler.figurewidget_resampler.FigureWidgetResampler.reload_data","title":"<code>reload_data()</code>","text":"<p>Reload all the data of FigureWidgetResampler for the current range-view.</p> <p>This is useful when adjusting the <code>hf_data</code> properties of the <code>FigureWidgetResampler</code>.</p> Source code in <code>plotly_resampler/figure_resampler/figurewidget_resampler.py</code> <pre><code>def reload_data(self):\n    \"\"\"Reload all the data of FigureWidgetResampler for the current range-view.\n\n    This is useful when adjusting the `hf_data` properties of the\n    ``FigureWidgetResampler``.\n    \"\"\"\n    if all(\n        self.layout[xaxis].autorange\n        or (\n            self.layout[xaxis].autorange is None\n            and self.layout[xaxis].range is None\n        )\n        for xaxis in self._xaxis_list\n    ):\n        self._update_spike_ranges(\n            self.layout, *[False] * len(self._xaxis_list), force_update=True\n        )\n    else:\n        # Resample the data for the current range-view\n        self._update_x_ranges(\n            self.layout,\n            # Pass the current view to trigger a resample operation\n            *[self.layout[xaxis_str][\"range\"] for xaxis_str in self._xaxis_list],\n            force_update=True,\n        )\n</code></pre>"},{"location":"api/figure_resampler/figurewidget_resampler/#figure_resampler.figurewidget_resampler.FigureWidgetResampler.reset_axes","title":"<code>reset_axes()</code>","text":"<p>Reset the axes of the FigureWidgetResampler.</p> <p>This is useful when adjusting the <code>hf_data</code> properties of the <code>FigureWidgetResampler</code>.</p> Source code in <code>plotly_resampler/figure_resampler/figurewidget_resampler.py</code> <pre><code>def reset_axes(self):\n    \"\"\"Reset the axes of the FigureWidgetResampler.\n\n    This is useful when adjusting the `hf_data` properties of the\n    ``FigureWidgetResampler``.\n    \"\"\"\n    self._update_spike_ranges(\n        self.layout, *[False] * len(self._xaxis_list), force_update=True\n    )\n    # Reset the layout\n    self.update_layout(\n        {\n            axis: {\"autorange\": None, \"range\": None}\n            for axis in self._xaxis_list + self._yaxis_list\n        }\n    )\n</code></pre>"},{"location":"api/figure_resampler/jupyter_dash_persistent_inline_output/","title":"jupyter_dash_persistent_inline_output","text":""},{"location":"api/figure_resampler/jupyter_dash_persistent_inline_output/#figure_resampler.jupyter_dash_persistent_inline_output.JupyterDashPersistentInlineOutput","title":"<code>JupyterDashPersistentInlineOutput</code>","text":"<p>Extension of the JupyterDash class to support the custom inline output for <code>FigureResampler</code> figures.</p> <p>Specifically we embed a div in the notebook to display the figure inline.</p> <ul> <li>In this div the figure is shown as an iframe when the server (of the dash app)    is alive.</li> <li>In this div the figure is shown as an image when the server (of the dash app)    is dead.</li> </ul> <p>As the HTML &amp; javascript code is embedded in the notebook output, which is loaded each time you open the notebook, the figure is always displayed (either as iframe or just an image). Hence, this extension enables to maintain always an output in the notebook.</p> <p>.. Note::     This subclass is only used when the mode is set to <code>\"inline_persistent\"</code> in     the :func:<code>FigureResampler.show_dash &lt;plotly_resampler.figure_resampler.FigureResampler.show_dash&gt;</code>     method. However, the mode should be passed as <code>\"inline\"</code> since this subclass     overwrites the inline behavior.</p> <p>.. Note::     This subclass utilizes the optional <code>flask_cors</code> package to detect whether the     server is alive or not.</p> Source code in <code>plotly_resampler/figure_resampler/jupyter_dash_persistent_inline_output.py</code> <pre><code>class JupyterDashPersistentInlineOutput:\n    \"\"\"Extension of the JupyterDash class to support the custom inline output for\n    ``FigureResampler`` figures.\n\n    Specifically we embed a div in the notebook to display the figure inline.\n\n     - In this div the figure is shown as an iframe when the server (of the dash app)\n       is alive.\n     - In this div the figure is shown as an image when the server (of the dash app)\n       is dead.\n\n    As the HTML &amp; javascript code is embedded in the notebook output, which is loaded\n    each time you open the notebook, the figure is always displayed (either as iframe\n    or just an image).\n    Hence, this extension enables to maintain always an output in the notebook.\n\n    .. Note::\n        This subclass is only used when the mode is set to ``\"inline_persistent\"`` in\n        the :func:`FigureResampler.show_dash &lt;plotly_resampler.figure_resampler.FigureResampler.show_dash&gt;`\n        method. However, the mode should be passed as ``\"inline\"`` since this subclass\n        overwrites the inline behavior.\n\n    .. Note::\n        This subclass utilizes the optional ``flask_cors`` package to detect whether the\n        server is alive or not.\n\n    \"\"\"\n\n    def __init__(self, fig: Figure) -&gt; None:\n        super().__init__()\n        self.fig = fig\n\n        # The unique id of this app\n        # This id is used to couple the output in the notebook with this app\n        # A fetch request is performed to the _is_alive_{uid} endpoint to check if the\n        # app is still alive.\n        self.uid = str(uuid.uuid4())\n\n    def _display_inline_output(self, dashboard_url, width, height):\n        \"\"\"Display the dash app persistent inline in the notebook.\n\n        The figure is displayed as an iframe in the notebook if the server is reachable,\n        otherwise as an image.\n        \"\"\"\n        # TODO: check whether an error gets logged in case of crash\n        # TODO: add option to opt out of this\n        from IPython.display import display\n\n        try:\n            import flask_cors  # noqa: F401\n        except (ImportError, ModuleNotFoundError):\n            warnings.warn(\n                \"'flask_cors' is not installed. The persistent inline output will \"\n                + \" not be able to detect whether the server is alive or not.\"\n            )\n\n        # Get the image from the dashboard and encode it as base64\n        fig = self.fig  # is stored in the show_dash method\n        f_width = 1000 if fig.layout.width is None else fig.layout.width\n        fig_base64 = base64.b64encode(\n            fig.to_image(format=\"png\", width=f_width, scale=1, height=fig.layout.height)\n        ).decode(\"utf8\")\n\n        # The html (&amp; javascript) code to display the app / figure\n        uid = self.uid\n        display(\n            {\n                \"text/html\": f\"\"\"\n                &lt;div id='PR_div__{uid}'&gt;&lt;/div&gt;\n                &lt;script type='text/javascript'&gt;\n                \"\"\"\n                + \"\"\"\n\n                function setOutput(timeout) {\n                    \"\"\"\n                +\n                # Variables should be in local scope (in the closure)\n                f\"\"\"\n                    var pr_div = document.getElementById('PR_div__{uid}');\n                    var url = '{dashboard_url}';\n                    var pr_img_src = 'data:image/png;base64, {fig_base64}';\n                    var is_alive_suffix = '_is_alive_{uid}';\n                    \"\"\"\n                + \"\"\"\n\n                    if (pr_div.firstChild) return  // return if already loaded\n\n                    const controller = new AbortController();\n                    const signal = controller.signal;\n\n                    return fetch(url + is_alive_suffix, {method: 'GET', signal: signal})\n                        .then(response =&gt; response.text())\n                        .then(data =&gt;\n                            {\n                                if (data == \"Alive\") {\n                                    console.log(\"Server is alive\");\n                                    iframeOutput(pr_div, url);\n                                } else {\n                                    // I think this case will never occur because of CORS\n                                    console.log(\"Server is dead\");\n                                    imageOutput(pr_div, pr_img_src);\n                                }\n                            }\n                        )\n                        .catch(error =&gt; {\n                            console.log(\"Server is unreachable\");\n                            imageOutput(pr_div, pr_img_src);\n                        })\n                }\n\n                setOutput(350);\n\n                function imageOutput(element, pr_img_src) {\n                    console.log('Setting image');\n                    var pr_img = document.createElement(\"img\");\n                    pr_img.setAttribute(\"src\", pr_img_src)\n                    pr_img.setAttribute(\"alt\", 'Server unreachable - using image instead');\n                    \"\"\"\n                + f\"\"\"\n                    pr_img.setAttribute(\"max-width\", '{width}');\n                    pr_img.setAttribute(\"max-height\", '{height}');\n                    pr_img.setAttribute(\"width\", 'auto');\n                    pr_img.setAttribute(\"height\", 'auto');\n                    \"\"\"\n                + \"\"\"\n                    element.appendChild(pr_img);\n                }\n\n                function iframeOutput(element, url) {\n                    console.log('Setting iframe');\n                    var pr_iframe = document.createElement(\"iframe\");\n                    pr_iframe.setAttribute(\"src\", url);\n                    pr_iframe.setAttribute(\"frameborder\", '0');\n                    pr_iframe.setAttribute(\"allowfullscreen\", '');\n                    \"\"\"\n                + f\"\"\"\n                    pr_iframe.setAttribute(\"width\", '{width}');\n                    pr_iframe.setAttribute(\"height\", '{height}');\n                    \"\"\"\n                + \"\"\"\n                    element.appendChild(pr_iframe);\n                }\n                &lt;/script&gt;\n                \"\"\"\n            },\n            raw=True,\n            clear=True,\n            display_id=uid,\n        )\n\n    # NOTE: Minimally adatped version from dash._jupyter.JupyterDash.run_server\n    def run_app(\n        self,\n        app,\n        width=\"100%\",\n        height=650,\n        host=\"127.0.0.1\",\n        port=8050,\n        server_url=None,\n    ):\n        \"\"\"Run the inline persistent dash app in the notebook.\n\n        Parameters\n        ----------\n        app : dash.Dash\n            A Dash application instance\n        width : str, optional\n            Width of app when displayed using mode=\"inline\", by default \"100%\"\n        height : int, optional\n            Height of app when displayed using mode=\"inline\", by default 650\n        host : str, optional\n            Host of the server, by default \"127.0.0.1\"\n        port : int, optional\n            Port used by the server, by default 8050\n        server_url : str, optional\n            Use if a custom url is required to display the app, by default None\n\n        \"\"\"\n        # Terminate any existing server using this port\n        old_server = JupyterDash._servers.get((host, port))\n        if old_server:\n            old_server.shutdown()\n            del JupyterDash._servers[(host, port)]\n\n        # Configure pathname prefix\n        if \"base_subpath\" in _jupyter_config:\n            requests_pathname_prefix = (\n                _jupyter_config[\"base_subpath\"].rstrip(\"/\") + \"/proxy/{port}/\"\n            )\n        else:\n            requests_pathname_prefix = app.config.get(\"requests_pathname_prefix\", None)\n\n        if requests_pathname_prefix is not None:\n            requests_pathname_prefix = requests_pathname_prefix.format(port=port)\n        else:\n            requests_pathname_prefix = \"/\"\n\n        # FIXME Move config initialization to main dash __init__\n        # low-level setter to circumvent Dash's config locking\n        # normally it's unsafe to alter requests_pathname_prefix this late, but\n        # Jupyter needs some unusual behavior.\n        dict.__setitem__(\n            app.config, \"requests_pathname_prefix\", requests_pathname_prefix\n        )\n\n        # # Compute server_url url\n        if server_url is None:\n            if \"server_url\" in _jupyter_config:\n                server_url = _jupyter_config[\"server_url\"].rstrip(\"/\")\n            else:\n                domain_base = os.environ.get(\"DASH_DOMAIN_BASE\", None)\n                if domain_base:\n                    # Dash Enterprise sets DASH_DOMAIN_BASE environment variable\n                    server_url = \"https://\" + domain_base\n                else:\n                    server_url = f\"http://{host}:{port}\"\n        else:\n            server_url = server_url.rstrip(\"/\")\n\n        # server_url = \"http://{host}:{port}\".format(host=host, port=port)\n\n        dashboard_url = f\"{server_url}{requests_pathname_prefix}\"\n\n        # prevent partial import of orjson when it's installed and mode=jupyterlab\n        # TODO: why do we need this? Why only in this mode? Importing here in\n        # all modes anyway, in case there's a way it can pop up in another mode\n        try:\n            # pylint: disable=C0415,W0611\n            import orjson  # noqa: F401\n        except ImportError:\n            pass\n\n        err_q = queue.Queue()\n\n        server = make_server(host, port, app.server, threaded=True, processes=0)\n        logging.getLogger(\"werkzeug\").setLevel(logging.ERROR)\n\n        # ---------------------------------------------------------------------\n        # NOTE: added this code to mimic the _alive_{token} endpoint but with cors\n        with contextlib.suppress(ImportWarning, ModuleNotFoundError):\n            from flask_cors import cross_origin\n\n            @app.server.route(f\"/_is_alive_{self.uid}\", methods=[\"GET\"])\n            @cross_origin(origins=[\"*\"], allow_headers=[\"Content-Type\"])\n            def broadcast_alive():\n                return \"Alive\"\n\n        # ---------------------------------------------------------------------\n\n        @retry(\n            stop_max_attempt_number=15,\n            wait_exponential_multiplier=100,\n            wait_exponential_max=1000,\n        )\n        def run():\n            try:\n                server.serve_forever()\n            except SystemExit:\n                pass\n            except Exception as error:\n                err_q.put(error)\n                raise error\n\n        thread = threading.Thread(target=run)\n        thread.daemon = True\n        thread.start()\n\n        JupyterDash._servers[(host, port)] = server\n\n        # Wait for server to start up\n        alive_url = f\"http://{host}:{port}/_alive_{JupyterDash.alive_token}\"\n\n        def _get_error():\n            try:\n                err = err_q.get_nowait()\n                if err:\n                    raise err\n            except queue.Empty:\n                pass\n\n        # Wait for app to respond to _alive endpoint\n        @retry(\n            stop_max_attempt_number=15,\n            wait_exponential_multiplier=10,\n            wait_exponential_max=1000,\n        )\n        def wait_for_app():\n            _get_error()\n            try:\n                req = requests.get(alive_url)\n                res = req.content.decode()\n                if req.status_code != 200:\n                    raise Exception(res)\n\n                if res != \"Alive\":\n                    url = f\"http://{host}:{port}\"\n                    raise OSError(\n                        f\"Address '{url}' already in use.\\n\"\n                        \"    Try passing a different port to run_server.\"\n                    )\n            except requests.ConnectionError as err:\n                _get_error()\n                raise err\n\n        try:\n            wait_for_app()\n            self._display_inline_output(dashboard_url, width=width, height=height)\n\n        except Exception as final_error:  # pylint: disable=broad-except\n            msg = str(final_error)\n            if msg.startswith(\"&lt;!\"):\n                display(HTML(msg))\n            else:\n                raise final_error\n</code></pre>"},{"location":"api/figure_resampler/jupyter_dash_persistent_inline_output/#figure_resampler.jupyter_dash_persistent_inline_output.JupyterDashPersistentInlineOutput.run_app","title":"<code>run_app(app, width='100%', height=650, host='127.0.0.1', port=8050, server_url=None)</code>","text":"<p>Run the inline persistent dash app in the notebook.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Dash</code> <p>A Dash application instance</p> required <code>width</code> <code>str</code> <p>Width of app when displayed using mode=\u201dinline\u201d, by default \u201c100%\u201d</p> <code>'100%'</code> <code>height</code> <code>int</code> <p>Height of app when displayed using mode=\u201dinline\u201d, by default 650</p> <code>650</code> <code>host</code> <code>str</code> <p>Host of the server, by default \u201c127.0.0.1\u201d</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>Port used by the server, by default 8050</p> <code>8050</code> <code>server_url</code> <code>str</code> <p>Use if a custom url is required to display the app, by default None</p> <code>None</code> Source code in <code>plotly_resampler/figure_resampler/jupyter_dash_persistent_inline_output.py</code> <pre><code>def run_app(\n    self,\n    app,\n    width=\"100%\",\n    height=650,\n    host=\"127.0.0.1\",\n    port=8050,\n    server_url=None,\n):\n    \"\"\"Run the inline persistent dash app in the notebook.\n\n    Parameters\n    ----------\n    app : dash.Dash\n        A Dash application instance\n    width : str, optional\n        Width of app when displayed using mode=\"inline\", by default \"100%\"\n    height : int, optional\n        Height of app when displayed using mode=\"inline\", by default 650\n    host : str, optional\n        Host of the server, by default \"127.0.0.1\"\n    port : int, optional\n        Port used by the server, by default 8050\n    server_url : str, optional\n        Use if a custom url is required to display the app, by default None\n\n    \"\"\"\n    # Terminate any existing server using this port\n    old_server = JupyterDash._servers.get((host, port))\n    if old_server:\n        old_server.shutdown()\n        del JupyterDash._servers[(host, port)]\n\n    # Configure pathname prefix\n    if \"base_subpath\" in _jupyter_config:\n        requests_pathname_prefix = (\n            _jupyter_config[\"base_subpath\"].rstrip(\"/\") + \"/proxy/{port}/\"\n        )\n    else:\n        requests_pathname_prefix = app.config.get(\"requests_pathname_prefix\", None)\n\n    if requests_pathname_prefix is not None:\n        requests_pathname_prefix = requests_pathname_prefix.format(port=port)\n    else:\n        requests_pathname_prefix = \"/\"\n\n    # FIXME Move config initialization to main dash __init__\n    # low-level setter to circumvent Dash's config locking\n    # normally it's unsafe to alter requests_pathname_prefix this late, but\n    # Jupyter needs some unusual behavior.\n    dict.__setitem__(\n        app.config, \"requests_pathname_prefix\", requests_pathname_prefix\n    )\n\n    # # Compute server_url url\n    if server_url is None:\n        if \"server_url\" in _jupyter_config:\n            server_url = _jupyter_config[\"server_url\"].rstrip(\"/\")\n        else:\n            domain_base = os.environ.get(\"DASH_DOMAIN_BASE\", None)\n            if domain_base:\n                # Dash Enterprise sets DASH_DOMAIN_BASE environment variable\n                server_url = \"https://\" + domain_base\n            else:\n                server_url = f\"http://{host}:{port}\"\n    else:\n        server_url = server_url.rstrip(\"/\")\n\n    # server_url = \"http://{host}:{port}\".format(host=host, port=port)\n\n    dashboard_url = f\"{server_url}{requests_pathname_prefix}\"\n\n    # prevent partial import of orjson when it's installed and mode=jupyterlab\n    # TODO: why do we need this? Why only in this mode? Importing here in\n    # all modes anyway, in case there's a way it can pop up in another mode\n    try:\n        # pylint: disable=C0415,W0611\n        import orjson  # noqa: F401\n    except ImportError:\n        pass\n\n    err_q = queue.Queue()\n\n    server = make_server(host, port, app.server, threaded=True, processes=0)\n    logging.getLogger(\"werkzeug\").setLevel(logging.ERROR)\n\n    # ---------------------------------------------------------------------\n    # NOTE: added this code to mimic the _alive_{token} endpoint but with cors\n    with contextlib.suppress(ImportWarning, ModuleNotFoundError):\n        from flask_cors import cross_origin\n\n        @app.server.route(f\"/_is_alive_{self.uid}\", methods=[\"GET\"])\n        @cross_origin(origins=[\"*\"], allow_headers=[\"Content-Type\"])\n        def broadcast_alive():\n            return \"Alive\"\n\n    # ---------------------------------------------------------------------\n\n    @retry(\n        stop_max_attempt_number=15,\n        wait_exponential_multiplier=100,\n        wait_exponential_max=1000,\n    )\n    def run():\n        try:\n            server.serve_forever()\n        except SystemExit:\n            pass\n        except Exception as error:\n            err_q.put(error)\n            raise error\n\n    thread = threading.Thread(target=run)\n    thread.daemon = True\n    thread.start()\n\n    JupyterDash._servers[(host, port)] = server\n\n    # Wait for server to start up\n    alive_url = f\"http://{host}:{port}/_alive_{JupyterDash.alive_token}\"\n\n    def _get_error():\n        try:\n            err = err_q.get_nowait()\n            if err:\n                raise err\n        except queue.Empty:\n            pass\n\n    # Wait for app to respond to _alive endpoint\n    @retry(\n        stop_max_attempt_number=15,\n        wait_exponential_multiplier=10,\n        wait_exponential_max=1000,\n    )\n    def wait_for_app():\n        _get_error()\n        try:\n            req = requests.get(alive_url)\n            res = req.content.decode()\n            if req.status_code != 200:\n                raise Exception(res)\n\n            if res != \"Alive\":\n                url = f\"http://{host}:{port}\"\n                raise OSError(\n                    f\"Address '{url}' already in use.\\n\"\n                    \"    Try passing a different port to run_server.\"\n                )\n        except requests.ConnectionError as err:\n            _get_error()\n            raise err\n\n    try:\n        wait_for_app()\n        self._display_inline_output(dashboard_url, width=width, height=height)\n\n    except Exception as final_error:  # pylint: disable=broad-except\n        msg = str(final_error)\n        if msg.startswith(\"&lt;!\"):\n            display(HTML(msg))\n        else:\n            raise final_error\n</code></pre>"},{"location":"api/figure_resampler/utils/","title":"utils","text":"<p>Utility functions for the figure_resampler submodule.</p>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.is_figure","title":"<code>is_figure(figure)</code>","text":"<p>Check if the figure is a plotly go.Figure or a FigureResampler.</p> <p>Note</p> <p>This method does not use isinstance(figure, go.Figure) as this will not work when go.Figure is decorated (after executing the <code>register_plotly_resampler</code> function).</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Any</code> <p>The figure to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the figure is a plotly go.Figure or a FigureResampler.</p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def is_figure(figure: Any) -&gt; bool:\n    \"\"\"Check if the figure is a plotly go.Figure or a FigureResampler.\n\n    !!! note\n\n        This method does not use isinstance(figure, go.Figure) as this will not work\n        when go.Figure is decorated (after executing the\n        ``register_plotly_resampler`` function).\n\n    Parameters\n    ----------\n    figure : Any\n        The figure to check.\n\n    Returns\n    -------\n    bool\n        True if the figure is a plotly go.Figure or a FigureResampler.\n    \"\"\"\n    return isinstance(figure, BaseFigure) and (not isinstance(figure, BaseFigureWidget))\n</code></pre>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.is_figurewidget","title":"<code>is_figurewidget(figure)</code>","text":"<p>Check if the figure is a plotly go.FigureWidget or a FigureWidgetResampler.</p> <p>Note</p> <p>This method does not use isinstance(figure, go.FigureWidget) as this will not work when go.FigureWidget is decorated (after executing the <code>register_plotly_resampler</code> function).</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Any</code> <p>The figure to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the figure is a plotly go.FigureWidget or a FigureWidgetResampler.</p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def is_figurewidget(figure: Any):\n    \"\"\"Check if the figure is a plotly go.FigureWidget or a FigureWidgetResampler.\n\n    !!! note\n\n        This method does not use isinstance(figure, go.FigureWidget) as this will not\n        work when go.FigureWidget is decorated (after executing the\n        ``register_plotly_resampler`` function).\n\n    Parameters\n    ----------\n    figure : Any\n        The figure to check.\n\n    Returns\n    -------\n    bool\n        True if the figure is a plotly go.FigureWidget or a FigureWidgetResampler.\n    \"\"\"\n    return isinstance(figure, BaseFigureWidget)\n</code></pre>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.is_fr","title":"<code>is_fr(figure)</code>","text":"<p>Check if the figure is a FigureResampler.</p> <p>Note</p> <p>This method will not return True if the figure is a plotly go.Figure.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Any</code> <p>The figure to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the figure is a FigureResampler.</p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def is_fr(figure: Any) -&gt; bool:\n    \"\"\"Check if the figure is a FigureResampler.\n\n    !!! note\n\n        This method will not return True if the figure is a plotly go.Figure.\n\n    Parameters\n    ----------\n    figure : Any\n        The figure to check.\n\n    Returns\n    -------\n    bool\n        True if the figure is a FigureResampler.\n    \"\"\"\n    from plotly_resampler import FigureResampler\n\n    return isinstance(figure, FigureResampler)\n</code></pre>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.is_fwr","title":"<code>is_fwr(figure)</code>","text":"<p>Check if the figure is a FigureWidgetResampler.</p> <p>Note</p> <p>This method will not return True if the figure is a plotly go.FigureWidget.</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Any</code> <p>The figure to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the figure is a FigureWidgetResampler.</p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def is_fwr(figure: Any) -&gt; bool:\n    \"\"\"Check if the figure is a FigureWidgetResampler.\n\n    !!! note\n\n        This method will not return True if the figure is a plotly go.FigureWidget.\n\n    Parameters\n    ----------\n    figure : Any\n        The figure to check.\n\n    Returns\n    -------\n    bool\n        True if the figure is a FigureWidgetResampler.\n    \"\"\"\n    from plotly_resampler import FigureWidgetResampler\n\n    return isinstance(figure, FigureWidgetResampler)\n</code></pre>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.round_number_str","title":"<code>round_number_str(number)</code>","text":"<p>Round a number to the nearest unit and convert to a string.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>float</code> <p>The number to round.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The rounded number as a string. If the number is == 0, None is returned.</p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def round_number_str(number: float) -&gt; str:\n    \"\"\"Round a number to the nearest unit and convert to a string.\n\n    Parameters\n    ----------\n    number : float\n        The number to round.\n\n    Returns\n    -------\n    str\n        The rounded number as a string.\n        If the number is == 0, None is returned.\n\n    \"\"\"\n    sign = \"-\" if number &lt; 0 else \"\"\n    number = abs(number)\n    if number &gt; 0.95:\n        for unit, scaling in [\n            (\"T\", int(1e12)),  # Trillion\n            (\"B\", int(1e9)),  # Billion\n            (\"M\", int(1e6)),  # Million\n            (\"k\", int(1e3)),  # Thousand\n        ]:\n            if number / scaling &gt; 0.95:\n                return f\"{round(number / scaling)}{unit}\"\n        return sign + str(round(number))\n    if number &gt; 0:  # avoid log10(0)\n        # we have a number between 0-0.95 -&gt; round till nearest non-zero digit\n        return sign + str(round(number, 1 + abs(int(math.log10(number)))))\n</code></pre>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.round_td_str","title":"<code>round_td_str(td)</code>","text":"<p>Round a timedelta to the nearest unit and convert to a string.</p> <p>Parameters:</p> Name Type Description Default <code>td</code> <code>Timedelta</code> <p>The timedelta to round.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The rounded timedelta as a string. If the timedelta is == 0, None is returned.</p> <code>!!! info \"See Also\"</code> <p><code>timedelta_to_str</code></p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def round_td_str(td: pd.Timedelta) -&gt; str:\n    \"\"\"Round a timedelta to the nearest unit and convert to a string.\n\n    Parameters\n    ----------\n    td : pd.Timedelta\n        The timedelta to round.\n\n    Returns\n    -------\n    str\n        The rounded timedelta as a string.\n        If the timedelta is == 0, None is returned.\n\n    !!! info \"See Also\"\n        [`timedelta_to_str`][figure_resampler.utils.timedelta_to_str]\n\n    \"\"\"\n    for t_s in (\"D\", \"h\", \"min\", \"s\", \"ms\", \"us\", \"ns\"):\n        if td &gt; 0.95 * pd.Timedelta(f\"1{t_s}\"):\n            return timedelta_to_str(td.round(t_s))\n</code></pre>"},{"location":"api/figure_resampler/utils/#figure_resampler.utils.timedelta_to_str","title":"<code>timedelta_to_str(td)</code>","text":"<p>Construct a tight string representation for the given timedelta arg.</p> <p>Parameters:</p> Name Type Description Default <code>td</code> <code>Timedelta</code> <p>The timedelta for which the string representation is constructed</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The tight string bounds of format \u2018$d-$h$m$s.$ms\u2019. If the timedelta is negative, the string starts with \u2018NEG\u2019.</p> Source code in <code>plotly_resampler/figure_resampler/utils.py</code> <pre><code>def timedelta_to_str(td: pd.Timedelta) -&gt; str:\n    \"\"\"Construct a tight string representation for the given timedelta arg.\n\n    Parameters\n    ----------\n    td: pd.Timedelta\n        The timedelta for which the string representation is constructed\n\n    Returns\n    -------\n    str:\n        The tight string bounds of format '$d-$h$m$s.$ms'.\n        If the timedelta is negative, the string starts with 'NEG'.\n\n    \"\"\"\n    out_str = \"\"\n\n    # Edge case if we deal with negative\n    if td &lt; pd.Timedelta(seconds=0):\n        td *= -1\n        out_str += \"NEG\"\n\n    # Note: this must happen after the *= -1\n    c = td.components\n    if c.days &gt; 0:\n        out_str += f\"{c.days}D\"\n    if c.hours &gt; 0 or c.minutes &gt; 0 or c.seconds &gt; 0 or c.milliseconds &gt; 0:\n        out_str += \"_\" if out_str else \"\"  # add seperator if non-empty\n\n    if c.hours &gt; 0:\n        out_str += f\"{c.hours}h\"\n    if c.minutes &gt; 0:\n        out_str += f\"{c.minutes}m\"\n    if c.seconds &gt; 0:\n        if c.milliseconds:\n            out_str += (\n                f\"{c.seconds}.{str(c.milliseconds / 1000).split('.')[-1].rstrip('0')}s\"\n            )\n        else:\n            out_str += f\"{c.seconds}s\"\n    elif c.milliseconds &gt; 0:\n        out_str += f\"{c.milliseconds}ms\"\n    if c.microseconds &gt; 0:\n        out_str += f\"{c.microseconds}us\"\n    if c.nanoseconds &gt; 0:\n        out_str += f\"{c.nanoseconds}ns\"\n    return out_str\n</code></pre>"}]}